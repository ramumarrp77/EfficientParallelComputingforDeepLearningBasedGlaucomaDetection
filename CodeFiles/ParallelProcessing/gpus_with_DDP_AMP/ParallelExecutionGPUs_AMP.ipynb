{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec50077f-82c0-41e4-a47c-db7b436fd8e1",
   "metadata": {},
   "source": [
    "# CSYE 7105 - High Perfoamnce Machine Learning & AI - Project - Team 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd13344-73cf-49e4-9aee-ecad90301452",
   "metadata": {},
   "source": [
    "## Analysis of Efficient Parallel Computing for Deep Learning-Based Glaucoma Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881ff63-c48e-4348-b9d8-c95f6f16db5f",
   "metadata": {},
   "source": [
    "### Parallel Execution using GPUs with DDP and AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6623177-2371-49dc-96ad-a5ef380fef9c",
   "metadata": {},
   "source": [
    "### Calling the main function through the torchrun command to train the model in 2 GPUs with DDP and AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dce895d-f04d-4c90-a4e9-b96d0330c056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0404 19:13:07.633000 1093138 site-packages/torch/distributed/run.py:793] \n",
      "W0404 19:13:07.633000 1093138 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0404 19:13:07.633000 1093138 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0404 19:13:07.633000 1093138 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "2025-04-04 19:13:15 - INFO - Running on node: d1004\n",
      "2025-04-04 19:13:15 - INFO - Running on node: d1004\n",
      "2025-04-04 19:13:15 - INFO - Running with world_size - GPU with AMP: 2 (Rank: 1)\n",
      "2025-04-04 19:13:15 - INFO - Running with world_size - GPU with AMP: 2 (Rank: 0)\n",
      "2025-04-04 19:13:15 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 19:13:15 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 19:16:20 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-04 19:16:20 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-04 19:16:22 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-04 19:16:22 - INFO - Total layers: 129\n",
      "2025-04-04 19:16:22 - INFO - Total parameters: 22,494,274\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:139: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:139: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 19:16:24 - INFO - Epoch [1/10], Batch [0/374], Loss: 0.7142\n",
      "2025-04-04 19:16:25 - INFO - Epoch [1/10], Batch [10/374], Loss: 0.7032\n",
      "2025-04-04 19:16:25 - INFO - Epoch [1/10], Batch [20/374], Loss: 0.6650\n",
      "2025-04-04 19:16:26 - INFO - Epoch [1/10], Batch [30/374], Loss: 0.6925\n",
      "2025-04-04 19:16:26 - INFO - Epoch [1/10], Batch [40/374], Loss: 0.6357\n",
      "2025-04-04 19:16:27 - INFO - Epoch [1/10], Batch [50/374], Loss: 0.6887\n",
      "2025-04-04 19:16:27 - INFO - Epoch [1/10], Batch [60/374], Loss: 0.6131\n",
      "2025-04-04 19:16:28 - INFO - Epoch [1/10], Batch [70/374], Loss: 0.6789\n",
      "2025-04-04 19:16:28 - INFO - Epoch [1/10], Batch [80/374], Loss: 0.6103\n",
      "2025-04-04 19:16:29 - INFO - Epoch [1/10], Batch [90/374], Loss: 0.7708\n",
      "2025-04-04 19:16:29 - INFO - Epoch [1/10], Batch [100/374], Loss: 0.7700\n",
      "2025-04-04 19:16:30 - INFO - Epoch [1/10], Batch [110/374], Loss: 0.7233\n",
      "2025-04-04 19:16:31 - INFO - Epoch [1/10], Batch [120/374], Loss: 0.7196\n",
      "2025-04-04 19:16:31 - INFO - Epoch [1/10], Batch [130/374], Loss: 0.6656\n",
      "2025-04-04 19:16:32 - INFO - Epoch [1/10], Batch [140/374], Loss: 0.7861\n",
      "2025-04-04 19:16:32 - INFO - Epoch [1/10], Batch [150/374], Loss: 0.7138\n",
      "2025-04-04 19:16:33 - INFO - Epoch [1/10], Batch [160/374], Loss: 0.6090\n",
      "2025-04-04 19:16:33 - INFO - Epoch [1/10], Batch [170/374], Loss: 0.6231\n",
      "2025-04-04 19:16:34 - INFO - Epoch [1/10], Batch [180/374], Loss: 0.6293\n",
      "2025-04-04 19:16:34 - INFO - Epoch [1/10], Batch [190/374], Loss: 0.6112\n",
      "2025-04-04 19:16:35 - INFO - Epoch [1/10], Batch [200/374], Loss: 0.6982\n",
      "2025-04-04 19:16:35 - INFO - Epoch [1/10], Batch [210/374], Loss: 0.7370\n",
      "2025-04-04 19:16:36 - INFO - Epoch [1/10], Batch [220/374], Loss: 0.6372\n",
      "2025-04-04 19:16:37 - INFO - Epoch [1/10], Batch [230/374], Loss: 0.5826\n",
      "2025-04-04 19:16:37 - INFO - Epoch [1/10], Batch [240/374], Loss: 0.6151\n",
      "2025-04-04 19:16:38 - INFO - Epoch [1/10], Batch [250/374], Loss: 0.5618\n",
      "2025-04-04 19:16:38 - INFO - Epoch [1/10], Batch [260/374], Loss: 0.5810\n",
      "2025-04-04 19:16:39 - INFO - Epoch [1/10], Batch [270/374], Loss: 0.5793\n",
      "2025-04-04 19:16:39 - INFO - Epoch [1/10], Batch [280/374], Loss: 0.5652\n",
      "2025-04-04 19:16:40 - INFO - Epoch [1/10], Batch [290/374], Loss: 0.5703\n",
      "2025-04-04 19:16:40 - INFO - Epoch [1/10], Batch [300/374], Loss: 0.6373\n",
      "2025-04-04 19:16:41 - INFO - Epoch [1/10], Batch [310/374], Loss: 0.6424\n",
      "2025-04-04 19:16:42 - INFO - Epoch [1/10], Batch [320/374], Loss: 0.8053\n",
      "2025-04-04 19:16:42 - INFO - Epoch [1/10], Batch [330/374], Loss: 0.6552\n",
      "2025-04-04 19:16:43 - INFO - Epoch [1/10], Batch [340/374], Loss: 0.5775\n",
      "2025-04-04 19:16:43 - INFO - Epoch [1/10], Batch [350/374], Loss: 0.6890\n",
      "2025-04-04 19:16:44 - INFO - Epoch [1/10], Batch [360/374], Loss: 0.6459\n",
      "2025-04-04 19:16:44 - INFO - Epoch [1/10], Batch [370/374], Loss: 0.5995\n",
      "2025-04-04 19:16:45 - INFO - Epoch [1/10] Train Loss: 0.6462, Train Accuracy: 61.34%\n",
      "2025-04-04 19:16:47 - INFO - Epoch [1/10] Val Loss: 0.5796, Val Accuracy: 72.79%\n",
      "2025-04-04 19:16:47 - INFO - New best model at epoch 1 with val accuracy: 72.79%\n",
      "2025-04-04 19:16:47 - INFO - Epoch [2/10], Batch [0/374], Loss: 0.5853\n",
      "2025-04-04 19:16:48 - INFO - Epoch [2/10], Batch [10/374], Loss: 0.5939\n",
      "2025-04-04 19:16:49 - INFO - Epoch [2/10], Batch [20/374], Loss: 0.5114\n",
      "2025-04-04 19:16:49 - INFO - Epoch [2/10], Batch [30/374], Loss: 0.6063\n",
      "2025-04-04 19:16:50 - INFO - Epoch [2/10], Batch [40/374], Loss: 0.8719\n",
      "2025-04-04 19:16:50 - INFO - Epoch [2/10], Batch [50/374], Loss: 0.5168\n",
      "2025-04-04 19:16:51 - INFO - Epoch [2/10], Batch [60/374], Loss: 0.7181\n",
      "2025-04-04 19:16:51 - INFO - Epoch [2/10], Batch [70/374], Loss: 0.7450\n",
      "2025-04-04 19:16:52 - INFO - Epoch [2/10], Batch [80/374], Loss: 0.5440\n",
      "2025-04-04 19:16:52 - INFO - Epoch [2/10], Batch [90/374], Loss: 0.4988\n",
      "2025-04-04 19:16:53 - INFO - Epoch [2/10], Batch [100/374], Loss: 0.4755\n",
      "2025-04-04 19:16:53 - INFO - Epoch [2/10], Batch [110/374], Loss: 0.5958\n",
      "2025-04-04 19:16:54 - INFO - Epoch [2/10], Batch [120/374], Loss: 0.5470\n",
      "2025-04-04 19:16:55 - INFO - Epoch [2/10], Batch [130/374], Loss: 0.5218\n",
      "2025-04-04 19:16:55 - INFO - Epoch [2/10], Batch [140/374], Loss: 0.6617\n",
      "2025-04-04 19:16:56 - INFO - Epoch [2/10], Batch [150/374], Loss: 0.6533\n",
      "2025-04-04 19:16:56 - INFO - Epoch [2/10], Batch [160/374], Loss: 0.5363\n",
      "2025-04-04 19:16:57 - INFO - Epoch [2/10], Batch [170/374], Loss: 0.6953\n",
      "2025-04-04 19:16:57 - INFO - Epoch [2/10], Batch [180/374], Loss: 0.6615\n",
      "2025-04-04 19:16:58 - INFO - Epoch [2/10], Batch [190/374], Loss: 0.4505\n",
      "2025-04-04 19:16:58 - INFO - Epoch [2/10], Batch [200/374], Loss: 0.6681\n",
      "2025-04-04 19:16:59 - INFO - Epoch [2/10], Batch [210/374], Loss: 0.5945\n",
      "2025-04-04 19:17:00 - INFO - Epoch [2/10], Batch [220/374], Loss: 0.7504\n",
      "2025-04-04 19:17:00 - INFO - Epoch [2/10], Batch [230/374], Loss: 0.6190\n",
      "2025-04-04 19:17:01 - INFO - Epoch [2/10], Batch [240/374], Loss: 0.6234\n",
      "2025-04-04 19:17:01 - INFO - Epoch [2/10], Batch [250/374], Loss: 0.6623\n",
      "2025-04-04 19:17:02 - INFO - Epoch [2/10], Batch [260/374], Loss: 0.6349\n",
      "2025-04-04 19:17:02 - INFO - Epoch [2/10], Batch [270/374], Loss: 0.4762\n",
      "2025-04-04 19:17:03 - INFO - Epoch [2/10], Batch [280/374], Loss: 0.4854\n",
      "2025-04-04 19:17:03 - INFO - Epoch [2/10], Batch [290/374], Loss: 0.5749\n",
      "2025-04-04 19:17:04 - INFO - Epoch [2/10], Batch [300/374], Loss: 0.5012\n",
      "2025-04-04 19:17:04 - INFO - Epoch [2/10], Batch [310/374], Loss: 0.5958\n",
      "2025-04-04 19:17:05 - INFO - Epoch [2/10], Batch [320/374], Loss: 0.6318\n",
      "2025-04-04 19:17:06 - INFO - Epoch [2/10], Batch [330/374], Loss: 0.4316\n",
      "2025-04-04 19:17:06 - INFO - Epoch [2/10], Batch [340/374], Loss: 0.5078\n",
      "2025-04-04 19:17:07 - INFO - Epoch [2/10], Batch [350/374], Loss: 0.6262\n",
      "2025-04-04 19:17:07 - INFO - Epoch [2/10], Batch [360/374], Loss: 0.5568\n",
      "2025-04-04 19:17:08 - INFO - Epoch [2/10], Batch [370/374], Loss: 0.5631\n",
      "2025-04-04 19:17:08 - INFO - Epoch [2/10] Train Loss: 0.5918, Train Accuracy: 68.36%\n",
      "2025-04-04 19:17:10 - INFO - Epoch [2/10] Val Loss: 0.5189, Val Accuracy: 76.97%\n",
      "2025-04-04 19:17:10 - INFO - New best model at epoch 2 with val accuracy: 76.97%\n",
      "2025-04-04 19:17:10 - INFO - Epoch [3/10], Batch [0/374], Loss: 0.6199\n",
      "2025-04-04 19:17:11 - INFO - Epoch [3/10], Batch [10/374], Loss: 0.6255\n",
      "2025-04-04 19:17:11 - INFO - Epoch [3/10], Batch [20/374], Loss: 0.6424\n",
      "2025-04-04 19:17:12 - INFO - Epoch [3/10], Batch [30/374], Loss: 0.6659\n",
      "2025-04-04 19:17:12 - INFO - Epoch [3/10], Batch [40/374], Loss: 0.4858\n",
      "2025-04-04 19:17:13 - INFO - Epoch [3/10], Batch [50/374], Loss: 0.7125\n",
      "2025-04-04 19:17:13 - INFO - Epoch [3/10], Batch [60/374], Loss: 0.6609\n",
      "2025-04-04 19:17:14 - INFO - Epoch [3/10], Batch [70/374], Loss: 0.5242\n",
      "2025-04-04 19:17:14 - INFO - Epoch [3/10], Batch [80/374], Loss: 0.4644\n",
      "2025-04-04 19:17:15 - INFO - Epoch [3/10], Batch [90/374], Loss: 0.5042\n",
      "2025-04-04 19:17:15 - INFO - Epoch [3/10], Batch [100/374], Loss: 0.5505\n",
      "2025-04-04 19:17:16 - INFO - Epoch [3/10], Batch [110/374], Loss: 0.5260\n",
      "2025-04-04 19:17:17 - INFO - Epoch [3/10], Batch [120/374], Loss: 0.4175\n",
      "2025-04-04 19:17:17 - INFO - Epoch [3/10], Batch [130/374], Loss: 0.5093\n",
      "2025-04-04 19:17:18 - INFO - Epoch [3/10], Batch [140/374], Loss: 0.5735\n",
      "2025-04-04 19:17:18 - INFO - Epoch [3/10], Batch [150/374], Loss: 0.5594\n",
      "2025-04-04 19:17:19 - INFO - Epoch [3/10], Batch [160/374], Loss: 0.4952\n",
      "2025-04-04 19:17:19 - INFO - Epoch [3/10], Batch [170/374], Loss: 0.5882\n",
      "2025-04-04 19:17:20 - INFO - Epoch [3/10], Batch [180/374], Loss: 0.5271\n",
      "2025-04-04 19:17:20 - INFO - Epoch [3/10], Batch [190/374], Loss: 0.6126\n",
      "2025-04-04 19:17:21 - INFO - Epoch [3/10], Batch [200/374], Loss: 0.5590\n",
      "2025-04-04 19:17:22 - INFO - Epoch [3/10], Batch [210/374], Loss: 0.7391\n",
      "2025-04-04 19:17:22 - INFO - Epoch [3/10], Batch [220/374], Loss: 0.5181\n",
      "2025-04-04 19:17:23 - INFO - Epoch [3/10], Batch [230/374], Loss: 0.4816\n",
      "2025-04-04 19:17:23 - INFO - Epoch [3/10], Batch [240/374], Loss: 0.4468\n",
      "2025-04-04 19:17:24 - INFO - Epoch [3/10], Batch [250/374], Loss: 0.5503\n",
      "2025-04-04 19:17:24 - INFO - Epoch [3/10], Batch [260/374], Loss: 0.4864\n",
      "2025-04-04 19:17:25 - INFO - Epoch [3/10], Batch [270/374], Loss: 0.4799\n",
      "2025-04-04 19:17:25 - INFO - Epoch [3/10], Batch [280/374], Loss: 0.4723\n",
      "2025-04-04 19:17:26 - INFO - Epoch [3/10], Batch [290/374], Loss: 0.6542\n",
      "2025-04-04 19:17:27 - INFO - Epoch [3/10], Batch [300/374], Loss: 0.3819\n",
      "2025-04-04 19:17:27 - INFO - Epoch [3/10], Batch [310/374], Loss: 0.7022\n",
      "2025-04-04 19:17:28 - INFO - Epoch [3/10], Batch [320/374], Loss: 0.4050\n",
      "2025-04-04 19:17:28 - INFO - Epoch [3/10], Batch [330/374], Loss: 0.5643\n",
      "2025-04-04 19:17:29 - INFO - Epoch [3/10], Batch [340/374], Loss: 0.6087\n",
      "2025-04-04 19:17:29 - INFO - Epoch [3/10], Batch [350/374], Loss: 0.4913\n",
      "2025-04-04 19:17:30 - INFO - Epoch [3/10], Batch [360/374], Loss: 0.6280\n",
      "2025-04-04 19:17:30 - INFO - Epoch [3/10], Batch [370/374], Loss: 0.3588\n",
      "2025-04-04 19:17:30 - INFO - Epoch [3/10] Train Loss: 0.5162, Train Accuracy: 73.85%\n",
      "2025-04-04 19:17:32 - INFO - Epoch [3/10] Val Loss: 0.4376, Val Accuracy: 80.17%\n",
      "2025-04-04 19:17:32 - INFO - New best model at epoch 3 with val accuracy: 80.17%\n",
      "2025-04-04 19:17:33 - INFO - Epoch [4/10], Batch [0/374], Loss: 0.4513\n",
      "2025-04-04 19:17:33 - INFO - Epoch [4/10], Batch [10/374], Loss: 0.4773\n",
      "2025-04-04 19:17:34 - INFO - Epoch [4/10], Batch [20/374], Loss: 0.3421\n",
      "2025-04-04 19:17:34 - INFO - Epoch [4/10], Batch [30/374], Loss: 0.3669\n",
      "2025-04-04 19:17:35 - INFO - Epoch [4/10], Batch [40/374], Loss: 0.3902\n",
      "2025-04-04 19:17:35 - INFO - Epoch [4/10], Batch [50/374], Loss: 0.5287\n",
      "2025-04-04 19:17:36 - INFO - Epoch [4/10], Batch [60/374], Loss: 0.7528\n",
      "2025-04-04 19:17:36 - INFO - Epoch [4/10], Batch [70/374], Loss: 0.4587\n",
      "2025-04-04 19:17:37 - INFO - Epoch [4/10], Batch [80/374], Loss: 0.5893\n",
      "2025-04-04 19:17:38 - INFO - Epoch [4/10], Batch [90/374], Loss: 0.3715\n",
      "2025-04-04 19:17:38 - INFO - Epoch [4/10], Batch [100/374], Loss: 0.4632\n",
      "2025-04-04 19:17:39 - INFO - Epoch [4/10], Batch [110/374], Loss: 0.4555\n",
      "2025-04-04 19:17:39 - INFO - Epoch [4/10], Batch [120/374], Loss: 0.4222\n",
      "2025-04-04 19:17:40 - INFO - Epoch [4/10], Batch [130/374], Loss: 0.5549\n",
      "2025-04-04 19:17:40 - INFO - Epoch [4/10], Batch [140/374], Loss: 0.3935\n",
      "2025-04-04 19:17:41 - INFO - Epoch [4/10], Batch [150/374], Loss: 0.4539\n",
      "2025-04-04 19:17:41 - INFO - Epoch [4/10], Batch [160/374], Loss: 0.5738\n",
      "2025-04-04 19:17:42 - INFO - Epoch [4/10], Batch [170/374], Loss: 0.5222\n",
      "2025-04-04 19:17:43 - INFO - Epoch [4/10], Batch [180/374], Loss: 0.5875\n",
      "2025-04-04 19:17:43 - INFO - Epoch [4/10], Batch [190/374], Loss: 0.4438\n",
      "2025-04-04 19:17:44 - INFO - Epoch [4/10], Batch [200/374], Loss: 0.4860\n",
      "2025-04-04 19:17:44 - INFO - Epoch [4/10], Batch [210/374], Loss: 0.3556\n",
      "2025-04-04 19:17:45 - INFO - Epoch [4/10], Batch [220/374], Loss: 0.4302\n",
      "2025-04-04 19:17:45 - INFO - Epoch [4/10], Batch [230/374], Loss: 0.3310\n",
      "2025-04-04 19:17:46 - INFO - Epoch [4/10], Batch [240/374], Loss: 0.3446\n",
      "2025-04-04 19:17:46 - INFO - Epoch [4/10], Batch [250/374], Loss: 0.4750\n",
      "2025-04-04 19:17:47 - INFO - Epoch [4/10], Batch [260/374], Loss: 0.7398\n",
      "2025-04-04 19:17:47 - INFO - Epoch [4/10], Batch [270/374], Loss: 0.3941\n",
      "2025-04-04 19:17:48 - INFO - Epoch [4/10], Batch [280/374], Loss: 0.3389\n",
      "2025-04-04 19:17:49 - INFO - Epoch [4/10], Batch [290/374], Loss: 0.3997\n",
      "2025-04-04 19:17:49 - INFO - Epoch [4/10], Batch [300/374], Loss: 0.4575\n",
      "2025-04-04 19:17:50 - INFO - Epoch [4/10], Batch [310/374], Loss: 0.3865\n",
      "2025-04-04 19:17:50 - INFO - Epoch [4/10], Batch [320/374], Loss: 0.5196\n",
      "2025-04-04 19:17:51 - INFO - Epoch [4/10], Batch [330/374], Loss: 0.3598\n",
      "2025-04-04 19:17:51 - INFO - Epoch [4/10], Batch [340/374], Loss: 0.3364\n",
      "2025-04-04 19:17:52 - INFO - Epoch [4/10], Batch [350/374], Loss: 0.4095\n",
      "2025-04-04 19:17:52 - INFO - Epoch [4/10], Batch [360/374], Loss: 0.2708\n",
      "2025-04-04 19:17:53 - INFO - Epoch [4/10], Batch [370/374], Loss: 0.3790\n",
      "2025-04-04 19:17:53 - INFO - Epoch [4/10] Train Loss: 0.4568, Train Accuracy: 78.03%\n",
      "2025-04-04 19:17:55 - INFO - Epoch [4/10] Val Loss: 0.3971, Val Accuracy: 83.05%\n",
      "2025-04-04 19:17:55 - INFO - New best model at epoch 4 with val accuracy: 83.05%\n",
      "2025-04-04 19:17:55 - INFO - Epoch [5/10], Batch [0/374], Loss: 0.3867\n",
      "2025-04-04 19:17:56 - INFO - Epoch [5/10], Batch [10/374], Loss: 0.3282\n",
      "2025-04-04 19:17:56 - INFO - Epoch [5/10], Batch [20/374], Loss: 0.3720\n",
      "2025-04-04 19:17:57 - INFO - Epoch [5/10], Batch [30/374], Loss: 0.3094\n",
      "2025-04-04 19:17:57 - INFO - Epoch [5/10], Batch [40/374], Loss: 0.3102\n",
      "2025-04-04 19:17:58 - INFO - Epoch [5/10], Batch [50/374], Loss: 0.3579\n",
      "2025-04-04 19:17:59 - INFO - Epoch [5/10], Batch [60/374], Loss: 0.4782\n",
      "2025-04-04 19:17:59 - INFO - Epoch [5/10], Batch [70/374], Loss: 0.3743\n",
      "2025-04-04 19:18:00 - INFO - Epoch [5/10], Batch [80/374], Loss: 0.3203\n",
      "2025-04-04 19:18:00 - INFO - Epoch [5/10], Batch [90/374], Loss: 0.3158\n",
      "2025-04-04 19:18:01 - INFO - Epoch [5/10], Batch [100/374], Loss: 0.3754\n",
      "2025-04-04 19:18:01 - INFO - Epoch [5/10], Batch [110/374], Loss: 0.5136\n",
      "2025-04-04 19:18:02 - INFO - Epoch [5/10], Batch [120/374], Loss: 0.3585\n",
      "2025-04-04 19:18:02 - INFO - Epoch [5/10], Batch [130/374], Loss: 0.5308\n",
      "2025-04-04 19:18:03 - INFO - Epoch [5/10], Batch [140/374], Loss: 0.3440\n",
      "2025-04-04 19:18:03 - INFO - Epoch [5/10], Batch [150/374], Loss: 0.3606\n",
      "2025-04-04 19:18:04 - INFO - Epoch [5/10], Batch [160/374], Loss: 0.3678\n",
      "2025-04-04 19:18:05 - INFO - Epoch [5/10], Batch [170/374], Loss: 0.2686\n",
      "2025-04-04 19:18:05 - INFO - Epoch [5/10], Batch [180/374], Loss: 0.4361\n",
      "2025-04-04 19:18:06 - INFO - Epoch [5/10], Batch [190/374], Loss: 0.5801\n",
      "2025-04-04 19:18:06 - INFO - Epoch [5/10], Batch [200/374], Loss: 0.5526\n",
      "2025-04-04 19:18:07 - INFO - Epoch [5/10], Batch [210/374], Loss: 0.2810\n",
      "2025-04-04 19:18:08 - INFO - Epoch [5/10], Batch [220/374], Loss: 0.3578\n",
      "2025-04-04 19:18:08 - INFO - Epoch [5/10], Batch [230/374], Loss: 0.4706\n",
      "2025-04-04 19:18:09 - INFO - Epoch [5/10], Batch [240/374], Loss: 0.4077\n",
      "2025-04-04 19:18:09 - INFO - Epoch [5/10], Batch [250/374], Loss: 0.4204\n",
      "2025-04-04 19:18:10 - INFO - Epoch [5/10], Batch [260/374], Loss: 0.2273\n",
      "2025-04-04 19:18:10 - INFO - Epoch [5/10], Batch [270/374], Loss: 0.4661\n",
      "2025-04-04 19:18:11 - INFO - Epoch [5/10], Batch [280/374], Loss: 0.4337\n",
      "2025-04-04 19:18:11 - INFO - Epoch [5/10], Batch [290/374], Loss: 0.5033\n",
      "2025-04-04 19:18:12 - INFO - Epoch [5/10], Batch [300/374], Loss: 0.3263\n",
      "2025-04-04 19:18:12 - INFO - Epoch [5/10], Batch [310/374], Loss: 0.2934\n",
      "2025-04-04 19:18:13 - INFO - Epoch [5/10], Batch [320/374], Loss: 0.3264\n",
      "2025-04-04 19:18:13 - INFO - Epoch [5/10], Batch [330/374], Loss: 0.3737\n",
      "2025-04-04 19:18:14 - INFO - Epoch [5/10], Batch [340/374], Loss: 0.2778\n",
      "2025-04-04 19:18:15 - INFO - Epoch [5/10], Batch [350/374], Loss: 0.3205\n",
      "2025-04-04 19:18:15 - INFO - Epoch [5/10], Batch [360/374], Loss: 0.3033\n",
      "2025-04-04 19:18:16 - INFO - Epoch [5/10], Batch [370/374], Loss: 0.5963\n",
      "2025-04-04 19:18:16 - INFO - Epoch [5/10] Train Loss: 0.4217, Train Accuracy: 80.29%\n",
      "2025-04-04 19:18:18 - INFO - Epoch [5/10] Val Loss: 0.3918, Val Accuracy: 82.57%\n",
      "2025-04-04 19:18:18 - INFO - Epoch [6/10], Batch [0/374], Loss: 0.3601\n",
      "2025-04-04 19:18:18 - INFO - Epoch [6/10], Batch [10/374], Loss: 0.4777\n",
      "2025-04-04 19:18:19 - INFO - Epoch [6/10], Batch [20/374], Loss: 0.5182\n",
      "2025-04-04 19:18:20 - INFO - Epoch [6/10], Batch [30/374], Loss: 0.4577\n",
      "2025-04-04 19:18:20 - INFO - Epoch [6/10], Batch [40/374], Loss: 0.4762\n",
      "2025-04-04 19:18:21 - INFO - Epoch [6/10], Batch [50/374], Loss: 0.2933\n",
      "2025-04-04 19:18:21 - INFO - Epoch [6/10], Batch [60/374], Loss: 0.3477\n",
      "2025-04-04 19:18:22 - INFO - Epoch [6/10], Batch [70/374], Loss: 0.6446\n",
      "2025-04-04 19:18:22 - INFO - Epoch [6/10], Batch [80/374], Loss: 0.3366\n",
      "2025-04-04 19:18:23 - INFO - Epoch [6/10], Batch [90/374], Loss: 0.4831\n",
      "2025-04-04 19:18:23 - INFO - Epoch [6/10], Batch [100/374], Loss: 0.3952\n",
      "2025-04-04 19:18:24 - INFO - Epoch [6/10], Batch [110/374], Loss: 0.4505\n",
      "2025-04-04 19:18:24 - INFO - Epoch [6/10], Batch [120/374], Loss: 0.4205\n",
      "2025-04-04 19:18:25 - INFO - Epoch [6/10], Batch [130/374], Loss: 0.3609\n",
      "2025-04-04 19:18:26 - INFO - Epoch [6/10], Batch [140/374], Loss: 0.3621\n",
      "2025-04-04 19:18:26 - INFO - Epoch [6/10], Batch [150/374], Loss: 0.2473\n",
      "2025-04-04 19:18:27 - INFO - Epoch [6/10], Batch [160/374], Loss: 0.3458\n",
      "2025-04-04 19:18:27 - INFO - Epoch [6/10], Batch [170/374], Loss: 0.2572\n",
      "2025-04-04 19:18:28 - INFO - Epoch [6/10], Batch [180/374], Loss: 0.2990\n",
      "2025-04-04 19:18:28 - INFO - Epoch [6/10], Batch [190/374], Loss: 0.2820\n",
      "2025-04-04 19:18:29 - INFO - Epoch [6/10], Batch [200/374], Loss: 0.3488\n",
      "2025-04-04 19:18:29 - INFO - Epoch [6/10], Batch [210/374], Loss: 0.2872\n",
      "2025-04-04 19:18:30 - INFO - Epoch [6/10], Batch [220/374], Loss: 0.4755\n",
      "2025-04-04 19:18:31 - INFO - Epoch [6/10], Batch [230/374], Loss: 0.4049\n",
      "2025-04-04 19:18:31 - INFO - Epoch [6/10], Batch [240/374], Loss: 0.7165\n",
      "2025-04-04 19:18:32 - INFO - Epoch [6/10], Batch [250/374], Loss: 0.4633\n",
      "2025-04-04 19:18:32 - INFO - Epoch [6/10], Batch [260/374], Loss: 0.3069\n",
      "2025-04-04 19:18:33 - INFO - Epoch [6/10], Batch [270/374], Loss: 0.2887\n",
      "2025-04-04 19:18:33 - INFO - Epoch [6/10], Batch [280/374], Loss: 0.2566\n",
      "2025-04-04 19:18:34 - INFO - Epoch [6/10], Batch [290/374], Loss: 0.2483\n",
      "2025-04-04 19:18:34 - INFO - Epoch [6/10], Batch [300/374], Loss: 0.4003\n",
      "2025-04-04 19:18:35 - INFO - Epoch [6/10], Batch [310/374], Loss: 0.3335\n",
      "2025-04-04 19:18:35 - INFO - Epoch [6/10], Batch [320/374], Loss: 0.3530\n",
      "2025-04-04 19:18:36 - INFO - Epoch [6/10], Batch [330/374], Loss: 0.4333\n",
      "2025-04-04 19:18:37 - INFO - Epoch [6/10], Batch [340/374], Loss: 0.3130\n",
      "2025-04-04 19:18:37 - INFO - Epoch [6/10], Batch [350/374], Loss: 0.3947\n",
      "2025-04-04 19:18:38 - INFO - Epoch [6/10], Batch [360/374], Loss: 0.5399\n",
      "2025-04-04 19:18:38 - INFO - Epoch [6/10], Batch [370/374], Loss: 0.4885\n",
      "2025-04-04 19:18:38 - INFO - Epoch [6/10] Train Loss: 0.3938, Train Accuracy: 81.74%\n",
      "2025-04-04 19:18:40 - INFO - Epoch [6/10] Val Loss: 0.3306, Val Accuracy: 86.12%\n",
      "2025-04-04 19:18:40 - INFO - New best model at epoch 6 with val accuracy: 86.12%\n",
      "2025-04-04 19:18:40 - INFO - Epoch [7/10], Batch [0/374], Loss: 0.2899\n",
      "2025-04-04 19:18:41 - INFO - Epoch [7/10], Batch [10/374], Loss: 0.4056\n",
      "2025-04-04 19:18:42 - INFO - Epoch [7/10], Batch [20/374], Loss: 0.3864\n",
      "2025-04-04 19:18:42 - INFO - Epoch [7/10], Batch [30/374], Loss: 0.4384\n",
      "2025-04-04 19:18:43 - INFO - Epoch [7/10], Batch [40/374], Loss: 0.4932\n",
      "2025-04-04 19:18:43 - INFO - Epoch [7/10], Batch [50/374], Loss: 0.4449\n",
      "2025-04-04 19:18:44 - INFO - Epoch [7/10], Batch [60/374], Loss: 0.5212\n",
      "2025-04-04 19:18:44 - INFO - Epoch [7/10], Batch [70/374], Loss: 0.2436\n",
      "2025-04-04 19:18:45 - INFO - Epoch [7/10], Batch [80/374], Loss: 0.5410\n",
      "2025-04-04 19:18:45 - INFO - Epoch [7/10], Batch [90/374], Loss: 0.2997\n",
      "2025-04-04 19:18:46 - INFO - Epoch [7/10], Batch [100/374], Loss: 0.3449\n",
      "2025-04-04 19:18:46 - INFO - Epoch [7/10], Batch [110/374], Loss: 0.2984\n",
      "2025-04-04 19:18:47 - INFO - Epoch [7/10], Batch [120/374], Loss: 0.5784\n",
      "2025-04-04 19:18:48 - INFO - Epoch [7/10], Batch [130/374], Loss: 0.3334\n",
      "2025-04-04 19:18:48 - INFO - Epoch [7/10], Batch [140/374], Loss: 0.2923\n",
      "2025-04-04 19:18:49 - INFO - Epoch [7/10], Batch [150/374], Loss: 0.1943\n",
      "2025-04-04 19:18:49 - INFO - Epoch [7/10], Batch [160/374], Loss: 0.4005\n",
      "2025-04-04 19:18:50 - INFO - Epoch [7/10], Batch [170/374], Loss: 0.2308\n",
      "2025-04-04 19:18:50 - INFO - Epoch [7/10], Batch [180/374], Loss: 0.3599\n",
      "2025-04-04 19:18:51 - INFO - Epoch [7/10], Batch [190/374], Loss: 0.4081\n",
      "2025-04-04 19:18:51 - INFO - Epoch [7/10], Batch [200/374], Loss: 0.3611\n",
      "2025-04-04 19:18:52 - INFO - Epoch [7/10], Batch [210/374], Loss: 0.2810\n",
      "2025-04-04 19:18:53 - INFO - Epoch [7/10], Batch [220/374], Loss: 0.3432\n",
      "2025-04-04 19:18:53 - INFO - Epoch [7/10], Batch [230/374], Loss: 0.3770\n",
      "2025-04-04 19:18:54 - INFO - Epoch [7/10], Batch [240/374], Loss: 0.4853\n",
      "2025-04-04 19:18:54 - INFO - Epoch [7/10], Batch [250/374], Loss: 0.3002\n",
      "2025-04-04 19:18:55 - INFO - Epoch [7/10], Batch [260/374], Loss: 0.2720\n",
      "2025-04-04 19:18:55 - INFO - Epoch [7/10], Batch [270/374], Loss: 0.4192\n",
      "2025-04-04 19:18:56 - INFO - Epoch [7/10], Batch [280/374], Loss: 0.3779\n",
      "2025-04-04 19:18:56 - INFO - Epoch [7/10], Batch [290/374], Loss: 0.2201\n",
      "2025-04-04 19:18:57 - INFO - Epoch [7/10], Batch [300/374], Loss: 0.3733\n",
      "2025-04-04 19:18:58 - INFO - Epoch [7/10], Batch [310/374], Loss: 0.3498\n",
      "2025-04-04 19:18:58 - INFO - Epoch [7/10], Batch [320/374], Loss: 0.4798\n",
      "2025-04-04 19:18:59 - INFO - Epoch [7/10], Batch [330/374], Loss: 0.3289\n",
      "2025-04-04 19:18:59 - INFO - Epoch [7/10], Batch [340/374], Loss: 0.3755\n",
      "2025-04-04 19:19:00 - INFO - Epoch [7/10], Batch [350/374], Loss: 0.5202\n",
      "2025-04-04 19:19:00 - INFO - Epoch [7/10], Batch [360/374], Loss: 0.3114\n",
      "2025-04-04 19:19:01 - INFO - Epoch [7/10], Batch [370/374], Loss: 0.5392\n",
      "2025-04-04 19:19:01 - INFO - Epoch [7/10] Train Loss: 0.3760, Train Accuracy: 82.70%\n",
      "2025-04-04 19:19:03 - INFO - Epoch [7/10] Val Loss: 0.3132, Val Accuracy: 86.50%\n",
      "2025-04-04 19:19:03 - INFO - New best model at epoch 7 with val accuracy: 86.50%\n",
      "2025-04-04 19:19:03 - INFO - Epoch [8/10], Batch [0/374], Loss: 0.4873\n",
      "2025-04-04 19:19:04 - INFO - Epoch [8/10], Batch [10/374], Loss: 0.3073\n",
      "2025-04-04 19:19:04 - INFO - Epoch [8/10], Batch [20/374], Loss: 0.4388\n",
      "2025-04-04 19:19:05 - INFO - Epoch [8/10], Batch [30/374], Loss: 0.2030\n",
      "2025-04-04 19:19:05 - INFO - Epoch [8/10], Batch [40/374], Loss: 0.4356\n",
      "2025-04-04 19:19:06 - INFO - Epoch [8/10], Batch [50/374], Loss: 0.2771\n",
      "2025-04-04 19:19:06 - INFO - Epoch [8/10], Batch [60/374], Loss: 0.3195\n",
      "2025-04-04 19:19:07 - INFO - Epoch [8/10], Batch [70/374], Loss: 0.3999\n",
      "2025-04-04 19:19:08 - INFO - Epoch [8/10], Batch [80/374], Loss: 0.1905\n",
      "2025-04-04 19:19:08 - INFO - Epoch [8/10], Batch [90/374], Loss: 0.1901\n",
      "2025-04-04 19:19:09 - INFO - Epoch [8/10], Batch [100/374], Loss: 0.4747\n",
      "2025-04-04 19:19:09 - INFO - Epoch [8/10], Batch [110/374], Loss: 0.2893\n",
      "2025-04-04 19:19:10 - INFO - Epoch [8/10], Batch [120/374], Loss: 0.2691\n",
      "2025-04-04 19:19:10 - INFO - Epoch [8/10], Batch [130/374], Loss: 0.2643\n",
      "2025-04-04 19:19:11 - INFO - Epoch [8/10], Batch [140/374], Loss: 0.3805\n",
      "2025-04-04 19:19:11 - INFO - Epoch [8/10], Batch [150/374], Loss: 0.4729\n",
      "2025-04-04 19:19:12 - INFO - Epoch [8/10], Batch [160/374], Loss: 0.7030\n",
      "2025-04-04 19:19:12 - INFO - Epoch [8/10], Batch [170/374], Loss: 0.3244\n",
      "2025-04-04 19:19:13 - INFO - Epoch [8/10], Batch [180/374], Loss: 0.5881\n",
      "2025-04-04 19:19:14 - INFO - Epoch [8/10], Batch [190/374], Loss: 0.3826\n",
      "2025-04-04 19:19:14 - INFO - Epoch [8/10], Batch [200/374], Loss: 0.3178\n",
      "2025-04-04 19:19:15 - INFO - Epoch [8/10], Batch [210/374], Loss: 0.4212\n",
      "2025-04-04 19:19:15 - INFO - Epoch [8/10], Batch [220/374], Loss: 0.2771\n",
      "2025-04-04 19:19:16 - INFO - Epoch [8/10], Batch [230/374], Loss: 0.2855\n",
      "2025-04-04 19:19:16 - INFO - Epoch [8/10], Batch [240/374], Loss: 0.2128\n",
      "2025-04-04 19:19:17 - INFO - Epoch [8/10], Batch [250/374], Loss: 0.2315\n",
      "2025-04-04 19:19:17 - INFO - Epoch [8/10], Batch [260/374], Loss: 0.3885\n",
      "2025-04-04 19:19:18 - INFO - Epoch [8/10], Batch [270/374], Loss: 0.2308\n",
      "2025-04-04 19:19:19 - INFO - Epoch [8/10], Batch [280/374], Loss: 0.5114\n",
      "2025-04-04 19:19:19 - INFO - Epoch [8/10], Batch [290/374], Loss: 0.3194\n",
      "2025-04-04 19:19:20 - INFO - Epoch [8/10], Batch [300/374], Loss: 0.3684\n",
      "2025-04-04 19:19:20 - INFO - Epoch [8/10], Batch [310/374], Loss: 0.2943\n",
      "2025-04-04 19:19:21 - INFO - Epoch [8/10], Batch [320/374], Loss: 0.5503\n",
      "2025-04-04 19:19:21 - INFO - Epoch [8/10], Batch [330/374], Loss: 0.2981\n",
      "2025-04-04 19:19:22 - INFO - Epoch [8/10], Batch [340/374], Loss: 0.3267\n",
      "2025-04-04 19:19:22 - INFO - Epoch [8/10], Batch [350/374], Loss: 0.3137\n",
      "2025-04-04 19:19:23 - INFO - Epoch [8/10], Batch [360/374], Loss: 0.5237\n",
      "2025-04-04 19:19:24 - INFO - Epoch [8/10], Batch [370/374], Loss: 0.3039\n",
      "2025-04-04 19:19:24 - INFO - Epoch [8/10] Train Loss: 0.3554, Train Accuracy: 83.72%\n",
      "2025-04-04 19:19:26 - INFO - Epoch [8/10] Val Loss: 0.2925, Val Accuracy: 87.75%\n",
      "2025-04-04 19:19:26 - INFO - New best model at epoch 8 with val accuracy: 87.75%\n",
      "2025-04-04 19:19:26 - INFO - Epoch [9/10], Batch [0/374], Loss: 0.4055\n",
      "2025-04-04 19:19:26 - INFO - Epoch [9/10], Batch [10/374], Loss: 0.4911\n",
      "2025-04-04 19:19:27 - INFO - Epoch [9/10], Batch [20/374], Loss: 0.2896\n",
      "2025-04-04 19:19:27 - INFO - Epoch [9/10], Batch [30/374], Loss: 0.3409\n",
      "2025-04-04 19:19:28 - INFO - Epoch [9/10], Batch [40/374], Loss: 0.3352\n",
      "2025-04-04 19:19:29 - INFO - Epoch [9/10], Batch [50/374], Loss: 0.4890\n",
      "2025-04-04 19:19:29 - INFO - Epoch [9/10], Batch [60/374], Loss: 0.5499\n",
      "2025-04-04 19:19:30 - INFO - Epoch [9/10], Batch [70/374], Loss: 0.2842\n",
      "2025-04-04 19:19:30 - INFO - Epoch [9/10], Batch [80/374], Loss: 0.3213\n",
      "2025-04-04 19:19:31 - INFO - Epoch [9/10], Batch [90/374], Loss: 0.4632\n",
      "2025-04-04 19:19:31 - INFO - Epoch [9/10], Batch [100/374], Loss: 0.2991\n",
      "2025-04-04 19:19:32 - INFO - Epoch [9/10], Batch [110/374], Loss: 0.2561\n",
      "2025-04-04 19:19:32 - INFO - Epoch [9/10], Batch [120/374], Loss: 0.2359\n",
      "2025-04-04 19:19:33 - INFO - Epoch [9/10], Batch [130/374], Loss: 0.2816\n",
      "2025-04-04 19:19:34 - INFO - Epoch [9/10], Batch [140/374], Loss: 0.3712\n",
      "2025-04-04 19:19:34 - INFO - Epoch [9/10], Batch [150/374], Loss: 0.2863\n",
      "2025-04-04 19:19:35 - INFO - Epoch [9/10], Batch [160/374], Loss: 0.2907\n",
      "2025-04-04 19:19:35 - INFO - Epoch [9/10], Batch [170/374], Loss: 0.1721\n",
      "2025-04-04 19:19:36 - INFO - Epoch [9/10], Batch [180/374], Loss: 0.4686\n",
      "2025-04-04 19:19:36 - INFO - Epoch [9/10], Batch [190/374], Loss: 0.3981\n",
      "2025-04-04 19:19:37 - INFO - Epoch [9/10], Batch [200/374], Loss: 0.2532\n",
      "2025-04-04 19:19:37 - INFO - Epoch [9/10], Batch [210/374], Loss: 0.4182\n",
      "2025-04-04 19:19:38 - INFO - Epoch [9/10], Batch [220/374], Loss: 0.3196\n",
      "2025-04-04 19:19:38 - INFO - Epoch [9/10], Batch [230/374], Loss: 0.2064\n",
      "2025-04-04 19:19:39 - INFO - Epoch [9/10], Batch [240/374], Loss: 0.2864\n",
      "2025-04-04 19:19:40 - INFO - Epoch [9/10], Batch [250/374], Loss: 0.2043\n",
      "2025-04-04 19:19:40 - INFO - Epoch [9/10], Batch [260/374], Loss: 0.2038\n",
      "2025-04-04 19:19:41 - INFO - Epoch [9/10], Batch [270/374], Loss: 0.2070\n",
      "2025-04-04 19:19:41 - INFO - Epoch [9/10], Batch [280/374], Loss: 0.3222\n",
      "2025-04-04 19:19:42 - INFO - Epoch [9/10], Batch [290/374], Loss: 0.3686\n",
      "2025-04-04 19:19:42 - INFO - Epoch [9/10], Batch [300/374], Loss: 0.3119\n",
      "2025-04-04 19:19:43 - INFO - Epoch [9/10], Batch [310/374], Loss: 0.3564\n",
      "2025-04-04 19:19:43 - INFO - Epoch [9/10], Batch [320/374], Loss: 0.4439\n",
      "2025-04-04 19:19:44 - INFO - Epoch [9/10], Batch [330/374], Loss: 0.3465\n",
      "2025-04-04 19:19:45 - INFO - Epoch [9/10], Batch [340/374], Loss: 0.3134\n",
      "2025-04-04 19:19:45 - INFO - Epoch [9/10], Batch [350/374], Loss: 0.5149\n",
      "2025-04-04 19:19:46 - INFO - Epoch [9/10], Batch [360/374], Loss: 0.2474\n",
      "2025-04-04 19:19:46 - INFO - Epoch [9/10], Batch [370/374], Loss: 0.2164\n",
      "2025-04-04 19:19:46 - INFO - Epoch [9/10] Train Loss: 0.3389, Train Accuracy: 84.38%\n",
      "2025-04-04 19:19:48 - INFO - Epoch [9/10] Val Loss: 0.2775, Val Accuracy: 88.45%\n",
      "2025-04-04 19:19:48 - INFO - New best model at epoch 9 with val accuracy: 88.45%\n",
      "2025-04-04 19:19:48 - INFO - Epoch [10/10], Batch [0/374], Loss: 0.3023\n",
      "2025-04-04 19:19:49 - INFO - Epoch [10/10], Batch [10/374], Loss: 0.4015\n",
      "2025-04-04 19:19:50 - INFO - Epoch [10/10], Batch [20/374], Loss: 0.4259\n",
      "2025-04-04 19:19:50 - INFO - Epoch [10/10], Batch [30/374], Loss: 0.4733\n",
      "2025-04-04 19:19:51 - INFO - Epoch [10/10], Batch [40/374], Loss: 0.2009\n",
      "2025-04-04 19:19:51 - INFO - Epoch [10/10], Batch [50/374], Loss: 0.2471\n",
      "2025-04-04 19:19:52 - INFO - Epoch [10/10], Batch [60/374], Loss: 0.2240\n",
      "2025-04-04 19:19:52 - INFO - Epoch [10/10], Batch [70/374], Loss: 0.3332\n",
      "2025-04-04 19:19:53 - INFO - Epoch [10/10], Batch [80/374], Loss: 0.3168\n",
      "2025-04-04 19:19:53 - INFO - Epoch [10/10], Batch [90/374], Loss: 0.2712\n",
      "2025-04-04 19:19:54 - INFO - Epoch [10/10], Batch [100/374], Loss: 0.3598\n",
      "2025-04-04 19:19:54 - INFO - Epoch [10/10], Batch [110/374], Loss: 0.1947\n",
      "2025-04-04 19:19:55 - INFO - Epoch [10/10], Batch [120/374], Loss: 0.3183\n",
      "2025-04-04 19:19:56 - INFO - Epoch [10/10], Batch [130/374], Loss: 0.2667\n",
      "2025-04-04 19:19:56 - INFO - Epoch [10/10], Batch [140/374], Loss: 0.2585\n",
      "2025-04-04 19:19:57 - INFO - Epoch [10/10], Batch [150/374], Loss: 0.3255\n",
      "2025-04-04 19:19:57 - INFO - Epoch [10/10], Batch [160/374], Loss: 0.2055\n",
      "2025-04-04 19:19:58 - INFO - Epoch [10/10], Batch [170/374], Loss: 0.2805\n",
      "2025-04-04 19:19:58 - INFO - Epoch [10/10], Batch [180/374], Loss: 0.4021\n",
      "2025-04-04 19:19:59 - INFO - Epoch [10/10], Batch [190/374], Loss: 0.4502\n",
      "2025-04-04 19:19:59 - INFO - Epoch [10/10], Batch [200/374], Loss: 0.2261\n",
      "2025-04-04 19:20:00 - INFO - Epoch [10/10], Batch [210/374], Loss: 0.2723\n",
      "2025-04-04 19:20:01 - INFO - Epoch [10/10], Batch [220/374], Loss: 0.3239\n",
      "2025-04-04 19:20:01 - INFO - Epoch [10/10], Batch [230/374], Loss: 0.2121\n",
      "2025-04-04 19:20:02 - INFO - Epoch [10/10], Batch [240/374], Loss: 0.3080\n",
      "2025-04-04 19:20:02 - INFO - Epoch [10/10], Batch [250/374], Loss: 0.3616\n",
      "2025-04-04 19:20:03 - INFO - Epoch [10/10], Batch [260/374], Loss: 0.3489\n",
      "2025-04-04 19:20:03 - INFO - Epoch [10/10], Batch [270/374], Loss: 0.2781\n",
      "2025-04-04 19:20:04 - INFO - Epoch [10/10], Batch [280/374], Loss: 0.3498\n",
      "2025-04-04 19:20:04 - INFO - Epoch [10/10], Batch [290/374], Loss: 0.2221\n",
      "2025-04-04 19:20:05 - INFO - Epoch [10/10], Batch [300/374], Loss: 0.2924\n",
      "2025-04-04 19:20:05 - INFO - Epoch [10/10], Batch [310/374], Loss: 0.3035\n",
      "2025-04-04 19:20:06 - INFO - Epoch [10/10], Batch [320/374], Loss: 0.2633\n",
      "2025-04-04 19:20:07 - INFO - Epoch [10/10], Batch [330/374], Loss: 0.5155\n",
      "2025-04-04 19:20:07 - INFO - Epoch [10/10], Batch [340/374], Loss: 0.2642\n",
      "2025-04-04 19:20:08 - INFO - Epoch [10/10], Batch [350/374], Loss: 0.2711\n",
      "2025-04-04 19:20:08 - INFO - Epoch [10/10], Batch [360/374], Loss: 0.3935\n",
      "2025-04-04 19:20:09 - INFO - Epoch [10/10], Batch [370/374], Loss: 0.3482\n",
      "2025-04-04 19:20:09 - INFO - Epoch [10/10] Train Loss: 0.3307, Train Accuracy: 85.18%\n",
      "2025-04-04 19:20:11 - INFO - Epoch [10/10] Val Loss: 0.2711, Val Accuracy: 88.52%\n",
      "2025-04-04 19:20:11 - INFO - New best model at epoch 10 with val accuracy: 88.52%\n",
      "2025-04-04 19:20:12 - INFO - Test Loss: 0.2730, Test Accuracy: 88.94%\n",
      "2025-04-04 19:20:12 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-04 19:20:12 - INFO - {\n",
      "    \"world_size\": 2,\n",
      "    \"train_time\": 229.06645798683167,\n",
      "    \"avg_epoch_time\": 22.906645798683165,\n",
      "    \"val_accuracy\": 88.51774530271399,\n",
      "    \"test_accuracy\": 88.93528183716076,\n",
      "    \"test_loss\": 0.2730251305536338,\n",
      "    \"total_time\": 229.06645798683167,\n",
      "    \"train_losses\": [\n",
      "        0.646209762000431,\n",
      "        0.5918380264153849,\n",
      "        0.5161895690323249,\n",
      "        0.45677403733664274,\n",
      "        0.4217156030157239,\n",
      "        0.3937545166233911,\n",
      "        0.3760223412515728,\n",
      "        0.35538855267873976,\n",
      "        0.3389298340742314,\n",
      "        0.33066876846054566\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        61.33567662565905,\n",
      "        68.35718470164868,\n",
      "        73.84718386475856,\n",
      "        78.03163444639719,\n",
      "        80.29123776048205,\n",
      "        81.73905766172902,\n",
      "        82.7014812955059,\n",
      "        83.72248723742572,\n",
      "        84.37526152816135,\n",
      "        85.17867603983596\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.5795643954286993,\n",
      "        0.5189103529780128,\n",
      "        0.43757783877161033,\n",
      "        0.39708778678997575,\n",
      "        0.39177452568055526,\n",
      "        0.3305899766309442,\n",
      "        0.31320607923515653,\n",
      "        0.2924645213344152,\n",
      "        0.2774940743243935,\n",
      "        0.2710984706173527\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        72.79053583855254,\n",
      "        76.96590118302018,\n",
      "        80.1670146137787,\n",
      "        83.05497564370216,\n",
      "        82.5678496868476,\n",
      "        86.1169102296451,\n",
      "        86.49965205288797,\n",
      "        87.75226165622826,\n",
      "        88.44815588030619,\n",
      "        88.51774530271399\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-04 19:20:12 - INFO - Model saved to models/training_using_gpus_amp_2_best_model.pt\n",
      "2025-04-04 19:20:14 - INFO - Loss plot saved as plots/training_using_gpus_amp_2_loss.png\n",
      "2025-04-04 19:20:14 - INFO - Accuracy plot saved as plots/training_using_gpus_amp_2_accuracy.png\n",
      "2025-04-04 19:20:14 - INFO - Runtime parameters saved as metrics/training_using_gpus_amp_2_params.json\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308298b0-3bb2-41ad-b373-97f74cc1d0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0404 19:20:23.492000 1093396 site-packages/torch/distributed/run.py:793] \n",
      "W0404 19:20:23.492000 1093396 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0404 19:20:23.492000 1093396 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0404 19:20:23.492000 1093396 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "2025-04-04 19:20:31 - INFO - Running on node: d1004\n",
      "2025-04-04 19:20:31 - INFO - Running on node: d1004\n",
      "2025-04-04 19:20:31 - INFO - Running on node: d1004\n",
      "2025-04-04 19:20:31 - INFO - Running with world_size - GPU with AMP: 4 (Rank: 0)\n",
      "2025-04-04 19:20:31 - INFO - Running with world_size - GPU with AMP: 4 (Rank: 1)\n",
      "2025-04-04 19:20:31 - INFO - Running with world_size - GPU with AMP: 4 (Rank: 2)\n",
      "2025-04-04 19:20:31 - INFO - Running on node: d1004\n",
      "2025-04-04 19:20:31 - INFO - Running with world_size - GPU with AMP: 4 (Rank: 3)\n",
      "2025-04-04 19:20:31 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 19:20:31 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 19:20:31 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 19:20:31 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 19:24:02 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-04 19:24:02 - INFO - [Rank 2] Data loaded.\n",
      "2025-04-04 19:24:02 - INFO - [Rank 3] Data loaded.\n",
      "2025-04-04 19:24:02 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-04 19:24:06 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-04 19:24:06 - INFO - Total layers: 129\n",
      "2025-04-04 19:24:06 - INFO - Total parameters: 22,494,274\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:139: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:139: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:139: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:139: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "/home/ramasamypandiaraj.r/ProjectNotebooks/ParallelProcessing/gpus_with_DDP_AMP/ddp_train.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 19:24:10 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6684\n",
      "2025-04-04 19:24:11 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.7431\n",
      "2025-04-04 19:24:12 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.5983\n",
      "2025-04-04 19:24:13 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.6318\n",
      "2025-04-04 19:24:14 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.6287\n",
      "2025-04-04 19:24:14 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6243\n",
      "2025-04-04 19:24:16 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.6058\n",
      "2025-04-04 19:24:16 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.6629\n",
      "2025-04-04 19:24:17 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.6281\n",
      "2025-04-04 19:24:18 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.7343\n",
      "2025-04-04 19:24:19 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.6491\n",
      "2025-04-04 19:24:20 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.5439\n",
      "2025-04-04 19:24:21 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6303\n",
      "2025-04-04 19:24:22 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.5866\n",
      "2025-04-04 19:24:23 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5625\n",
      "2025-04-04 19:24:24 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.6420\n",
      "2025-04-04 19:24:25 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.6126\n",
      "2025-04-04 19:24:26 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5863\n",
      "2025-04-04 19:24:27 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.6629\n",
      "2025-04-04 19:24:28 - INFO - Epoch [1/10] Train Loss: 0.6413, Train Accuracy: 60.65%\n",
      "2025-04-04 19:24:30 - INFO - Epoch [1/10] Val Loss: 0.6277, Val Accuracy: 68.96%\n",
      "2025-04-04 19:24:30 - INFO - New best model at epoch 1 with val accuracy: 68.96%\n",
      "2025-04-04 19:24:31 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.6028\n",
      "2025-04-04 19:24:32 - INFO - Epoch [2/10], Batch [10/187], Loss: 0.5523\n",
      "2025-04-04 19:24:33 - INFO - Epoch [2/10], Batch [20/187], Loss: 0.5970\n",
      "2025-04-04 19:24:33 - INFO - Epoch [2/10], Batch [30/187], Loss: 0.8121\n",
      "2025-04-04 19:24:34 - INFO - Epoch [2/10], Batch [40/187], Loss: 0.5612\n",
      "2025-04-04 19:24:36 - INFO - Epoch [2/10], Batch [50/187], Loss: 0.5352\n",
      "2025-04-04 19:24:36 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.5381\n",
      "2025-04-04 19:24:37 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.6234\n",
      "2025-04-04 19:24:38 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.5969\n",
      "2025-04-04 19:24:39 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.6877\n",
      "2025-04-04 19:24:40 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.8033\n",
      "2025-04-04 19:24:41 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.6114\n",
      "2025-04-04 19:24:42 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.6905\n",
      "2025-04-04 19:24:43 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.6939\n",
      "2025-04-04 19:24:44 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.5995\n",
      "2025-04-04 19:24:45 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.6319\n",
      "2025-04-04 19:24:46 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.5594\n",
      "2025-04-04 19:24:47 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.5167\n",
      "2025-04-04 19:24:48 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.6686\n",
      "2025-04-04 19:24:48 - INFO - Epoch [2/10] Train Loss: 0.6148, Train Accuracy: 64.99%\n",
      "2025-04-04 19:24:50 - INFO - Epoch [2/10] Val Loss: 0.5629, Val Accuracy: 72.58%\n",
      "2025-04-04 19:24:50 - INFO - New best model at epoch 2 with val accuracy: 72.58%\n",
      "2025-04-04 19:24:50 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.7859\n",
      "2025-04-04 19:24:51 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.5321\n",
      "2025-04-04 19:24:52 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.5092\n",
      "2025-04-04 19:24:53 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.6497\n",
      "2025-04-04 19:24:54 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.5762\n",
      "2025-04-04 19:24:55 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.7298\n",
      "2025-04-04 19:24:56 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.6563\n",
      "2025-04-04 19:24:57 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.5399\n",
      "2025-04-04 19:24:58 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.5937\n",
      "2025-04-04 19:24:59 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.5435\n",
      "2025-04-04 19:25:00 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.5828\n",
      "2025-04-04 19:25:01 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.6362\n",
      "2025-04-04 19:25:02 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.5544\n",
      "2025-04-04 19:25:02 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5605\n",
      "2025-04-04 19:25:03 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.5651\n",
      "2025-04-04 19:25:04 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.5140\n",
      "2025-04-04 19:25:05 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.4041\n",
      "2025-04-04 19:25:06 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.6484\n",
      "2025-04-04 19:25:07 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.5235\n",
      "2025-04-04 19:25:08 - INFO - Epoch [3/10] Train Loss: 0.5751, Train Accuracy: 68.05%\n",
      "2025-04-04 19:25:09 - INFO - Epoch [3/10] Val Loss: 0.5494, Val Accuracy: 73.63%\n",
      "2025-04-04 19:25:09 - INFO - New best model at epoch 3 with val accuracy: 73.63%\n",
      "2025-04-04 19:25:09 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.6555\n",
      "2025-04-04 19:25:10 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.4173\n",
      "2025-04-04 19:25:11 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.5113\n",
      "2025-04-04 19:25:12 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.5313\n",
      "2025-04-04 19:25:13 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.6502\n",
      "2025-04-04 19:25:14 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.5889\n",
      "2025-04-04 19:25:15 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.4873\n",
      "2025-04-04 19:25:16 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.4868\n",
      "2025-04-04 19:25:17 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.4889\n",
      "2025-04-04 19:25:18 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.6044\n",
      "2025-04-04 19:25:19 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4859\n",
      "2025-04-04 19:25:20 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.5345\n",
      "2025-04-04 19:25:21 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.4952\n",
      "2025-04-04 19:25:22 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.5870\n",
      "2025-04-04 19:25:23 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.5059\n",
      "2025-04-04 19:25:24 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.4685\n",
      "2025-04-04 19:25:25 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.5149\n",
      "2025-04-04 19:25:26 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.5154\n",
      "2025-04-04 19:25:27 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.5170\n",
      "2025-04-04 19:25:27 - INFO - Epoch [4/10] Train Loss: 0.5388, Train Accuracy: 71.58%\n",
      "2025-04-04 19:25:28 - INFO - Epoch [4/10] Val Loss: 0.6069, Val Accuracy: 74.39%\n",
      "2025-04-04 19:25:28 - INFO - New best model at epoch 4 with val accuracy: 74.39%\n",
      "2025-04-04 19:25:29 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.5083\n",
      "2025-04-04 19:25:30 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.5626\n",
      "2025-04-04 19:25:31 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.4758\n",
      "2025-04-04 19:25:32 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.6766\n",
      "2025-04-04 19:25:33 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.5549\n",
      "2025-04-04 19:25:34 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.4043\n",
      "2025-04-04 19:25:35 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.5331\n",
      "2025-04-04 19:25:36 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.3780\n",
      "2025-04-04 19:25:37 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.4159\n",
      "2025-04-04 19:25:38 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.5479\n",
      "2025-04-04 19:25:38 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.5499\n",
      "2025-04-04 19:25:39 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3239\n",
      "2025-04-04 19:25:40 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.4465\n",
      "2025-04-04 19:25:41 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.3356\n",
      "2025-04-04 19:25:42 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.4420\n",
      "2025-04-04 19:25:43 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.5626\n",
      "2025-04-04 19:25:44 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.4265\n",
      "2025-04-04 19:25:45 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.4245\n",
      "2025-04-04 19:25:46 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.5865\n",
      "2025-04-04 19:25:46 - INFO - Epoch [5/10] Train Loss: 0.4883, Train Accuracy: 75.80%\n",
      "2025-04-04 19:25:48 - INFO - Epoch [5/10] Val Loss: 0.4431, Val Accuracy: 79.68%\n",
      "2025-04-04 19:25:48 - INFO - New best model at epoch 5 with val accuracy: 79.68%\n",
      "2025-04-04 19:25:48 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.4929\n",
      "2025-04-04 19:25:49 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.5173\n",
      "2025-04-04 19:25:50 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.5964\n",
      "2025-04-04 19:25:51 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.4785\n",
      "2025-04-04 19:25:52 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2968\n",
      "2025-04-04 19:25:53 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.6646\n",
      "2025-04-04 19:25:54 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.4787\n",
      "2025-04-04 19:25:55 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.7047\n",
      "2025-04-04 19:25:56 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.3362\n",
      "2025-04-04 19:25:57 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.3640\n",
      "2025-04-04 19:25:58 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.5371\n",
      "2025-04-04 19:25:59 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2925\n",
      "2025-04-04 19:26:00 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.7676\n",
      "2025-04-04 19:26:01 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.3861\n",
      "2025-04-04 19:26:02 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.3596\n",
      "2025-04-04 19:26:03 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.4346\n",
      "2025-04-04 19:26:04 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.3958\n",
      "2025-04-04 19:26:04 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.4914\n",
      "2025-04-04 19:26:05 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.3309\n",
      "2025-04-04 19:26:06 - INFO - Epoch [6/10] Train Loss: 0.4348, Train Accuracy: 78.49%\n",
      "2025-04-04 19:26:07 - INFO - Epoch [6/10] Val Loss: 0.3752, Val Accuracy: 84.27%\n",
      "2025-04-04 19:26:07 - INFO - New best model at epoch 6 with val accuracy: 84.27%\n",
      "2025-04-04 19:26:08 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.3098\n",
      "2025-04-04 19:26:09 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.4779\n",
      "2025-04-04 19:26:09 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.5600\n",
      "2025-04-04 19:26:10 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.3226\n",
      "2025-04-04 19:26:11 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.3808\n",
      "2025-04-04 19:26:12 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.3940\n",
      "2025-04-04 19:26:13 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.5489\n",
      "2025-04-04 19:26:14 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.4625\n",
      "2025-04-04 19:26:15 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.3232\n",
      "2025-04-04 19:26:16 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.2354\n",
      "2025-04-04 19:26:17 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.5066\n",
      "2025-04-04 19:26:18 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.4296\n",
      "2025-04-04 19:26:19 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.4242\n",
      "2025-04-04 19:26:20 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.3691\n",
      "2025-04-04 19:26:21 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.5774\n",
      "2025-04-04 19:26:22 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.5249\n",
      "2025-04-04 19:26:23 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.3640\n",
      "2025-04-04 19:26:24 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.5013\n",
      "2025-04-04 19:26:25 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2501\n",
      "2025-04-04 19:26:25 - INFO - Epoch [7/10] Train Loss: 0.4109, Train Accuracy: 80.35%\n",
      "2025-04-04 19:26:26 - INFO - Epoch [7/10] Val Loss: 0.3485, Val Accuracy: 85.39%\n",
      "2025-04-04 19:26:26 - INFO - New best model at epoch 7 with val accuracy: 85.39%\n",
      "2025-04-04 19:26:27 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.4836\n",
      "2025-04-04 19:26:28 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.4508\n",
      "2025-04-04 19:26:29 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.4691\n",
      "2025-04-04 19:26:30 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.2835\n",
      "2025-04-04 19:26:31 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.3178\n",
      "2025-04-04 19:26:32 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2775\n",
      "2025-04-04 19:26:33 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.1771\n",
      "2025-04-04 19:26:33 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.4739\n",
      "2025-04-04 19:26:34 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.5345\n",
      "2025-04-04 19:26:35 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.4281\n",
      "2025-04-04 19:26:36 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.4954\n",
      "2025-04-04 19:26:37 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.2614\n",
      "2025-04-04 19:26:38 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.2470\n",
      "2025-04-04 19:26:39 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.3342\n",
      "2025-04-04 19:26:40 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.3666\n",
      "2025-04-04 19:26:41 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.4119\n",
      "2025-04-04 19:26:42 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.3681\n",
      "2025-04-04 19:26:43 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3916\n",
      "2025-04-04 19:26:44 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.2754\n",
      "2025-04-04 19:26:44 - INFO - Epoch [8/10] Train Loss: 0.3779, Train Accuracy: 82.83%\n",
      "2025-04-04 19:26:45 - INFO - Epoch [8/10] Val Loss: 0.3112, Val Accuracy: 86.99%\n",
      "2025-04-04 19:26:45 - INFO - New best model at epoch 8 with val accuracy: 86.99%\n",
      "2025-04-04 19:26:46 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.2852\n",
      "2025-04-04 19:26:47 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.3787\n",
      "2025-04-04 19:26:48 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.2495\n",
      "2025-04-04 19:26:49 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.4207\n",
      "2025-04-04 19:26:50 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.2840\n",
      "2025-04-04 19:26:51 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2414\n",
      "2025-04-04 19:26:52 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.2109\n",
      "2025-04-04 19:26:53 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2707\n",
      "2025-04-04 19:26:53 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.4037\n",
      "2025-04-04 19:26:55 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.3468\n",
      "2025-04-04 19:26:56 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4148\n",
      "2025-04-04 19:26:56 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.2126\n",
      "2025-04-04 19:26:57 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.3045\n",
      "2025-04-04 19:26:58 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2160\n",
      "2025-04-04 19:26:59 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.4009\n",
      "2025-04-04 19:27:00 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.2770\n",
      "2025-04-04 19:27:01 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.3857\n",
      "2025-04-04 19:27:02 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.2956\n",
      "2025-04-04 19:27:03 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.3238\n",
      "2025-04-04 19:27:03 - INFO - Epoch [9/10] Train Loss: 0.3609, Train Accuracy: 83.30%\n",
      "2025-04-04 19:27:05 - INFO - Epoch [9/10] Val Loss: 0.2932, Val Accuracy: 87.75%\n",
      "2025-04-04 19:27:05 - INFO - New best model at epoch 9 with val accuracy: 87.75%\n",
      "2025-04-04 19:27:05 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.4488\n",
      "2025-04-04 19:27:06 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.4538\n",
      "2025-04-04 19:27:07 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.3673\n",
      "2025-04-04 19:27:08 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.2921\n",
      "2025-04-04 19:27:09 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.4405\n",
      "2025-04-04 19:27:10 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2635\n",
      "2025-04-04 19:27:11 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2347\n",
      "2025-04-04 19:27:12 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.3454\n",
      "2025-04-04 19:27:13 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.4275\n",
      "2025-04-04 19:27:14 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.3825\n",
      "2025-04-04 19:27:15 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.2146\n",
      "2025-04-04 19:27:16 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.3745\n",
      "2025-04-04 19:27:17 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.2842\n",
      "2025-04-04 19:27:18 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.3236\n",
      "2025-04-04 19:27:19 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.5838\n",
      "2025-04-04 19:27:19 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.4302\n",
      "2025-04-04 19:27:20 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.2380\n",
      "2025-04-04 19:27:21 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.4718\n",
      "2025-04-04 19:27:22 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.3308\n",
      "2025-04-04 19:27:22 - INFO - Epoch [10/10] Train Loss: 0.3493, Train Accuracy: 83.78%\n",
      "2025-04-04 19:27:24 - INFO - Epoch [10/10] Val Loss: 0.2829, Val Accuracy: 88.17%\n",
      "2025-04-04 19:27:24 - INFO - New best model at epoch 10 with val accuracy: 88.17%\n",
      "2025-04-04 19:27:26 - INFO - Test Loss: 0.2995, Test Accuracy: 87.34%\n",
      "2025-04-04 19:27:26 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-04 19:27:26 - INFO - {\n",
      "    \"world_size\": 4,\n",
      "    \"train_time\": 197.83912801742554,\n",
      "    \"avg_epoch_time\": 19.783912801742552,\n",
      "    \"val_accuracy\": 88.16979819067502,\n",
      "    \"test_accuracy\": 87.34353268428373,\n",
      "    \"test_loss\": 0.29947710567787394,\n",
      "    \"total_time\": 197.83912801742554,\n",
      "    \"train_losses\": [\n",
      "        0.6412518679147984,\n",
      "        0.6148269312950357,\n",
      "        0.5751397699042843,\n",
      "        0.5387962841738217,\n",
      "        0.48829011147989887,\n",
      "        0.4347505896560318,\n",
      "        0.4109335389646027,\n",
      "        0.3779095295832247,\n",
      "        0.36091915325140855,\n",
      "        0.3492699241812758\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        60.65271966527197,\n",
      "        64.98744769874477,\n",
      "        68.05020920502092,\n",
      "        71.581589958159,\n",
      "        75.79916317991632,\n",
      "        78.49372384937239,\n",
      "        80.35146443514644,\n",
      "        82.82845188284519,\n",
      "        83.2970711297071,\n",
      "        83.78242677824268\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.627695694762796,\n",
      "        0.5629490279620442,\n",
      "        0.5494283037046301,\n",
      "        0.6069209627916021,\n",
      "        0.44306428090681527,\n",
      "        0.37524790770491545,\n",
      "        0.34848237867229254,\n",
      "        0.31116104349960616,\n",
      "        0.29317161426331817,\n",
      "        0.28288789557016963\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        68.96311760612387,\n",
      "        72.58176757132915,\n",
      "        73.62560890744606,\n",
      "        74.3910925539318,\n",
      "        79.67988865692415,\n",
      "        84.27279053583855,\n",
      "        85.38622129436325,\n",
      "        86.98677800974252,\n",
      "        87.75226165622826,\n",
      "        88.16979819067502\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-04 19:27:26 - INFO - Model saved to models/training_using_gpus_amp_4_best_model.pt\n",
      "2025-04-04 19:27:29 - INFO - Loss plot saved as plots/training_using_gpus_amp_4_loss.png\n",
      "2025-04-04 19:27:29 - INFO - Accuracy plot saved as plots/training_using_gpus_amp_4_accuracy.png\n",
      "2025-04-04 19:27:29 - INFO - Runtime parameters saved as metrics/training_using_gpus_amp_4_params.json\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=4 main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
