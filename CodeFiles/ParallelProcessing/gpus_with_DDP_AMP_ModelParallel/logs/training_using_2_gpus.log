2025-04-04 20:03:30 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...
2025-04-04 20:03:30 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...
2025-04-04 20:06:57 - INFO - [Rank 1] Data loaded.
2025-04-04 20:06:57 - INFO - [Rank 0] Data loaded.
2025-04-04 20:07:00 - INFO - Model architecture: MedicalCNN
2025-04-04 20:07:00 - INFO - Total layers: 129
2025-04-04 20:07:00 - INFO - Total parameters: 22,494,274
2025-04-04 20:07:02 - INFO - Epoch [1/10], Batch [0/374], Loss: 0.6838
2025-04-04 20:07:02 - INFO - Epoch [1/10], Batch [10/374], Loss: 0.7080
2025-04-04 20:07:03 - INFO - Epoch [1/10], Batch [20/374], Loss: 0.6697
2025-04-04 20:07:04 - INFO - Epoch [1/10], Batch [30/374], Loss: 0.6936
2025-04-04 20:07:05 - INFO - Epoch [1/10], Batch [40/374], Loss: 0.6248
2025-04-04 20:07:05 - INFO - Epoch [1/10], Batch [50/374], Loss: 0.6716
2025-04-04 20:07:06 - INFO - Epoch [1/10], Batch [60/374], Loss: 0.6268
2025-04-04 20:07:07 - INFO - Epoch [1/10], Batch [70/374], Loss: 0.6815
2025-04-04 20:07:07 - INFO - Epoch [1/10], Batch [80/374], Loss: 0.6101
2025-04-04 20:07:08 - INFO - Epoch [1/10], Batch [90/374], Loss: 0.6993
2025-04-04 20:07:09 - INFO - Epoch [1/10], Batch [100/374], Loss: 0.6989
2025-04-04 20:07:09 - INFO - Epoch [1/10], Batch [110/374], Loss: 0.5763
2025-04-04 20:07:10 - INFO - Epoch [1/10], Batch [120/374], Loss: 0.5612
2025-04-04 20:07:11 - INFO - Epoch [1/10], Batch [130/374], Loss: 0.5974
2025-04-04 20:07:11 - INFO - Epoch [1/10], Batch [140/374], Loss: 0.6731
2025-04-04 20:07:12 - INFO - Epoch [1/10], Batch [150/374], Loss: 0.7900
2025-04-04 20:07:13 - INFO - Epoch [1/10], Batch [160/374], Loss: 0.5967
2025-04-04 20:07:13 - INFO - Epoch [1/10], Batch [170/374], Loss: 0.6076
2025-04-04 20:07:14 - INFO - Epoch [1/10], Batch [180/374], Loss: 0.6135
2025-04-04 20:07:15 - INFO - Epoch [1/10], Batch [190/374], Loss: 0.7444
2025-04-04 20:07:15 - INFO - Epoch [1/10], Batch [200/374], Loss: 0.7502
2025-04-04 20:07:16 - INFO - Epoch [1/10], Batch [210/374], Loss: 0.7212
2025-04-04 20:07:17 - INFO - Epoch [1/10], Batch [220/374], Loss: 0.6360
2025-04-04 20:07:17 - INFO - Epoch [1/10], Batch [230/374], Loss: 0.5847
2025-04-04 20:07:18 - INFO - Epoch [1/10], Batch [240/374], Loss: 0.6184
2025-04-04 20:07:19 - INFO - Epoch [1/10], Batch [250/374], Loss: 0.5642
2025-04-04 20:07:19 - INFO - Epoch [1/10], Batch [260/374], Loss: 0.5777
2025-04-04 20:07:20 - INFO - Epoch [1/10], Batch [270/374], Loss: 0.5440
2025-04-04 20:07:21 - INFO - Epoch [1/10], Batch [280/374], Loss: 0.5233
2025-04-04 20:07:21 - INFO - Epoch [1/10], Batch [290/374], Loss: 0.5324
2025-04-04 20:07:22 - INFO - Epoch [1/10], Batch [300/374], Loss: 0.5455
2025-04-04 20:07:23 - INFO - Epoch [1/10], Batch [310/374], Loss: 0.6937
2025-04-04 20:07:23 - INFO - Epoch [1/10], Batch [320/374], Loss: 0.7009
2025-04-04 20:07:24 - INFO - Epoch [1/10], Batch [330/374], Loss: 0.6862
2025-04-04 20:07:25 - INFO - Epoch [1/10], Batch [340/374], Loss: 0.5326
2025-04-04 20:07:25 - INFO - Epoch [1/10], Batch [350/374], Loss: 0.7209
2025-04-04 20:07:26 - INFO - Epoch [1/10], Batch [360/374], Loss: 0.6879
2025-04-04 20:07:27 - INFO - Epoch [1/10], Batch [370/374], Loss: 0.5562
2025-04-04 20:07:27 - INFO - Epoch [1/10] Train Loss: 0.6354, Train Accuracy: 61.26%
2025-04-04 20:07:30 - INFO - Epoch [1/10] Val Loss: 0.5808, Val Accuracy: 72.79%
2025-04-04 20:07:30 - INFO - New best model at epoch 1 with val accuracy: 72.79%
2025-04-04 20:07:30 - INFO - Epoch [2/10], Batch [0/374], Loss: 0.6766
2025-04-04 20:07:31 - INFO - Epoch [2/10], Batch [10/374], Loss: 0.5300
2025-04-04 20:07:31 - INFO - Epoch [2/10], Batch [20/374], Loss: 0.5141
2025-04-04 20:07:32 - INFO - Epoch [2/10], Batch [30/374], Loss: 0.5314
2025-04-04 20:07:33 - INFO - Epoch [2/10], Batch [40/374], Loss: 0.8331
2025-04-04 20:07:33 - INFO - Epoch [2/10], Batch [50/374], Loss: 0.5443
2025-04-04 20:07:34 - INFO - Epoch [2/10], Batch [60/374], Loss: 0.8355
2025-04-04 20:07:35 - INFO - Epoch [2/10], Batch [70/374], Loss: 0.6691
2025-04-04 20:07:35 - INFO - Epoch [2/10], Batch [80/374], Loss: 0.5280
2025-04-04 20:07:36 - INFO - Epoch [2/10], Batch [90/374], Loss: 0.6082
2025-04-04 20:07:37 - INFO - Epoch [2/10], Batch [100/374], Loss: 0.5406
2025-04-04 20:07:37 - INFO - Epoch [2/10], Batch [110/374], Loss: 0.6845
2025-04-04 20:07:38 - INFO - Epoch [2/10], Batch [120/374], Loss: 0.4416
2025-04-04 20:07:39 - INFO - Epoch [2/10], Batch [130/374], Loss: 0.5229
2025-04-04 20:07:39 - INFO - Epoch [2/10], Batch [140/374], Loss: 0.6502
2025-04-04 20:07:40 - INFO - Epoch [2/10], Batch [150/374], Loss: 0.6327
2025-04-04 20:07:41 - INFO - Epoch [2/10], Batch [160/374], Loss: 0.6453
2025-04-04 20:07:41 - INFO - Epoch [2/10], Batch [170/374], Loss: 0.6742
2025-04-04 20:07:42 - INFO - Epoch [2/10], Batch [180/374], Loss: 0.5733
2025-04-04 20:07:43 - INFO - Epoch [2/10], Batch [190/374], Loss: 0.4796
2025-04-04 20:07:43 - INFO - Epoch [2/10], Batch [200/374], Loss: 0.6755
2025-04-04 20:07:44 - INFO - Epoch [2/10], Batch [210/374], Loss: 0.5743
2025-04-04 20:07:45 - INFO - Epoch [2/10], Batch [220/374], Loss: 0.6947
2025-04-04 20:07:45 - INFO - Epoch [2/10], Batch [230/374], Loss: 0.6045
2025-04-04 20:07:46 - INFO - Epoch [2/10], Batch [240/374], Loss: 0.6226
2025-04-04 20:07:47 - INFO - Epoch [2/10], Batch [250/374], Loss: 0.7182
2025-04-04 20:07:47 - INFO - Epoch [2/10], Batch [260/374], Loss: 0.6642
2025-04-04 20:07:48 - INFO - Epoch [2/10], Batch [270/374], Loss: 0.4559
2025-04-04 20:07:49 - INFO - Epoch [2/10], Batch [280/374], Loss: 0.6385
2025-04-04 20:07:49 - INFO - Epoch [2/10], Batch [290/374], Loss: 0.6583
2025-04-04 20:07:50 - INFO - Epoch [2/10], Batch [300/374], Loss: 0.5364
2025-04-04 20:07:51 - INFO - Epoch [2/10], Batch [310/374], Loss: 0.5990
2025-04-04 20:07:51 - INFO - Epoch [2/10], Batch [320/374], Loss: 0.6201
2025-04-04 20:07:52 - INFO - Epoch [2/10], Batch [330/374], Loss: 0.4914
2025-04-04 20:07:53 - INFO - Epoch [2/10], Batch [340/374], Loss: 0.5170
2025-04-04 20:07:53 - INFO - Epoch [2/10], Batch [350/374], Loss: 0.6378
2025-04-04 20:07:54 - INFO - Epoch [2/10], Batch [360/374], Loss: 0.6872
2025-04-04 20:07:55 - INFO - Epoch [2/10], Batch [370/374], Loss: 0.6059
2025-04-04 20:07:55 - INFO - Epoch [2/10] Train Loss: 0.6026, Train Accuracy: 66.14%
2025-04-04 20:07:57 - INFO - Epoch [2/10] Val Loss: 0.5522, Val Accuracy: 75.02%
2025-04-04 20:07:57 - INFO - New best model at epoch 2 with val accuracy: 75.02%
2025-04-04 20:07:57 - INFO - Epoch [3/10], Batch [0/374], Loss: 0.8042
2025-04-04 20:07:58 - INFO - Epoch [3/10], Batch [10/374], Loss: 0.6346
2025-04-04 20:07:58 - INFO - Epoch [3/10], Batch [20/374], Loss: 0.6416
2025-04-04 20:07:59 - INFO - Epoch [3/10], Batch [30/374], Loss: 0.5247
2025-04-04 20:08:00 - INFO - Epoch [3/10], Batch [40/374], Loss: 0.4421
2025-04-04 20:08:00 - INFO - Epoch [3/10], Batch [50/374], Loss: 0.6293
2025-04-04 20:08:01 - INFO - Epoch [3/10], Batch [60/374], Loss: 0.7218
2025-04-04 20:08:02 - INFO - Epoch [3/10], Batch [70/374], Loss: 0.4624
2025-04-04 20:08:02 - INFO - Epoch [3/10], Batch [80/374], Loss: 0.7159
2025-04-04 20:08:03 - INFO - Epoch [3/10], Batch [90/374], Loss: 0.5686
2025-04-04 20:08:04 - INFO - Epoch [3/10], Batch [100/374], Loss: 0.5420
2025-04-04 20:08:04 - INFO - Epoch [3/10], Batch [110/374], Loss: 0.6292
2025-04-04 20:08:05 - INFO - Epoch [3/10], Batch [120/374], Loss: 0.5211
2025-04-04 20:08:06 - INFO - Epoch [3/10], Batch [130/374], Loss: 0.5781
2025-04-04 20:08:06 - INFO - Epoch [3/10], Batch [140/374], Loss: 0.4691
2025-04-04 20:08:07 - INFO - Epoch [3/10], Batch [150/374], Loss: 0.6312
2025-04-04 20:08:08 - INFO - Epoch [3/10], Batch [160/374], Loss: 0.6742
2025-04-04 20:08:09 - INFO - Epoch [3/10], Batch [170/374], Loss: 0.7409
2025-04-04 20:08:09 - INFO - Epoch [3/10], Batch [180/374], Loss: 0.6560
2025-04-04 20:08:10 - INFO - Epoch [3/10], Batch [190/374], Loss: 0.7135
2025-04-04 20:08:11 - INFO - Epoch [3/10], Batch [200/374], Loss: 0.5367
2025-04-04 20:08:11 - INFO - Epoch [3/10], Batch [210/374], Loss: 0.8367
2025-04-04 20:08:12 - INFO - Epoch [3/10], Batch [220/374], Loss: 0.5840
2025-04-04 20:08:13 - INFO - Epoch [3/10], Batch [230/374], Loss: 0.6321
2025-04-04 20:08:13 - INFO - Epoch [3/10], Batch [240/374], Loss: 0.5337
2025-04-04 20:08:14 - INFO - Epoch [3/10], Batch [250/374], Loss: 0.6456
2025-04-04 20:08:15 - INFO - Epoch [3/10], Batch [260/374], Loss: 0.5041
2025-04-04 20:08:15 - INFO - Epoch [3/10], Batch [270/374], Loss: 0.5007
2025-04-04 20:08:16 - INFO - Epoch [3/10], Batch [280/374], Loss: 0.6639
2025-04-04 20:08:17 - INFO - Epoch [3/10], Batch [290/374], Loss: 0.5041
2025-04-04 20:08:17 - INFO - Epoch [3/10], Batch [300/374], Loss: 0.3901
2025-04-04 20:08:18 - INFO - Epoch [3/10], Batch [310/374], Loss: 0.5802
2025-04-04 20:08:19 - INFO - Epoch [3/10], Batch [320/374], Loss: 0.5660
2025-04-04 20:08:19 - INFO - Epoch [3/10], Batch [330/374], Loss: 0.4727
2025-04-04 20:08:20 - INFO - Epoch [3/10], Batch [340/374], Loss: 0.4676
2025-04-04 20:08:21 - INFO - Epoch [3/10], Batch [350/374], Loss: 0.5322
2025-04-04 20:08:21 - INFO - Epoch [3/10], Batch [360/374], Loss: 0.5507
2025-04-04 20:08:22 - INFO - Epoch [3/10], Batch [370/374], Loss: 0.3628
2025-04-04 20:08:22 - INFO - Epoch [3/10] Train Loss: 0.5686, Train Accuracy: 69.17%
2025-04-04 20:08:24 - INFO - Epoch [3/10] Val Loss: 0.4967, Val Accuracy: 76.72%
2025-04-04 20:08:24 - INFO - New best model at epoch 3 with val accuracy: 76.72%
2025-04-04 20:08:24 - INFO - Epoch [4/10], Batch [0/374], Loss: 0.5602
2025-04-04 20:08:25 - INFO - Epoch [4/10], Batch [10/374], Loss: 0.6478
2025-04-04 20:08:26 - INFO - Epoch [4/10], Batch [20/374], Loss: 0.4227
2025-04-04 20:08:26 - INFO - Epoch [4/10], Batch [30/374], Loss: 0.5273
2025-04-04 20:08:27 - INFO - Epoch [4/10], Batch [40/374], Loss: 0.4663
2025-04-04 20:08:28 - INFO - Epoch [4/10], Batch [50/374], Loss: 0.4480
2025-04-04 20:08:28 - INFO - Epoch [4/10], Batch [60/374], Loss: 0.6289
2025-04-04 20:08:29 - INFO - Epoch [4/10], Batch [70/374], Loss: 0.5293
2025-04-04 20:08:30 - INFO - Epoch [4/10], Batch [80/374], Loss: 0.5389
2025-04-04 20:08:30 - INFO - Epoch [4/10], Batch [90/374], Loss: 0.5728
2025-04-04 20:08:31 - INFO - Epoch [4/10], Batch [100/374], Loss: 0.5390
2025-04-04 20:08:32 - INFO - Epoch [4/10], Batch [110/374], Loss: 0.5082
2025-04-04 20:08:32 - INFO - Epoch [4/10], Batch [120/374], Loss: 0.5408
2025-04-04 20:08:33 - INFO - Epoch [4/10], Batch [130/374], Loss: 0.5606
2025-04-04 20:08:34 - INFO - Epoch [4/10], Batch [140/374], Loss: 0.4486
2025-04-04 20:08:34 - INFO - Epoch [4/10], Batch [150/374], Loss: 0.5192
2025-04-04 20:08:35 - INFO - Epoch [4/10], Batch [160/374], Loss: 0.6806
2025-04-04 20:08:36 - INFO - Epoch [4/10], Batch [170/374], Loss: 0.4347
2025-04-04 20:08:36 - INFO - Epoch [4/10], Batch [180/374], Loss: 0.5914
2025-04-04 20:08:37 - INFO - Epoch [4/10], Batch [190/374], Loss: 0.4319
2025-04-04 20:08:38 - INFO - Epoch [4/10], Batch [200/374], Loss: 0.5695
2025-04-04 20:08:38 - INFO - Epoch [4/10], Batch [210/374], Loss: 0.3692
2025-04-04 20:08:39 - INFO - Epoch [4/10], Batch [220/374], Loss: 0.4783
2025-04-04 20:08:40 - INFO - Epoch [4/10], Batch [230/374], Loss: 0.4583
2025-04-04 20:08:40 - INFO - Epoch [4/10], Batch [240/374], Loss: 0.4835
2025-04-04 20:08:41 - INFO - Epoch [4/10], Batch [250/374], Loss: 0.6485
2025-04-04 20:08:42 - INFO - Epoch [4/10], Batch [260/374], Loss: 0.6065
2025-04-04 20:08:43 - INFO - Epoch [4/10], Batch [270/374], Loss: 0.3986
2025-04-04 20:08:43 - INFO - Epoch [4/10], Batch [280/374], Loss: 0.5467
2025-04-04 20:08:44 - INFO - Epoch [4/10], Batch [290/374], Loss: 0.4915
2025-04-04 20:08:45 - INFO - Epoch [4/10], Batch [300/374], Loss: 0.4590
2025-04-04 20:08:45 - INFO - Epoch [4/10], Batch [310/374], Loss: 0.4099
2025-04-04 20:08:46 - INFO - Epoch [4/10], Batch [320/374], Loss: 0.6055
2025-04-04 20:08:47 - INFO - Epoch [4/10], Batch [330/374], Loss: 0.5598
2025-04-04 20:08:47 - INFO - Epoch [4/10], Batch [340/374], Loss: 0.5348
2025-04-04 20:08:48 - INFO - Epoch [4/10], Batch [350/374], Loss: 0.4683
2025-04-04 20:08:49 - INFO - Epoch [4/10], Batch [360/374], Loss: 0.4318
2025-04-04 20:08:49 - INFO - Epoch [4/10], Batch [370/374], Loss: 0.4221
2025-04-04 20:08:49 - INFO - Epoch [4/10] Train Loss: 0.5176, Train Accuracy: 73.97%
2025-04-04 20:08:51 - INFO - Epoch [4/10] Val Loss: 0.4151, Val Accuracy: 81.63%
2025-04-04 20:08:51 - INFO - New best model at epoch 4 with val accuracy: 81.63%
2025-04-04 20:08:52 - INFO - Epoch [5/10], Batch [0/374], Loss: 0.5121
2025-04-04 20:08:52 - INFO - Epoch [5/10], Batch [10/374], Loss: 0.3603
2025-04-04 20:08:53 - INFO - Epoch [5/10], Batch [20/374], Loss: 0.4209
2025-04-04 20:08:54 - INFO - Epoch [5/10], Batch [30/374], Loss: 0.3640
2025-04-04 20:08:54 - INFO - Epoch [5/10], Batch [40/374], Loss: 0.3726
2025-04-04 20:08:55 - INFO - Epoch [5/10], Batch [50/374], Loss: 0.2871
2025-04-04 20:08:56 - INFO - Epoch [5/10], Batch [60/374], Loss: 0.5130
2025-04-04 20:08:56 - INFO - Epoch [5/10], Batch [70/374], Loss: 0.3146
2025-04-04 20:08:57 - INFO - Epoch [5/10], Batch [80/374], Loss: 0.3651
2025-04-04 20:08:58 - INFO - Epoch [5/10], Batch [90/374], Loss: 0.3388
2025-04-04 20:08:58 - INFO - Epoch [5/10], Batch [100/374], Loss: 0.4455
2025-04-04 20:08:59 - INFO - Epoch [5/10], Batch [110/374], Loss: 0.3699
2025-04-04 20:09:00 - INFO - Epoch [5/10], Batch [120/374], Loss: 0.4583
2025-04-04 20:09:00 - INFO - Epoch [5/10], Batch [130/374], Loss: 0.5406
2025-04-04 20:09:01 - INFO - Epoch [5/10], Batch [140/374], Loss: 0.4365
2025-04-04 20:09:02 - INFO - Epoch [5/10], Batch [150/374], Loss: 0.4666
2025-04-04 20:09:02 - INFO - Epoch [5/10], Batch [160/374], Loss: 0.3653
2025-04-04 20:09:03 - INFO - Epoch [5/10], Batch [170/374], Loss: 0.4058
2025-04-04 20:09:04 - INFO - Epoch [5/10], Batch [180/374], Loss: 0.4616
2025-04-04 20:09:04 - INFO - Epoch [5/10], Batch [190/374], Loss: 0.5762
2025-04-04 20:09:05 - INFO - Epoch [5/10], Batch [200/374], Loss: 0.5463
2025-04-04 20:09:06 - INFO - Epoch [5/10], Batch [210/374], Loss: 0.3059
2025-04-04 20:09:06 - INFO - Epoch [5/10], Batch [220/374], Loss: 0.2749
2025-04-04 20:09:07 - INFO - Epoch [5/10], Batch [230/374], Loss: 0.4695
2025-04-04 20:09:08 - INFO - Epoch [5/10], Batch [240/374], Loss: 0.4772
2025-04-04 20:09:08 - INFO - Epoch [5/10], Batch [250/374], Loss: 0.6243
2025-04-04 20:09:09 - INFO - Epoch [5/10], Batch [260/374], Loss: 0.3165
2025-04-04 20:09:10 - INFO - Epoch [5/10], Batch [270/374], Loss: 0.4618
2025-04-04 20:09:10 - INFO - Epoch [5/10], Batch [280/374], Loss: 0.4602
2025-04-04 20:09:11 - INFO - Epoch [5/10], Batch [290/374], Loss: 0.5892
2025-04-04 20:09:12 - INFO - Epoch [5/10], Batch [300/374], Loss: 0.5503
2025-04-04 20:09:12 - INFO - Epoch [5/10], Batch [310/374], Loss: 0.2914
2025-04-04 20:09:13 - INFO - Epoch [5/10], Batch [320/374], Loss: 0.2988
2025-04-04 20:09:14 - INFO - Epoch [5/10], Batch [330/374], Loss: 0.4718
2025-04-04 20:09:14 - INFO - Epoch [5/10], Batch [340/374], Loss: 0.4063
2025-04-04 20:09:15 - INFO - Epoch [5/10], Batch [350/374], Loss: 0.3666
2025-04-04 20:09:16 - INFO - Epoch [5/10], Batch [360/374], Loss: 0.3565
2025-04-04 20:09:17 - INFO - Epoch [5/10], Batch [370/374], Loss: 0.8692
2025-04-04 20:09:17 - INFO - Epoch [5/10] Train Loss: 0.4563, Train Accuracy: 78.26%
2025-04-04 20:09:18 - INFO - Epoch [5/10] Val Loss: 0.3807, Val Accuracy: 83.92%
2025-04-04 20:09:18 - INFO - New best model at epoch 5 with val accuracy: 83.92%
2025-04-04 20:09:19 - INFO - Epoch [6/10], Batch [0/374], Loss: 0.4300
2025-04-04 20:09:19 - INFO - Epoch [6/10], Batch [10/374], Loss: 0.5098
2025-04-04 20:09:20 - INFO - Epoch [6/10], Batch [20/374], Loss: 0.5187
2025-04-04 20:09:21 - INFO - Epoch [6/10], Batch [30/374], Loss: 0.4962
2025-04-04 20:09:22 - INFO - Epoch [6/10], Batch [40/374], Loss: 0.4618
2025-04-04 20:09:22 - INFO - Epoch [6/10], Batch [50/374], Loss: 0.3991
2025-04-04 20:09:23 - INFO - Epoch [6/10], Batch [60/374], Loss: 0.3982
2025-04-04 20:09:24 - INFO - Epoch [6/10], Batch [70/374], Loss: 0.5379
2025-04-04 20:09:24 - INFO - Epoch [6/10], Batch [80/374], Loss: 0.3943
2025-04-04 20:09:25 - INFO - Epoch [6/10], Batch [90/374], Loss: 0.4849
2025-04-04 20:09:26 - INFO - Epoch [6/10], Batch [100/374], Loss: 0.3566
2025-04-04 20:09:26 - INFO - Epoch [6/10], Batch [110/374], Loss: 0.4200
2025-04-04 20:09:27 - INFO - Epoch [6/10], Batch [120/374], Loss: 0.3335
2025-04-04 20:09:28 - INFO - Epoch [6/10], Batch [130/374], Loss: 0.3662
2025-04-04 20:09:28 - INFO - Epoch [6/10], Batch [140/374], Loss: 0.3488
2025-04-04 20:09:29 - INFO - Epoch [6/10], Batch [150/374], Loss: 0.3176
2025-04-04 20:09:30 - INFO - Epoch [6/10], Batch [160/374], Loss: 0.3532
2025-04-04 20:09:30 - INFO - Epoch [6/10], Batch [170/374], Loss: 0.2483
2025-04-04 20:09:31 - INFO - Epoch [6/10], Batch [180/374], Loss: 0.3792
2025-04-04 20:09:32 - INFO - Epoch [6/10], Batch [190/374], Loss: 0.2988
2025-04-04 20:09:32 - INFO - Epoch [6/10], Batch [200/374], Loss: 0.3908
2025-04-04 20:09:33 - INFO - Epoch [6/10], Batch [210/374], Loss: 0.3910
2025-04-04 20:09:34 - INFO - Epoch [6/10], Batch [220/374], Loss: 0.3998
2025-04-04 20:09:34 - INFO - Epoch [6/10], Batch [230/374], Loss: 0.3587
2025-04-04 20:09:35 - INFO - Epoch [6/10], Batch [240/374], Loss: 0.7851
2025-04-04 20:09:36 - INFO - Epoch [6/10], Batch [250/374], Loss: 0.4329
2025-04-04 20:09:36 - INFO - Epoch [6/10], Batch [260/374], Loss: 0.4457
2025-04-04 20:09:37 - INFO - Epoch [6/10], Batch [270/374], Loss: 0.3555
2025-04-04 20:09:38 - INFO - Epoch [6/10], Batch [280/374], Loss: 0.2860
2025-04-04 20:09:38 - INFO - Epoch [6/10], Batch [290/374], Loss: 0.3098
2025-04-04 20:09:39 - INFO - Epoch [6/10], Batch [300/374], Loss: 0.4860
2025-04-04 20:09:40 - INFO - Epoch [6/10], Batch [310/374], Loss: 0.4452
2025-04-04 20:09:40 - INFO - Epoch [6/10], Batch [320/374], Loss: 0.4817
2025-04-04 20:09:41 - INFO - Epoch [6/10], Batch [330/374], Loss: 0.4391
2025-04-04 20:09:42 - INFO - Epoch [6/10], Batch [340/374], Loss: 0.3743
2025-04-04 20:09:42 - INFO - Epoch [6/10], Batch [350/374], Loss: 0.3200
2025-04-04 20:09:43 - INFO - Epoch [6/10], Batch [360/374], Loss: 0.5473
2025-04-04 20:09:44 - INFO - Epoch [6/10], Batch [370/374], Loss: 0.5026
2025-04-04 20:09:44 - INFO - Epoch [6/10] Train Loss: 0.4189, Train Accuracy: 79.91%
2025-04-04 20:09:46 - INFO - Epoch [6/10] Val Loss: 0.3585, Val Accuracy: 84.24%
2025-04-04 20:09:46 - INFO - New best model at epoch 6 with val accuracy: 84.24%
2025-04-04 20:09:46 - INFO - Epoch [7/10], Batch [0/374], Loss: 0.2941
2025-04-04 20:09:47 - INFO - Epoch [7/10], Batch [10/374], Loss: 0.5168
2025-04-04 20:09:47 - INFO - Epoch [7/10], Batch [20/374], Loss: 0.3429
2025-04-04 20:09:48 - INFO - Epoch [7/10], Batch [30/374], Loss: 0.3878
2025-04-04 20:09:49 - INFO - Epoch [7/10], Batch [40/374], Loss: 0.5414
2025-04-04 20:09:49 - INFO - Epoch [7/10], Batch [50/374], Loss: 0.3505
2025-04-04 20:09:50 - INFO - Epoch [7/10], Batch [60/374], Loss: 0.5921
2025-04-04 20:09:51 - INFO - Epoch [7/10], Batch [70/374], Loss: 0.2587
2025-04-04 20:09:51 - INFO - Epoch [7/10], Batch [80/374], Loss: 0.3568
2025-04-04 20:09:52 - INFO - Epoch [7/10], Batch [90/374], Loss: 0.2885
2025-04-04 20:09:53 - INFO - Epoch [7/10], Batch [100/374], Loss: 0.4238
2025-04-04 20:09:53 - INFO - Epoch [7/10], Batch [110/374], Loss: 0.4263
2025-04-04 20:09:54 - INFO - Epoch [7/10], Batch [120/374], Loss: 0.4126
2025-04-04 20:09:55 - INFO - Epoch [7/10], Batch [130/374], Loss: 0.3301
2025-04-04 20:09:56 - INFO - Epoch [7/10], Batch [140/374], Loss: 0.2898
2025-04-04 20:09:56 - INFO - Epoch [7/10], Batch [150/374], Loss: 0.2160
2025-04-04 20:09:57 - INFO - Epoch [7/10], Batch [160/374], Loss: 0.4347
2025-04-04 20:09:58 - INFO - Epoch [7/10], Batch [170/374], Loss: 0.2100
2025-04-04 20:09:58 - INFO - Epoch [7/10], Batch [180/374], Loss: 0.5093
2025-04-04 20:09:59 - INFO - Epoch [7/10], Batch [190/374], Loss: 0.4231
2025-04-04 20:10:00 - INFO - Epoch [7/10], Batch [200/374], Loss: 0.4068
2025-04-04 20:10:00 - INFO - Epoch [7/10], Batch [210/374], Loss: 0.2793
2025-04-04 20:10:01 - INFO - Epoch [7/10], Batch [220/374], Loss: 0.3377
2025-04-04 20:10:02 - INFO - Epoch [7/10], Batch [230/374], Loss: 0.3135
2025-04-04 20:10:02 - INFO - Epoch [7/10], Batch [240/374], Loss: 0.4560
2025-04-04 20:10:03 - INFO - Epoch [7/10], Batch [250/374], Loss: 0.2750
2025-04-04 20:10:04 - INFO - Epoch [7/10], Batch [260/374], Loss: 0.2622
2025-04-04 20:10:04 - INFO - Epoch [7/10], Batch [270/374], Loss: 0.4923
2025-04-04 20:10:05 - INFO - Epoch [7/10], Batch [280/374], Loss: 0.3131
2025-04-04 20:10:06 - INFO - Epoch [7/10], Batch [290/374], Loss: 0.2554
2025-04-04 20:10:06 - INFO - Epoch [7/10], Batch [300/374], Loss: 0.4267
2025-04-04 20:10:07 - INFO - Epoch [7/10], Batch [310/374], Loss: 0.3880
2025-04-04 20:10:08 - INFO - Epoch [7/10], Batch [320/374], Loss: 0.5381
2025-04-04 20:10:08 - INFO - Epoch [7/10], Batch [330/374], Loss: 0.4164
2025-04-04 20:10:09 - INFO - Epoch [7/10], Batch [340/374], Loss: 0.3711
2025-04-04 20:10:10 - INFO - Epoch [7/10], Batch [350/374], Loss: 0.4316
2025-04-04 20:10:10 - INFO - Epoch [7/10], Batch [360/374], Loss: 0.4417
2025-04-04 20:10:11 - INFO - Epoch [7/10], Batch [370/374], Loss: 0.3430
2025-04-04 20:10:11 - INFO - Epoch [7/10] Train Loss: 0.3882, Train Accuracy: 81.86%
2025-04-04 20:10:13 - INFO - Epoch [7/10] Val Loss: 0.3250, Val Accuracy: 86.08%
2025-04-04 20:10:13 - INFO - New best model at epoch 7 with val accuracy: 86.08%
2025-04-04 20:10:13 - INFO - Epoch [8/10], Batch [0/374], Loss: 0.6456
2025-04-04 20:10:14 - INFO - Epoch [8/10], Batch [10/374], Loss: 0.2247
2025-04-04 20:10:15 - INFO - Epoch [8/10], Batch [20/374], Loss: 0.4909
2025-04-04 20:10:15 - INFO - Epoch [8/10], Batch [30/374], Loss: 0.3387
2025-04-04 20:10:16 - INFO - Epoch [8/10], Batch [40/374], Loss: 0.3924
2025-04-04 20:10:17 - INFO - Epoch [8/10], Batch [50/374], Loss: 0.2837
2025-04-04 20:10:17 - INFO - Epoch [8/10], Batch [60/374], Loss: 0.2614
2025-04-04 20:10:18 - INFO - Epoch [8/10], Batch [70/374], Loss: 0.5213
2025-04-04 20:10:19 - INFO - Epoch [8/10], Batch [80/374], Loss: 0.3169
2025-04-04 20:10:19 - INFO - Epoch [8/10], Batch [90/374], Loss: 0.2928
2025-04-04 20:10:20 - INFO - Epoch [8/10], Batch [100/374], Loss: 0.4761
2025-04-04 20:10:21 - INFO - Epoch [8/10], Batch [110/374], Loss: 0.2234
2025-04-04 20:10:21 - INFO - Epoch [8/10], Batch [120/374], Loss: 0.3529
2025-04-04 20:10:22 - INFO - Epoch [8/10], Batch [130/374], Loss: 0.2938
2025-04-04 20:10:23 - INFO - Epoch [8/10], Batch [140/374], Loss: 0.3666
2025-04-04 20:10:23 - INFO - Epoch [8/10], Batch [150/374], Loss: 0.5168
2025-04-04 20:10:24 - INFO - Epoch [8/10], Batch [160/374], Loss: 0.8811
2025-04-04 20:10:25 - INFO - Epoch [8/10], Batch [170/374], Loss: 0.3488
2025-04-04 20:10:25 - INFO - Epoch [8/10], Batch [180/374], Loss: 0.6783
2025-04-04 20:10:26 - INFO - Epoch [8/10], Batch [190/374], Loss: 0.6346
2025-04-04 20:10:27 - INFO - Epoch [8/10], Batch [200/374], Loss: 0.3689
2025-04-04 20:10:27 - INFO - Epoch [8/10], Batch [210/374], Loss: 0.3511
2025-04-04 20:10:28 - INFO - Epoch [8/10], Batch [220/374], Loss: 0.2354
2025-04-04 20:10:29 - INFO - Epoch [8/10], Batch [230/374], Loss: 0.3427
2025-04-04 20:10:30 - INFO - Epoch [8/10], Batch [240/374], Loss: 0.2413
2025-04-04 20:10:30 - INFO - Epoch [8/10], Batch [250/374], Loss: 0.1921
2025-04-04 20:10:31 - INFO - Epoch [8/10], Batch [260/374], Loss: 0.3164
2025-04-04 20:10:32 - INFO - Epoch [8/10], Batch [270/374], Loss: 0.3153
2025-04-04 20:10:32 - INFO - Epoch [8/10], Batch [280/374], Loss: 0.5542
2025-04-04 20:10:33 - INFO - Epoch [8/10], Batch [290/374], Loss: 0.4630
2025-04-04 20:10:34 - INFO - Epoch [8/10], Batch [300/374], Loss: 0.3625
2025-04-04 20:10:34 - INFO - Epoch [8/10], Batch [310/374], Loss: 0.3481
2025-04-04 20:10:35 - INFO - Epoch [8/10], Batch [320/374], Loss: 0.6796
2025-04-04 20:10:36 - INFO - Epoch [8/10], Batch [330/374], Loss: 0.4063
2025-04-04 20:10:36 - INFO - Epoch [8/10], Batch [340/374], Loss: 0.3400
2025-04-04 20:10:37 - INFO - Epoch [8/10], Batch [350/374], Loss: 0.4249
2025-04-04 20:10:38 - INFO - Epoch [8/10], Batch [360/374], Loss: 0.4262
2025-04-04 20:10:38 - INFO - Epoch [8/10], Batch [370/374], Loss: 0.4107
2025-04-04 20:10:39 - INFO - Epoch [8/10] Train Loss: 0.3743, Train Accuracy: 82.61%
2025-04-04 20:10:40 - INFO - Epoch [8/10] Val Loss: 0.3039, Val Accuracy: 87.37%
2025-04-04 20:10:40 - INFO - New best model at epoch 8 with val accuracy: 87.37%
2025-04-04 20:10:41 - INFO - Epoch [9/10], Batch [0/374], Loss: 0.3833
2025-04-04 20:10:41 - INFO - Epoch [9/10], Batch [10/374], Loss: 0.5219
2025-04-04 20:10:42 - INFO - Epoch [9/10], Batch [20/374], Loss: 0.3310
2025-04-04 20:10:43 - INFO - Epoch [9/10], Batch [30/374], Loss: 0.3971
2025-04-04 20:10:43 - INFO - Epoch [9/10], Batch [40/374], Loss: 0.2863
2025-04-04 20:10:44 - INFO - Epoch [9/10], Batch [50/374], Loss: 0.6879
2025-04-04 20:10:45 - INFO - Epoch [9/10], Batch [60/374], Loss: 0.4315
2025-04-04 20:10:45 - INFO - Epoch [9/10], Batch [70/374], Loss: 0.4105
2025-04-04 20:10:46 - INFO - Epoch [9/10], Batch [80/374], Loss: 0.3640
2025-04-04 20:10:47 - INFO - Epoch [9/10], Batch [90/374], Loss: 0.4068
2025-04-04 20:10:47 - INFO - Epoch [9/10], Batch [100/374], Loss: 0.3634
2025-04-04 20:10:48 - INFO - Epoch [9/10], Batch [110/374], Loss: 0.3245
2025-04-04 20:10:49 - INFO - Epoch [9/10], Batch [120/374], Loss: 0.2949
2025-04-04 20:10:49 - INFO - Epoch [9/10], Batch [130/374], Loss: 0.3467
2025-04-04 20:10:50 - INFO - Epoch [9/10], Batch [140/374], Loss: 0.4638
2025-04-04 20:10:51 - INFO - Epoch [9/10], Batch [150/374], Loss: 0.2858
2025-04-04 20:10:51 - INFO - Epoch [9/10], Batch [160/374], Loss: 0.3680
2025-04-04 20:10:52 - INFO - Epoch [9/10], Batch [170/374], Loss: 0.2191
2025-04-04 20:10:53 - INFO - Epoch [9/10], Batch [180/374], Loss: 0.4242
2025-04-04 20:10:53 - INFO - Epoch [9/10], Batch [190/374], Loss: 0.3902
2025-04-04 20:10:54 - INFO - Epoch [9/10], Batch [200/374], Loss: 0.2792
2025-04-04 20:10:55 - INFO - Epoch [9/10], Batch [210/374], Loss: 0.2443
2025-04-04 20:10:55 - INFO - Epoch [9/10], Batch [220/374], Loss: 0.3500
2025-04-04 20:10:56 - INFO - Epoch [9/10], Batch [230/374], Loss: 0.2845
2025-04-04 20:10:57 - INFO - Epoch [9/10], Batch [240/374], Loss: 0.3468
2025-04-04 20:10:57 - INFO - Epoch [9/10], Batch [250/374], Loss: 0.2229
2025-04-04 20:10:58 - INFO - Epoch [9/10], Batch [260/374], Loss: 0.2025
2025-04-04 20:10:59 - INFO - Epoch [9/10], Batch [270/374], Loss: 0.2655
2025-04-04 20:10:59 - INFO - Epoch [9/10], Batch [280/374], Loss: 0.2774
2025-04-04 20:11:00 - INFO - Epoch [9/10], Batch [290/374], Loss: 0.3932
2025-04-04 20:11:01 - INFO - Epoch [9/10], Batch [300/374], Loss: 0.3808
2025-04-04 20:11:02 - INFO - Epoch [9/10], Batch [310/374], Loss: 0.3305
2025-04-04 20:11:02 - INFO - Epoch [9/10], Batch [320/374], Loss: 0.4577
2025-04-04 20:11:03 - INFO - Epoch [9/10], Batch [330/374], Loss: 0.4278
2025-04-04 20:11:04 - INFO - Epoch [9/10], Batch [340/374], Loss: 0.2618
2025-04-04 20:11:04 - INFO - Epoch [9/10], Batch [350/374], Loss: 0.5117
2025-04-04 20:11:05 - INFO - Epoch [9/10], Batch [360/374], Loss: 0.2076
2025-04-04 20:11:06 - INFO - Epoch [9/10], Batch [370/374], Loss: 0.2293
2025-04-04 20:11:06 - INFO - Epoch [9/10] Train Loss: 0.3579, Train Accuracy: 83.51%
2025-04-04 20:11:08 - INFO - Epoch [9/10] Val Loss: 0.2914, Val Accuracy: 88.03%
2025-04-04 20:11:08 - INFO - New best model at epoch 9 with val accuracy: 88.03%
2025-04-04 20:11:08 - INFO - Epoch [10/10], Batch [0/374], Loss: 0.3374
2025-04-04 20:11:09 - INFO - Epoch [10/10], Batch [10/374], Loss: 0.3736
2025-04-04 20:11:09 - INFO - Epoch [10/10], Batch [20/374], Loss: 0.4531
2025-04-04 20:11:10 - INFO - Epoch [10/10], Batch [30/374], Loss: 0.5231
2025-04-04 20:11:11 - INFO - Epoch [10/10], Batch [40/374], Loss: 0.2678
2025-04-04 20:11:11 - INFO - Epoch [10/10], Batch [50/374], Loss: 0.3781
2025-04-04 20:11:12 - INFO - Epoch [10/10], Batch [60/374], Loss: 0.1824
2025-04-04 20:11:13 - INFO - Epoch [10/10], Batch [70/374], Loss: 0.2925
2025-04-04 20:11:13 - INFO - Epoch [10/10], Batch [80/374], Loss: 0.3166
2025-04-04 20:11:14 - INFO - Epoch [10/10], Batch [90/374], Loss: 0.2956
2025-04-04 20:11:15 - INFO - Epoch [10/10], Batch [100/374], Loss: 0.3685
2025-04-04 20:11:15 - INFO - Epoch [10/10], Batch [110/374], Loss: 0.2750
2025-04-04 20:11:16 - INFO - Epoch [10/10], Batch [120/374], Loss: 0.4123
2025-04-04 20:11:17 - INFO - Epoch [10/10], Batch [130/374], Loss: 0.4147
2025-04-04 20:11:17 - INFO - Epoch [10/10], Batch [140/374], Loss: 0.3172
2025-04-04 20:11:18 - INFO - Epoch [10/10], Batch [150/374], Loss: 0.2925
2025-04-04 20:11:19 - INFO - Epoch [10/10], Batch [160/374], Loss: 0.2732
2025-04-04 20:11:19 - INFO - Epoch [10/10], Batch [170/374], Loss: 0.2563
2025-04-04 20:11:20 - INFO - Epoch [10/10], Batch [180/374], Loss: 0.5732
2025-04-04 20:11:21 - INFO - Epoch [10/10], Batch [190/374], Loss: 0.4567
2025-04-04 20:11:21 - INFO - Epoch [10/10], Batch [200/374], Loss: 0.3132
2025-04-04 20:11:22 - INFO - Epoch [10/10], Batch [210/374], Loss: 0.3159
2025-04-04 20:11:23 - INFO - Epoch [10/10], Batch [220/374], Loss: 0.3106
2025-04-04 20:11:23 - INFO - Epoch [10/10], Batch [230/374], Loss: 0.2136
2025-04-04 20:11:24 - INFO - Epoch [10/10], Batch [240/374], Loss: 0.2606
2025-04-04 20:11:25 - INFO - Epoch [10/10], Batch [250/374], Loss: 0.3547
2025-04-04 20:11:25 - INFO - Epoch [10/10], Batch [260/374], Loss: 0.3005
2025-04-04 20:11:26 - INFO - Epoch [10/10], Batch [270/374], Loss: 0.3062
2025-04-04 20:11:27 - INFO - Epoch [10/10], Batch [280/374], Loss: 0.3476
2025-04-04 20:11:27 - INFO - Epoch [10/10], Batch [290/374], Loss: 0.2663
2025-04-04 20:11:28 - INFO - Epoch [10/10], Batch [300/374], Loss: 0.3411
2025-04-04 20:11:29 - INFO - Epoch [10/10], Batch [310/374], Loss: 0.4638
2025-04-04 20:11:29 - INFO - Epoch [10/10], Batch [320/374], Loss: 0.3057
2025-04-04 20:11:30 - INFO - Epoch [10/10], Batch [330/374], Loss: 0.4519
2025-04-04 20:11:31 - INFO - Epoch [10/10], Batch [340/374], Loss: 0.3528
2025-04-04 20:11:31 - INFO - Epoch [10/10], Batch [350/374], Loss: 0.2483
2025-04-04 20:11:32 - INFO - Epoch [10/10], Batch [360/374], Loss: 0.4287
2025-04-04 20:11:33 - INFO - Epoch [10/10], Batch [370/374], Loss: 0.4043
2025-04-04 20:11:33 - INFO - Epoch [10/10] Train Loss: 0.3488, Train Accuracy: 83.84%
2025-04-04 20:11:35 - INFO - Epoch [10/10] Val Loss: 0.2855, Val Accuracy: 88.31%
2025-04-04 20:11:35 - INFO - New best model at epoch 10 with val accuracy: 88.31%
2025-04-04 20:11:36 - INFO - Test Loss: 0.2951, Test Accuracy: 88.38%
2025-04-04 20:11:36 - INFO - 
===== Final Performance Results =====
2025-04-04 20:11:36 - INFO - {
    "world_size": 2,
    "train_time": 275.1992199420929,
    "avg_epoch_time": 27.51992199420929,
    "val_accuracy": 88.3089770354906,
    "test_accuracy": 88.3785664578984,
    "test_loss": 0.29506914924231353,
    "total_time": 275.1992199420929,
    "train_losses": [
        0.6353901035516908,
        0.6025732843429337,
        0.568621723686704,
        0.517621310927078,
        0.45630984047594725,
        0.4189116405496957,
        0.38824075318268725,
        0.37430600557469734,
        0.35791079620555194,
        0.3488222189513856
    ],
    "train_accuracies": [
        61.26035651518956,
        66.13942589338019,
        69.16896811448657,
        73.97271738220772,
        78.25759477780568,
        79.91463720813458,
        81.8562222780149,
        82.60942338270985,
        83.5132647083438,
        83.8396518537116
    ],
    "val_losses": [
        0.5808497312256422,
        0.5522465018157587,
        0.49670582690335185,
        0.4151200259320837,
        0.3806615364410517,
        0.35852783499447605,
        0.3250030197446846,
        0.30390571361626695,
        0.2914407421875597,
        0.2855457641552452
    ],
    "val_accuracies": [
        72.79053583855254,
        75.01739735560194,
        76.7223382045929,
        81.62839248434238,
        83.92484342379959,
        84.23799582463465,
        86.0821155184412,
        87.36951983298539,
        88.03061934585944,
        88.3089770354906
    ],
    "hyperparameters": {
        "batch_size": 32,
        "num_epochs": 10,
        "initial_lr": 0.001,
        "weight_decay": 0.0001,
        "num_classes": 2,
        "save_model": true
    }
}
2025-04-04 20:11:36 - INFO - Model saved to models/training_using_gpus_2_best_model.pt
