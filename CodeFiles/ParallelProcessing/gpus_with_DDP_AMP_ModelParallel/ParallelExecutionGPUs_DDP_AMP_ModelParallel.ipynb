{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21d2a87-3c00-4346-9c03-c752e62fd0da",
   "metadata": {},
   "source": [
    "# CSYE 7105 - High Perfoamnce Machine Learning & AI - Project - Team 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736a793-cb96-404f-a6c5-698c7b7f4c32",
   "metadata": {},
   "source": [
    "## Analysis of Efficient Parallel Computing for Deep Learning-Based Glaucoma Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862afdcb-2b4b-40b6-a5bc-c09381a59cb6",
   "metadata": {},
   "source": [
    "### Parallel Execution using GPUs with DDP, AMP and Model Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151561c-4ec0-42c2-b183-12a2ea55cfff",
   "metadata": {},
   "source": [
    "### Calling the main function through the torchrun command to train the model in 2 GPUs with DDP, AMP and Model Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dccbea05-7756-4f54-a95c-5dfaaddc4f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0404 20:03:22.351000 1094822 site-packages/torch/distributed/run.py:793] \n",
      "W0404 20:03:22.351000 1094822 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0404 20:03:22.351000 1094822 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0404 20:03:22.351000 1094822 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Running on node: d1004Running on node: d1004\n",
      "\n",
      "Running with world_size: 2 (Rank: 0)Running with world_size: 2 (Rank: 1)\n",
      "\n",
      "2025-04-04 20:03:30 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 20:03:30 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 20:06:57 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-04 20:06:57 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-04 20:07:00 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-04 20:07:00 - INFO - Total layers: 129\n",
      "2025-04-04 20:07:00 - INFO - Total parameters: 22,494,274\n",
      "2025-04-04 20:07:02 - INFO - Epoch [1/10], Batch [0/374], Loss: 0.6838\n",
      "2025-04-04 20:07:02 - INFO - Epoch [1/10], Batch [10/374], Loss: 0.7080\n",
      "2025-04-04 20:07:03 - INFO - Epoch [1/10], Batch [20/374], Loss: 0.6697\n",
      "2025-04-04 20:07:04 - INFO - Epoch [1/10], Batch [30/374], Loss: 0.6936\n",
      "2025-04-04 20:07:05 - INFO - Epoch [1/10], Batch [40/374], Loss: 0.6248\n",
      "2025-04-04 20:07:05 - INFO - Epoch [1/10], Batch [50/374], Loss: 0.6716\n",
      "2025-04-04 20:07:06 - INFO - Epoch [1/10], Batch [60/374], Loss: 0.6268\n",
      "2025-04-04 20:07:07 - INFO - Epoch [1/10], Batch [70/374], Loss: 0.6815\n",
      "2025-04-04 20:07:07 - INFO - Epoch [1/10], Batch [80/374], Loss: 0.6101\n",
      "2025-04-04 20:07:08 - INFO - Epoch [1/10], Batch [90/374], Loss: 0.6993\n",
      "2025-04-04 20:07:09 - INFO - Epoch [1/10], Batch [100/374], Loss: 0.6989\n",
      "2025-04-04 20:07:09 - INFO - Epoch [1/10], Batch [110/374], Loss: 0.5763\n",
      "2025-04-04 20:07:10 - INFO - Epoch [1/10], Batch [120/374], Loss: 0.5612\n",
      "2025-04-04 20:07:11 - INFO - Epoch [1/10], Batch [130/374], Loss: 0.5974\n",
      "2025-04-04 20:07:11 - INFO - Epoch [1/10], Batch [140/374], Loss: 0.6731\n",
      "2025-04-04 20:07:12 - INFO - Epoch [1/10], Batch [150/374], Loss: 0.7900\n",
      "2025-04-04 20:07:13 - INFO - Epoch [1/10], Batch [160/374], Loss: 0.5967\n",
      "2025-04-04 20:07:13 - INFO - Epoch [1/10], Batch [170/374], Loss: 0.6076\n",
      "2025-04-04 20:07:14 - INFO - Epoch [1/10], Batch [180/374], Loss: 0.6135\n",
      "2025-04-04 20:07:15 - INFO - Epoch [1/10], Batch [190/374], Loss: 0.7444\n",
      "2025-04-04 20:07:15 - INFO - Epoch [1/10], Batch [200/374], Loss: 0.7502\n",
      "2025-04-04 20:07:16 - INFO - Epoch [1/10], Batch [210/374], Loss: 0.7212\n",
      "2025-04-04 20:07:17 - INFO - Epoch [1/10], Batch [220/374], Loss: 0.6360\n",
      "2025-04-04 20:07:17 - INFO - Epoch [1/10], Batch [230/374], Loss: 0.5847\n",
      "2025-04-04 20:07:18 - INFO - Epoch [1/10], Batch [240/374], Loss: 0.6184\n",
      "2025-04-04 20:07:19 - INFO - Epoch [1/10], Batch [250/374], Loss: 0.5642\n",
      "2025-04-04 20:07:19 - INFO - Epoch [1/10], Batch [260/374], Loss: 0.5777\n",
      "2025-04-04 20:07:20 - INFO - Epoch [1/10], Batch [270/374], Loss: 0.5440\n",
      "2025-04-04 20:07:21 - INFO - Epoch [1/10], Batch [280/374], Loss: 0.5233\n",
      "2025-04-04 20:07:21 - INFO - Epoch [1/10], Batch [290/374], Loss: 0.5324\n",
      "2025-04-04 20:07:22 - INFO - Epoch [1/10], Batch [300/374], Loss: 0.5455\n",
      "2025-04-04 20:07:23 - INFO - Epoch [1/10], Batch [310/374], Loss: 0.6937\n",
      "2025-04-04 20:07:23 - INFO - Epoch [1/10], Batch [320/374], Loss: 0.7009\n",
      "2025-04-04 20:07:24 - INFO - Epoch [1/10], Batch [330/374], Loss: 0.6862\n",
      "2025-04-04 20:07:25 - INFO - Epoch [1/10], Batch [340/374], Loss: 0.5326\n",
      "2025-04-04 20:07:25 - INFO - Epoch [1/10], Batch [350/374], Loss: 0.7209\n",
      "2025-04-04 20:07:26 - INFO - Epoch [1/10], Batch [360/374], Loss: 0.6879\n",
      "2025-04-04 20:07:27 - INFO - Epoch [1/10], Batch [370/374], Loss: 0.5562\n",
      "2025-04-04 20:07:27 - INFO - Epoch [1/10] Train Loss: 0.6354, Train Accuracy: 61.26%\n",
      "2025-04-04 20:07:30 - INFO - Epoch [1/10] Val Loss: 0.5808, Val Accuracy: 72.79%\n",
      "2025-04-04 20:07:30 - INFO - New best model at epoch 1 with val accuracy: 72.79%\n",
      "2025-04-04 20:07:30 - INFO - Epoch [2/10], Batch [0/374], Loss: 0.6766\n",
      "2025-04-04 20:07:31 - INFO - Epoch [2/10], Batch [10/374], Loss: 0.5300\n",
      "2025-04-04 20:07:31 - INFO - Epoch [2/10], Batch [20/374], Loss: 0.5141\n",
      "2025-04-04 20:07:32 - INFO - Epoch [2/10], Batch [30/374], Loss: 0.5314\n",
      "2025-04-04 20:07:33 - INFO - Epoch [2/10], Batch [40/374], Loss: 0.8331\n",
      "2025-04-04 20:07:33 - INFO - Epoch [2/10], Batch [50/374], Loss: 0.5443\n",
      "2025-04-04 20:07:34 - INFO - Epoch [2/10], Batch [60/374], Loss: 0.8355\n",
      "2025-04-04 20:07:35 - INFO - Epoch [2/10], Batch [70/374], Loss: 0.6691\n",
      "2025-04-04 20:07:35 - INFO - Epoch [2/10], Batch [80/374], Loss: 0.5280\n",
      "2025-04-04 20:07:36 - INFO - Epoch [2/10], Batch [90/374], Loss: 0.6082\n",
      "2025-04-04 20:07:37 - INFO - Epoch [2/10], Batch [100/374], Loss: 0.5406\n",
      "2025-04-04 20:07:37 - INFO - Epoch [2/10], Batch [110/374], Loss: 0.6845\n",
      "2025-04-04 20:07:38 - INFO - Epoch [2/10], Batch [120/374], Loss: 0.4416\n",
      "2025-04-04 20:07:39 - INFO - Epoch [2/10], Batch [130/374], Loss: 0.5229\n",
      "2025-04-04 20:07:39 - INFO - Epoch [2/10], Batch [140/374], Loss: 0.6502\n",
      "2025-04-04 20:07:40 - INFO - Epoch [2/10], Batch [150/374], Loss: 0.6327\n",
      "2025-04-04 20:07:41 - INFO - Epoch [2/10], Batch [160/374], Loss: 0.6453\n",
      "2025-04-04 20:07:41 - INFO - Epoch [2/10], Batch [170/374], Loss: 0.6742\n",
      "2025-04-04 20:07:42 - INFO - Epoch [2/10], Batch [180/374], Loss: 0.5733\n",
      "2025-04-04 20:07:43 - INFO - Epoch [2/10], Batch [190/374], Loss: 0.4796\n",
      "2025-04-04 20:07:43 - INFO - Epoch [2/10], Batch [200/374], Loss: 0.6755\n",
      "2025-04-04 20:07:44 - INFO - Epoch [2/10], Batch [210/374], Loss: 0.5743\n",
      "2025-04-04 20:07:45 - INFO - Epoch [2/10], Batch [220/374], Loss: 0.6947\n",
      "2025-04-04 20:07:45 - INFO - Epoch [2/10], Batch [230/374], Loss: 0.6045\n",
      "2025-04-04 20:07:46 - INFO - Epoch [2/10], Batch [240/374], Loss: 0.6226\n",
      "2025-04-04 20:07:47 - INFO - Epoch [2/10], Batch [250/374], Loss: 0.7182\n",
      "2025-04-04 20:07:47 - INFO - Epoch [2/10], Batch [260/374], Loss: 0.6642\n",
      "2025-04-04 20:07:48 - INFO - Epoch [2/10], Batch [270/374], Loss: 0.4559\n",
      "2025-04-04 20:07:49 - INFO - Epoch [2/10], Batch [280/374], Loss: 0.6385\n",
      "2025-04-04 20:07:49 - INFO - Epoch [2/10], Batch [290/374], Loss: 0.6583\n",
      "2025-04-04 20:07:50 - INFO - Epoch [2/10], Batch [300/374], Loss: 0.5364\n",
      "2025-04-04 20:07:51 - INFO - Epoch [2/10], Batch [310/374], Loss: 0.5990\n",
      "2025-04-04 20:07:51 - INFO - Epoch [2/10], Batch [320/374], Loss: 0.6201\n",
      "2025-04-04 20:07:52 - INFO - Epoch [2/10], Batch [330/374], Loss: 0.4914\n",
      "2025-04-04 20:07:53 - INFO - Epoch [2/10], Batch [340/374], Loss: 0.5170\n",
      "2025-04-04 20:07:53 - INFO - Epoch [2/10], Batch [350/374], Loss: 0.6378\n",
      "2025-04-04 20:07:54 - INFO - Epoch [2/10], Batch [360/374], Loss: 0.6872\n",
      "2025-04-04 20:07:55 - INFO - Epoch [2/10], Batch [370/374], Loss: 0.6059\n",
      "2025-04-04 20:07:55 - INFO - Epoch [2/10] Train Loss: 0.6026, Train Accuracy: 66.14%\n",
      "2025-04-04 20:07:57 - INFO - Epoch [2/10] Val Loss: 0.5522, Val Accuracy: 75.02%\n",
      "2025-04-04 20:07:57 - INFO - New best model at epoch 2 with val accuracy: 75.02%\n",
      "2025-04-04 20:07:57 - INFO - Epoch [3/10], Batch [0/374], Loss: 0.8042\n",
      "2025-04-04 20:07:58 - INFO - Epoch [3/10], Batch [10/374], Loss: 0.6346\n",
      "2025-04-04 20:07:58 - INFO - Epoch [3/10], Batch [20/374], Loss: 0.6416\n",
      "2025-04-04 20:07:59 - INFO - Epoch [3/10], Batch [30/374], Loss: 0.5247\n",
      "2025-04-04 20:08:00 - INFO - Epoch [3/10], Batch [40/374], Loss: 0.4421\n",
      "2025-04-04 20:08:00 - INFO - Epoch [3/10], Batch [50/374], Loss: 0.6293\n",
      "2025-04-04 20:08:01 - INFO - Epoch [3/10], Batch [60/374], Loss: 0.7218\n",
      "2025-04-04 20:08:02 - INFO - Epoch [3/10], Batch [70/374], Loss: 0.4624\n",
      "2025-04-04 20:08:02 - INFO - Epoch [3/10], Batch [80/374], Loss: 0.7159\n",
      "2025-04-04 20:08:03 - INFO - Epoch [3/10], Batch [90/374], Loss: 0.5686\n",
      "2025-04-04 20:08:04 - INFO - Epoch [3/10], Batch [100/374], Loss: 0.5420\n",
      "2025-04-04 20:08:04 - INFO - Epoch [3/10], Batch [110/374], Loss: 0.6292\n",
      "2025-04-04 20:08:05 - INFO - Epoch [3/10], Batch [120/374], Loss: 0.5211\n",
      "2025-04-04 20:08:06 - INFO - Epoch [3/10], Batch [130/374], Loss: 0.5781\n",
      "2025-04-04 20:08:06 - INFO - Epoch [3/10], Batch [140/374], Loss: 0.4691\n",
      "2025-04-04 20:08:07 - INFO - Epoch [3/10], Batch [150/374], Loss: 0.6312\n",
      "2025-04-04 20:08:08 - INFO - Epoch [3/10], Batch [160/374], Loss: 0.6742\n",
      "2025-04-04 20:08:09 - INFO - Epoch [3/10], Batch [170/374], Loss: 0.7409\n",
      "2025-04-04 20:08:09 - INFO - Epoch [3/10], Batch [180/374], Loss: 0.6560\n",
      "2025-04-04 20:08:10 - INFO - Epoch [3/10], Batch [190/374], Loss: 0.7135\n",
      "2025-04-04 20:08:11 - INFO - Epoch [3/10], Batch [200/374], Loss: 0.5367\n",
      "2025-04-04 20:08:11 - INFO - Epoch [3/10], Batch [210/374], Loss: 0.8367\n",
      "2025-04-04 20:08:12 - INFO - Epoch [3/10], Batch [220/374], Loss: 0.5840\n",
      "2025-04-04 20:08:13 - INFO - Epoch [3/10], Batch [230/374], Loss: 0.6321\n",
      "2025-04-04 20:08:13 - INFO - Epoch [3/10], Batch [240/374], Loss: 0.5337\n",
      "2025-04-04 20:08:14 - INFO - Epoch [3/10], Batch [250/374], Loss: 0.6456\n",
      "2025-04-04 20:08:15 - INFO - Epoch [3/10], Batch [260/374], Loss: 0.5041\n",
      "2025-04-04 20:08:15 - INFO - Epoch [3/10], Batch [270/374], Loss: 0.5007\n",
      "2025-04-04 20:08:16 - INFO - Epoch [3/10], Batch [280/374], Loss: 0.6639\n",
      "2025-04-04 20:08:17 - INFO - Epoch [3/10], Batch [290/374], Loss: 0.5041\n",
      "2025-04-04 20:08:17 - INFO - Epoch [3/10], Batch [300/374], Loss: 0.3901\n",
      "2025-04-04 20:08:18 - INFO - Epoch [3/10], Batch [310/374], Loss: 0.5802\n",
      "2025-04-04 20:08:19 - INFO - Epoch [3/10], Batch [320/374], Loss: 0.5660\n",
      "2025-04-04 20:08:19 - INFO - Epoch [3/10], Batch [330/374], Loss: 0.4727\n",
      "2025-04-04 20:08:20 - INFO - Epoch [3/10], Batch [340/374], Loss: 0.4676\n",
      "2025-04-04 20:08:21 - INFO - Epoch [3/10], Batch [350/374], Loss: 0.5322\n",
      "2025-04-04 20:08:21 - INFO - Epoch [3/10], Batch [360/374], Loss: 0.5507\n",
      "2025-04-04 20:08:22 - INFO - Epoch [3/10], Batch [370/374], Loss: 0.3628\n",
      "2025-04-04 20:08:22 - INFO - Epoch [3/10] Train Loss: 0.5686, Train Accuracy: 69.17%\n",
      "2025-04-04 20:08:24 - INFO - Epoch [3/10] Val Loss: 0.4967, Val Accuracy: 76.72%\n",
      "2025-04-04 20:08:24 - INFO - New best model at epoch 3 with val accuracy: 76.72%\n",
      "2025-04-04 20:08:24 - INFO - Epoch [4/10], Batch [0/374], Loss: 0.5602\n",
      "2025-04-04 20:08:25 - INFO - Epoch [4/10], Batch [10/374], Loss: 0.6478\n",
      "2025-04-04 20:08:26 - INFO - Epoch [4/10], Batch [20/374], Loss: 0.4227\n",
      "2025-04-04 20:08:26 - INFO - Epoch [4/10], Batch [30/374], Loss: 0.5273\n",
      "2025-04-04 20:08:27 - INFO - Epoch [4/10], Batch [40/374], Loss: 0.4663\n",
      "2025-04-04 20:08:28 - INFO - Epoch [4/10], Batch [50/374], Loss: 0.4480\n",
      "2025-04-04 20:08:28 - INFO - Epoch [4/10], Batch [60/374], Loss: 0.6289\n",
      "2025-04-04 20:08:29 - INFO - Epoch [4/10], Batch [70/374], Loss: 0.5293\n",
      "2025-04-04 20:08:30 - INFO - Epoch [4/10], Batch [80/374], Loss: 0.5389\n",
      "2025-04-04 20:08:30 - INFO - Epoch [4/10], Batch [90/374], Loss: 0.5728\n",
      "2025-04-04 20:08:31 - INFO - Epoch [4/10], Batch [100/374], Loss: 0.5390\n",
      "2025-04-04 20:08:32 - INFO - Epoch [4/10], Batch [110/374], Loss: 0.5082\n",
      "2025-04-04 20:08:32 - INFO - Epoch [4/10], Batch [120/374], Loss: 0.5408\n",
      "2025-04-04 20:08:33 - INFO - Epoch [4/10], Batch [130/374], Loss: 0.5606\n",
      "2025-04-04 20:08:34 - INFO - Epoch [4/10], Batch [140/374], Loss: 0.4486\n",
      "2025-04-04 20:08:34 - INFO - Epoch [4/10], Batch [150/374], Loss: 0.5192\n",
      "2025-04-04 20:08:35 - INFO - Epoch [4/10], Batch [160/374], Loss: 0.6806\n",
      "2025-04-04 20:08:36 - INFO - Epoch [4/10], Batch [170/374], Loss: 0.4347\n",
      "2025-04-04 20:08:36 - INFO - Epoch [4/10], Batch [180/374], Loss: 0.5914\n",
      "2025-04-04 20:08:37 - INFO - Epoch [4/10], Batch [190/374], Loss: 0.4319\n",
      "2025-04-04 20:08:38 - INFO - Epoch [4/10], Batch [200/374], Loss: 0.5695\n",
      "2025-04-04 20:08:38 - INFO - Epoch [4/10], Batch [210/374], Loss: 0.3692\n",
      "2025-04-04 20:08:39 - INFO - Epoch [4/10], Batch [220/374], Loss: 0.4783\n",
      "2025-04-04 20:08:40 - INFO - Epoch [4/10], Batch [230/374], Loss: 0.4583\n",
      "2025-04-04 20:08:40 - INFO - Epoch [4/10], Batch [240/374], Loss: 0.4835\n",
      "2025-04-04 20:08:41 - INFO - Epoch [4/10], Batch [250/374], Loss: 0.6485\n",
      "2025-04-04 20:08:42 - INFO - Epoch [4/10], Batch [260/374], Loss: 0.6065\n",
      "2025-04-04 20:08:43 - INFO - Epoch [4/10], Batch [270/374], Loss: 0.3986\n",
      "2025-04-04 20:08:43 - INFO - Epoch [4/10], Batch [280/374], Loss: 0.5467\n",
      "2025-04-04 20:08:44 - INFO - Epoch [4/10], Batch [290/374], Loss: 0.4915\n",
      "2025-04-04 20:08:45 - INFO - Epoch [4/10], Batch [300/374], Loss: 0.4590\n",
      "2025-04-04 20:08:45 - INFO - Epoch [4/10], Batch [310/374], Loss: 0.4099\n",
      "2025-04-04 20:08:46 - INFO - Epoch [4/10], Batch [320/374], Loss: 0.6055\n",
      "2025-04-04 20:08:47 - INFO - Epoch [4/10], Batch [330/374], Loss: 0.5598\n",
      "2025-04-04 20:08:47 - INFO - Epoch [4/10], Batch [340/374], Loss: 0.5348\n",
      "2025-04-04 20:08:48 - INFO - Epoch [4/10], Batch [350/374], Loss: 0.4683\n",
      "2025-04-04 20:08:49 - INFO - Epoch [4/10], Batch [360/374], Loss: 0.4318\n",
      "2025-04-04 20:08:49 - INFO - Epoch [4/10], Batch [370/374], Loss: 0.4221\n",
      "2025-04-04 20:08:49 - INFO - Epoch [4/10] Train Loss: 0.5176, Train Accuracy: 73.97%\n",
      "2025-04-04 20:08:51 - INFO - Epoch [4/10] Val Loss: 0.4151, Val Accuracy: 81.63%\n",
      "2025-04-04 20:08:51 - INFO - New best model at epoch 4 with val accuracy: 81.63%\n",
      "2025-04-04 20:08:52 - INFO - Epoch [5/10], Batch [0/374], Loss: 0.5121\n",
      "2025-04-04 20:08:52 - INFO - Epoch [5/10], Batch [10/374], Loss: 0.3603\n",
      "2025-04-04 20:08:53 - INFO - Epoch [5/10], Batch [20/374], Loss: 0.4209\n",
      "2025-04-04 20:08:54 - INFO - Epoch [5/10], Batch [30/374], Loss: 0.3640\n",
      "2025-04-04 20:08:54 - INFO - Epoch [5/10], Batch [40/374], Loss: 0.3726\n",
      "2025-04-04 20:08:55 - INFO - Epoch [5/10], Batch [50/374], Loss: 0.2871\n",
      "2025-04-04 20:08:56 - INFO - Epoch [5/10], Batch [60/374], Loss: 0.5130\n",
      "2025-04-04 20:08:56 - INFO - Epoch [5/10], Batch [70/374], Loss: 0.3146\n",
      "2025-04-04 20:08:57 - INFO - Epoch [5/10], Batch [80/374], Loss: 0.3651\n",
      "2025-04-04 20:08:58 - INFO - Epoch [5/10], Batch [90/374], Loss: 0.3388\n",
      "2025-04-04 20:08:58 - INFO - Epoch [5/10], Batch [100/374], Loss: 0.4455\n",
      "2025-04-04 20:08:59 - INFO - Epoch [5/10], Batch [110/374], Loss: 0.3699\n",
      "2025-04-04 20:09:00 - INFO - Epoch [5/10], Batch [120/374], Loss: 0.4583\n",
      "2025-04-04 20:09:00 - INFO - Epoch [5/10], Batch [130/374], Loss: 0.5406\n",
      "2025-04-04 20:09:01 - INFO - Epoch [5/10], Batch [140/374], Loss: 0.4365\n",
      "2025-04-04 20:09:02 - INFO - Epoch [5/10], Batch [150/374], Loss: 0.4666\n",
      "2025-04-04 20:09:02 - INFO - Epoch [5/10], Batch [160/374], Loss: 0.3653\n",
      "2025-04-04 20:09:03 - INFO - Epoch [5/10], Batch [170/374], Loss: 0.4058\n",
      "2025-04-04 20:09:04 - INFO - Epoch [5/10], Batch [180/374], Loss: 0.4616\n",
      "2025-04-04 20:09:04 - INFO - Epoch [5/10], Batch [190/374], Loss: 0.5762\n",
      "2025-04-04 20:09:05 - INFO - Epoch [5/10], Batch [200/374], Loss: 0.5463\n",
      "2025-04-04 20:09:06 - INFO - Epoch [5/10], Batch [210/374], Loss: 0.3059\n",
      "2025-04-04 20:09:06 - INFO - Epoch [5/10], Batch [220/374], Loss: 0.2749\n",
      "2025-04-04 20:09:07 - INFO - Epoch [5/10], Batch [230/374], Loss: 0.4695\n",
      "2025-04-04 20:09:08 - INFO - Epoch [5/10], Batch [240/374], Loss: 0.4772\n",
      "2025-04-04 20:09:08 - INFO - Epoch [5/10], Batch [250/374], Loss: 0.6243\n",
      "2025-04-04 20:09:09 - INFO - Epoch [5/10], Batch [260/374], Loss: 0.3165\n",
      "2025-04-04 20:09:10 - INFO - Epoch [5/10], Batch [270/374], Loss: 0.4618\n",
      "2025-04-04 20:09:10 - INFO - Epoch [5/10], Batch [280/374], Loss: 0.4602\n",
      "2025-04-04 20:09:11 - INFO - Epoch [5/10], Batch [290/374], Loss: 0.5892\n",
      "2025-04-04 20:09:12 - INFO - Epoch [5/10], Batch [300/374], Loss: 0.5503\n",
      "2025-04-04 20:09:12 - INFO - Epoch [5/10], Batch [310/374], Loss: 0.2914\n",
      "2025-04-04 20:09:13 - INFO - Epoch [5/10], Batch [320/374], Loss: 0.2988\n",
      "2025-04-04 20:09:14 - INFO - Epoch [5/10], Batch [330/374], Loss: 0.4718\n",
      "2025-04-04 20:09:14 - INFO - Epoch [5/10], Batch [340/374], Loss: 0.4063\n",
      "2025-04-04 20:09:15 - INFO - Epoch [5/10], Batch [350/374], Loss: 0.3666\n",
      "2025-04-04 20:09:16 - INFO - Epoch [5/10], Batch [360/374], Loss: 0.3565\n",
      "2025-04-04 20:09:17 - INFO - Epoch [5/10], Batch [370/374], Loss: 0.8692\n",
      "2025-04-04 20:09:17 - INFO - Epoch [5/10] Train Loss: 0.4563, Train Accuracy: 78.26%\n",
      "2025-04-04 20:09:18 - INFO - Epoch [5/10] Val Loss: 0.3807, Val Accuracy: 83.92%\n",
      "2025-04-04 20:09:18 - INFO - New best model at epoch 5 with val accuracy: 83.92%\n",
      "2025-04-04 20:09:19 - INFO - Epoch [6/10], Batch [0/374], Loss: 0.4300\n",
      "2025-04-04 20:09:19 - INFO - Epoch [6/10], Batch [10/374], Loss: 0.5098\n",
      "2025-04-04 20:09:20 - INFO - Epoch [6/10], Batch [20/374], Loss: 0.5187\n",
      "2025-04-04 20:09:21 - INFO - Epoch [6/10], Batch [30/374], Loss: 0.4962\n",
      "2025-04-04 20:09:22 - INFO - Epoch [6/10], Batch [40/374], Loss: 0.4618\n",
      "2025-04-04 20:09:22 - INFO - Epoch [6/10], Batch [50/374], Loss: 0.3991\n",
      "2025-04-04 20:09:23 - INFO - Epoch [6/10], Batch [60/374], Loss: 0.3982\n",
      "2025-04-04 20:09:24 - INFO - Epoch [6/10], Batch [70/374], Loss: 0.5379\n",
      "2025-04-04 20:09:24 - INFO - Epoch [6/10], Batch [80/374], Loss: 0.3943\n",
      "2025-04-04 20:09:25 - INFO - Epoch [6/10], Batch [90/374], Loss: 0.4849\n",
      "2025-04-04 20:09:26 - INFO - Epoch [6/10], Batch [100/374], Loss: 0.3566\n",
      "2025-04-04 20:09:26 - INFO - Epoch [6/10], Batch [110/374], Loss: 0.4200\n",
      "2025-04-04 20:09:27 - INFO - Epoch [6/10], Batch [120/374], Loss: 0.3335\n",
      "2025-04-04 20:09:28 - INFO - Epoch [6/10], Batch [130/374], Loss: 0.3662\n",
      "2025-04-04 20:09:28 - INFO - Epoch [6/10], Batch [140/374], Loss: 0.3488\n",
      "2025-04-04 20:09:29 - INFO - Epoch [6/10], Batch [150/374], Loss: 0.3176\n",
      "2025-04-04 20:09:30 - INFO - Epoch [6/10], Batch [160/374], Loss: 0.3532\n",
      "2025-04-04 20:09:30 - INFO - Epoch [6/10], Batch [170/374], Loss: 0.2483\n",
      "2025-04-04 20:09:31 - INFO - Epoch [6/10], Batch [180/374], Loss: 0.3792\n",
      "2025-04-04 20:09:32 - INFO - Epoch [6/10], Batch [190/374], Loss: 0.2988\n",
      "2025-04-04 20:09:32 - INFO - Epoch [6/10], Batch [200/374], Loss: 0.3908\n",
      "2025-04-04 20:09:33 - INFO - Epoch [6/10], Batch [210/374], Loss: 0.3910\n",
      "2025-04-04 20:09:34 - INFO - Epoch [6/10], Batch [220/374], Loss: 0.3998\n",
      "2025-04-04 20:09:34 - INFO - Epoch [6/10], Batch [230/374], Loss: 0.3587\n",
      "2025-04-04 20:09:35 - INFO - Epoch [6/10], Batch [240/374], Loss: 0.7851\n",
      "2025-04-04 20:09:36 - INFO - Epoch [6/10], Batch [250/374], Loss: 0.4329\n",
      "2025-04-04 20:09:36 - INFO - Epoch [6/10], Batch [260/374], Loss: 0.4457\n",
      "2025-04-04 20:09:37 - INFO - Epoch [6/10], Batch [270/374], Loss: 0.3555\n",
      "2025-04-04 20:09:38 - INFO - Epoch [6/10], Batch [280/374], Loss: 0.2860\n",
      "2025-04-04 20:09:38 - INFO - Epoch [6/10], Batch [290/374], Loss: 0.3098\n",
      "2025-04-04 20:09:39 - INFO - Epoch [6/10], Batch [300/374], Loss: 0.4860\n",
      "2025-04-04 20:09:40 - INFO - Epoch [6/10], Batch [310/374], Loss: 0.4452\n",
      "2025-04-04 20:09:40 - INFO - Epoch [6/10], Batch [320/374], Loss: 0.4817\n",
      "2025-04-04 20:09:41 - INFO - Epoch [6/10], Batch [330/374], Loss: 0.4391\n",
      "2025-04-04 20:09:42 - INFO - Epoch [6/10], Batch [340/374], Loss: 0.3743\n",
      "2025-04-04 20:09:42 - INFO - Epoch [6/10], Batch [350/374], Loss: 0.3200\n",
      "2025-04-04 20:09:43 - INFO - Epoch [6/10], Batch [360/374], Loss: 0.5473\n",
      "2025-04-04 20:09:44 - INFO - Epoch [6/10], Batch [370/374], Loss: 0.5026\n",
      "2025-04-04 20:09:44 - INFO - Epoch [6/10] Train Loss: 0.4189, Train Accuracy: 79.91%\n",
      "2025-04-04 20:09:46 - INFO - Epoch [6/10] Val Loss: 0.3585, Val Accuracy: 84.24%\n",
      "2025-04-04 20:09:46 - INFO - New best model at epoch 6 with val accuracy: 84.24%\n",
      "2025-04-04 20:09:46 - INFO - Epoch [7/10], Batch [0/374], Loss: 0.2941\n",
      "2025-04-04 20:09:47 - INFO - Epoch [7/10], Batch [10/374], Loss: 0.5168\n",
      "2025-04-04 20:09:47 - INFO - Epoch [7/10], Batch [20/374], Loss: 0.3429\n",
      "2025-04-04 20:09:48 - INFO - Epoch [7/10], Batch [30/374], Loss: 0.3878\n",
      "2025-04-04 20:09:49 - INFO - Epoch [7/10], Batch [40/374], Loss: 0.5414\n",
      "2025-04-04 20:09:49 - INFO - Epoch [7/10], Batch [50/374], Loss: 0.3505\n",
      "2025-04-04 20:09:50 - INFO - Epoch [7/10], Batch [60/374], Loss: 0.5921\n",
      "2025-04-04 20:09:51 - INFO - Epoch [7/10], Batch [70/374], Loss: 0.2587\n",
      "2025-04-04 20:09:51 - INFO - Epoch [7/10], Batch [80/374], Loss: 0.3568\n",
      "2025-04-04 20:09:52 - INFO - Epoch [7/10], Batch [90/374], Loss: 0.2885\n",
      "2025-04-04 20:09:53 - INFO - Epoch [7/10], Batch [100/374], Loss: 0.4238\n",
      "2025-04-04 20:09:53 - INFO - Epoch [7/10], Batch [110/374], Loss: 0.4263\n",
      "2025-04-04 20:09:54 - INFO - Epoch [7/10], Batch [120/374], Loss: 0.4126\n",
      "2025-04-04 20:09:55 - INFO - Epoch [7/10], Batch [130/374], Loss: 0.3301\n",
      "2025-04-04 20:09:56 - INFO - Epoch [7/10], Batch [140/374], Loss: 0.2898\n",
      "2025-04-04 20:09:56 - INFO - Epoch [7/10], Batch [150/374], Loss: 0.2160\n",
      "2025-04-04 20:09:57 - INFO - Epoch [7/10], Batch [160/374], Loss: 0.4347\n",
      "2025-04-04 20:09:58 - INFO - Epoch [7/10], Batch [170/374], Loss: 0.2100\n",
      "2025-04-04 20:09:58 - INFO - Epoch [7/10], Batch [180/374], Loss: 0.5093\n",
      "2025-04-04 20:09:59 - INFO - Epoch [7/10], Batch [190/374], Loss: 0.4231\n",
      "2025-04-04 20:10:00 - INFO - Epoch [7/10], Batch [200/374], Loss: 0.4068\n",
      "2025-04-04 20:10:00 - INFO - Epoch [7/10], Batch [210/374], Loss: 0.2793\n",
      "2025-04-04 20:10:01 - INFO - Epoch [7/10], Batch [220/374], Loss: 0.3377\n",
      "2025-04-04 20:10:02 - INFO - Epoch [7/10], Batch [230/374], Loss: 0.3135\n",
      "2025-04-04 20:10:02 - INFO - Epoch [7/10], Batch [240/374], Loss: 0.4560\n",
      "2025-04-04 20:10:03 - INFO - Epoch [7/10], Batch [250/374], Loss: 0.2750\n",
      "2025-04-04 20:10:04 - INFO - Epoch [7/10], Batch [260/374], Loss: 0.2622\n",
      "2025-04-04 20:10:04 - INFO - Epoch [7/10], Batch [270/374], Loss: 0.4923\n",
      "2025-04-04 20:10:05 - INFO - Epoch [7/10], Batch [280/374], Loss: 0.3131\n",
      "2025-04-04 20:10:06 - INFO - Epoch [7/10], Batch [290/374], Loss: 0.2554\n",
      "2025-04-04 20:10:06 - INFO - Epoch [7/10], Batch [300/374], Loss: 0.4267\n",
      "2025-04-04 20:10:07 - INFO - Epoch [7/10], Batch [310/374], Loss: 0.3880\n",
      "2025-04-04 20:10:08 - INFO - Epoch [7/10], Batch [320/374], Loss: 0.5381\n",
      "2025-04-04 20:10:08 - INFO - Epoch [7/10], Batch [330/374], Loss: 0.4164\n",
      "2025-04-04 20:10:09 - INFO - Epoch [7/10], Batch [340/374], Loss: 0.3711\n",
      "2025-04-04 20:10:10 - INFO - Epoch [7/10], Batch [350/374], Loss: 0.4316\n",
      "2025-04-04 20:10:10 - INFO - Epoch [7/10], Batch [360/374], Loss: 0.4417\n",
      "2025-04-04 20:10:11 - INFO - Epoch [7/10], Batch [370/374], Loss: 0.3430\n",
      "2025-04-04 20:10:11 - INFO - Epoch [7/10] Train Loss: 0.3882, Train Accuracy: 81.86%\n",
      "2025-04-04 20:10:13 - INFO - Epoch [7/10] Val Loss: 0.3250, Val Accuracy: 86.08%\n",
      "2025-04-04 20:10:13 - INFO - New best model at epoch 7 with val accuracy: 86.08%\n",
      "2025-04-04 20:10:13 - INFO - Epoch [8/10], Batch [0/374], Loss: 0.6456\n",
      "2025-04-04 20:10:14 - INFO - Epoch [8/10], Batch [10/374], Loss: 0.2247\n",
      "2025-04-04 20:10:15 - INFO - Epoch [8/10], Batch [20/374], Loss: 0.4909\n",
      "2025-04-04 20:10:15 - INFO - Epoch [8/10], Batch [30/374], Loss: 0.3387\n",
      "2025-04-04 20:10:16 - INFO - Epoch [8/10], Batch [40/374], Loss: 0.3924\n",
      "2025-04-04 20:10:17 - INFO - Epoch [8/10], Batch [50/374], Loss: 0.2837\n",
      "2025-04-04 20:10:17 - INFO - Epoch [8/10], Batch [60/374], Loss: 0.2614\n",
      "2025-04-04 20:10:18 - INFO - Epoch [8/10], Batch [70/374], Loss: 0.5213\n",
      "2025-04-04 20:10:19 - INFO - Epoch [8/10], Batch [80/374], Loss: 0.3169\n",
      "2025-04-04 20:10:19 - INFO - Epoch [8/10], Batch [90/374], Loss: 0.2928\n",
      "2025-04-04 20:10:20 - INFO - Epoch [8/10], Batch [100/374], Loss: 0.4761\n",
      "2025-04-04 20:10:21 - INFO - Epoch [8/10], Batch [110/374], Loss: 0.2234\n",
      "2025-04-04 20:10:21 - INFO - Epoch [8/10], Batch [120/374], Loss: 0.3529\n",
      "2025-04-04 20:10:22 - INFO - Epoch [8/10], Batch [130/374], Loss: 0.2938\n",
      "2025-04-04 20:10:23 - INFO - Epoch [8/10], Batch [140/374], Loss: 0.3666\n",
      "2025-04-04 20:10:23 - INFO - Epoch [8/10], Batch [150/374], Loss: 0.5168\n",
      "2025-04-04 20:10:24 - INFO - Epoch [8/10], Batch [160/374], Loss: 0.8811\n",
      "2025-04-04 20:10:25 - INFO - Epoch [8/10], Batch [170/374], Loss: 0.3488\n",
      "2025-04-04 20:10:25 - INFO - Epoch [8/10], Batch [180/374], Loss: 0.6783\n",
      "2025-04-04 20:10:26 - INFO - Epoch [8/10], Batch [190/374], Loss: 0.6346\n",
      "2025-04-04 20:10:27 - INFO - Epoch [8/10], Batch [200/374], Loss: 0.3689\n",
      "2025-04-04 20:10:27 - INFO - Epoch [8/10], Batch [210/374], Loss: 0.3511\n",
      "2025-04-04 20:10:28 - INFO - Epoch [8/10], Batch [220/374], Loss: 0.2354\n",
      "2025-04-04 20:10:29 - INFO - Epoch [8/10], Batch [230/374], Loss: 0.3427\n",
      "2025-04-04 20:10:30 - INFO - Epoch [8/10], Batch [240/374], Loss: 0.2413\n",
      "2025-04-04 20:10:30 - INFO - Epoch [8/10], Batch [250/374], Loss: 0.1921\n",
      "2025-04-04 20:10:31 - INFO - Epoch [8/10], Batch [260/374], Loss: 0.3164\n",
      "2025-04-04 20:10:32 - INFO - Epoch [8/10], Batch [270/374], Loss: 0.3153\n",
      "2025-04-04 20:10:32 - INFO - Epoch [8/10], Batch [280/374], Loss: 0.5542\n",
      "2025-04-04 20:10:33 - INFO - Epoch [8/10], Batch [290/374], Loss: 0.4630\n",
      "2025-04-04 20:10:34 - INFO - Epoch [8/10], Batch [300/374], Loss: 0.3625\n",
      "2025-04-04 20:10:34 - INFO - Epoch [8/10], Batch [310/374], Loss: 0.3481\n",
      "2025-04-04 20:10:35 - INFO - Epoch [8/10], Batch [320/374], Loss: 0.6796\n",
      "2025-04-04 20:10:36 - INFO - Epoch [8/10], Batch [330/374], Loss: 0.4063\n",
      "2025-04-04 20:10:36 - INFO - Epoch [8/10], Batch [340/374], Loss: 0.3400\n",
      "2025-04-04 20:10:37 - INFO - Epoch [8/10], Batch [350/374], Loss: 0.4249\n",
      "2025-04-04 20:10:38 - INFO - Epoch [8/10], Batch [360/374], Loss: 0.4262\n",
      "2025-04-04 20:10:38 - INFO - Epoch [8/10], Batch [370/374], Loss: 0.4107\n",
      "2025-04-04 20:10:39 - INFO - Epoch [8/10] Train Loss: 0.3743, Train Accuracy: 82.61%\n",
      "2025-04-04 20:10:40 - INFO - Epoch [8/10] Val Loss: 0.3039, Val Accuracy: 87.37%\n",
      "2025-04-04 20:10:40 - INFO - New best model at epoch 8 with val accuracy: 87.37%\n",
      "2025-04-04 20:10:41 - INFO - Epoch [9/10], Batch [0/374], Loss: 0.3833\n",
      "2025-04-04 20:10:41 - INFO - Epoch [9/10], Batch [10/374], Loss: 0.5219\n",
      "2025-04-04 20:10:42 - INFO - Epoch [9/10], Batch [20/374], Loss: 0.3310\n",
      "2025-04-04 20:10:43 - INFO - Epoch [9/10], Batch [30/374], Loss: 0.3971\n",
      "2025-04-04 20:10:43 - INFO - Epoch [9/10], Batch [40/374], Loss: 0.2863\n",
      "2025-04-04 20:10:44 - INFO - Epoch [9/10], Batch [50/374], Loss: 0.6879\n",
      "2025-04-04 20:10:45 - INFO - Epoch [9/10], Batch [60/374], Loss: 0.4315\n",
      "2025-04-04 20:10:45 - INFO - Epoch [9/10], Batch [70/374], Loss: 0.4105\n",
      "2025-04-04 20:10:46 - INFO - Epoch [9/10], Batch [80/374], Loss: 0.3640\n",
      "2025-04-04 20:10:47 - INFO - Epoch [9/10], Batch [90/374], Loss: 0.4068\n",
      "2025-04-04 20:10:47 - INFO - Epoch [9/10], Batch [100/374], Loss: 0.3634\n",
      "2025-04-04 20:10:48 - INFO - Epoch [9/10], Batch [110/374], Loss: 0.3245\n",
      "2025-04-04 20:10:49 - INFO - Epoch [9/10], Batch [120/374], Loss: 0.2949\n",
      "2025-04-04 20:10:49 - INFO - Epoch [9/10], Batch [130/374], Loss: 0.3467\n",
      "2025-04-04 20:10:50 - INFO - Epoch [9/10], Batch [140/374], Loss: 0.4638\n",
      "2025-04-04 20:10:51 - INFO - Epoch [9/10], Batch [150/374], Loss: 0.2858\n",
      "2025-04-04 20:10:51 - INFO - Epoch [9/10], Batch [160/374], Loss: 0.3680\n",
      "2025-04-04 20:10:52 - INFO - Epoch [9/10], Batch [170/374], Loss: 0.2191\n",
      "2025-04-04 20:10:53 - INFO - Epoch [9/10], Batch [180/374], Loss: 0.4242\n",
      "2025-04-04 20:10:53 - INFO - Epoch [9/10], Batch [190/374], Loss: 0.3902\n",
      "2025-04-04 20:10:54 - INFO - Epoch [9/10], Batch [200/374], Loss: 0.2792\n",
      "2025-04-04 20:10:55 - INFO - Epoch [9/10], Batch [210/374], Loss: 0.2443\n",
      "2025-04-04 20:10:55 - INFO - Epoch [9/10], Batch [220/374], Loss: 0.3500\n",
      "2025-04-04 20:10:56 - INFO - Epoch [9/10], Batch [230/374], Loss: 0.2845\n",
      "2025-04-04 20:10:57 - INFO - Epoch [9/10], Batch [240/374], Loss: 0.3468\n",
      "2025-04-04 20:10:57 - INFO - Epoch [9/10], Batch [250/374], Loss: 0.2229\n",
      "2025-04-04 20:10:58 - INFO - Epoch [9/10], Batch [260/374], Loss: 0.2025\n",
      "2025-04-04 20:10:59 - INFO - Epoch [9/10], Batch [270/374], Loss: 0.2655\n",
      "2025-04-04 20:10:59 - INFO - Epoch [9/10], Batch [280/374], Loss: 0.2774\n",
      "2025-04-04 20:11:00 - INFO - Epoch [9/10], Batch [290/374], Loss: 0.3932\n",
      "2025-04-04 20:11:01 - INFO - Epoch [9/10], Batch [300/374], Loss: 0.3808\n",
      "2025-04-04 20:11:02 - INFO - Epoch [9/10], Batch [310/374], Loss: 0.3305\n",
      "2025-04-04 20:11:02 - INFO - Epoch [9/10], Batch [320/374], Loss: 0.4577\n",
      "2025-04-04 20:11:03 - INFO - Epoch [9/10], Batch [330/374], Loss: 0.4278\n",
      "2025-04-04 20:11:04 - INFO - Epoch [9/10], Batch [340/374], Loss: 0.2618\n",
      "2025-04-04 20:11:04 - INFO - Epoch [9/10], Batch [350/374], Loss: 0.5117\n",
      "2025-04-04 20:11:05 - INFO - Epoch [9/10], Batch [360/374], Loss: 0.2076\n",
      "2025-04-04 20:11:06 - INFO - Epoch [9/10], Batch [370/374], Loss: 0.2293\n",
      "2025-04-04 20:11:06 - INFO - Epoch [9/10] Train Loss: 0.3579, Train Accuracy: 83.51%\n",
      "2025-04-04 20:11:08 - INFO - Epoch [9/10] Val Loss: 0.2914, Val Accuracy: 88.03%\n",
      "2025-04-04 20:11:08 - INFO - New best model at epoch 9 with val accuracy: 88.03%\n",
      "2025-04-04 20:11:08 - INFO - Epoch [10/10], Batch [0/374], Loss: 0.3374\n",
      "2025-04-04 20:11:09 - INFO - Epoch [10/10], Batch [10/374], Loss: 0.3736\n",
      "2025-04-04 20:11:09 - INFO - Epoch [10/10], Batch [20/374], Loss: 0.4531\n",
      "2025-04-04 20:11:10 - INFO - Epoch [10/10], Batch [30/374], Loss: 0.5231\n",
      "2025-04-04 20:11:11 - INFO - Epoch [10/10], Batch [40/374], Loss: 0.2678\n",
      "2025-04-04 20:11:11 - INFO - Epoch [10/10], Batch [50/374], Loss: 0.3781\n",
      "2025-04-04 20:11:12 - INFO - Epoch [10/10], Batch [60/374], Loss: 0.1824\n",
      "2025-04-04 20:11:13 - INFO - Epoch [10/10], Batch [70/374], Loss: 0.2925\n",
      "2025-04-04 20:11:13 - INFO - Epoch [10/10], Batch [80/374], Loss: 0.3166\n",
      "2025-04-04 20:11:14 - INFO - Epoch [10/10], Batch [90/374], Loss: 0.2956\n",
      "2025-04-04 20:11:15 - INFO - Epoch [10/10], Batch [100/374], Loss: 0.3685\n",
      "2025-04-04 20:11:15 - INFO - Epoch [10/10], Batch [110/374], Loss: 0.2750\n",
      "2025-04-04 20:11:16 - INFO - Epoch [10/10], Batch [120/374], Loss: 0.4123\n",
      "2025-04-04 20:11:17 - INFO - Epoch [10/10], Batch [130/374], Loss: 0.4147\n",
      "2025-04-04 20:11:17 - INFO - Epoch [10/10], Batch [140/374], Loss: 0.3172\n",
      "2025-04-04 20:11:18 - INFO - Epoch [10/10], Batch [150/374], Loss: 0.2925\n",
      "2025-04-04 20:11:19 - INFO - Epoch [10/10], Batch [160/374], Loss: 0.2732\n",
      "2025-04-04 20:11:19 - INFO - Epoch [10/10], Batch [170/374], Loss: 0.2563\n",
      "2025-04-04 20:11:20 - INFO - Epoch [10/10], Batch [180/374], Loss: 0.5732\n",
      "2025-04-04 20:11:21 - INFO - Epoch [10/10], Batch [190/374], Loss: 0.4567\n",
      "2025-04-04 20:11:21 - INFO - Epoch [10/10], Batch [200/374], Loss: 0.3132\n",
      "2025-04-04 20:11:22 - INFO - Epoch [10/10], Batch [210/374], Loss: 0.3159\n",
      "2025-04-04 20:11:23 - INFO - Epoch [10/10], Batch [220/374], Loss: 0.3106\n",
      "2025-04-04 20:11:23 - INFO - Epoch [10/10], Batch [230/374], Loss: 0.2136\n",
      "2025-04-04 20:11:24 - INFO - Epoch [10/10], Batch [240/374], Loss: 0.2606\n",
      "2025-04-04 20:11:25 - INFO - Epoch [10/10], Batch [250/374], Loss: 0.3547\n",
      "2025-04-04 20:11:25 - INFO - Epoch [10/10], Batch [260/374], Loss: 0.3005\n",
      "2025-04-04 20:11:26 - INFO - Epoch [10/10], Batch [270/374], Loss: 0.3062\n",
      "2025-04-04 20:11:27 - INFO - Epoch [10/10], Batch [280/374], Loss: 0.3476\n",
      "2025-04-04 20:11:27 - INFO - Epoch [10/10], Batch [290/374], Loss: 0.2663\n",
      "2025-04-04 20:11:28 - INFO - Epoch [10/10], Batch [300/374], Loss: 0.3411\n",
      "2025-04-04 20:11:29 - INFO - Epoch [10/10], Batch [310/374], Loss: 0.4638\n",
      "2025-04-04 20:11:29 - INFO - Epoch [10/10], Batch [320/374], Loss: 0.3057\n",
      "2025-04-04 20:11:30 - INFO - Epoch [10/10], Batch [330/374], Loss: 0.4519\n",
      "2025-04-04 20:11:31 - INFO - Epoch [10/10], Batch [340/374], Loss: 0.3528\n",
      "2025-04-04 20:11:31 - INFO - Epoch [10/10], Batch [350/374], Loss: 0.2483\n",
      "2025-04-04 20:11:32 - INFO - Epoch [10/10], Batch [360/374], Loss: 0.4287\n",
      "2025-04-04 20:11:33 - INFO - Epoch [10/10], Batch [370/374], Loss: 0.4043\n",
      "2025-04-04 20:11:33 - INFO - Epoch [10/10] Train Loss: 0.3488, Train Accuracy: 83.84%\n",
      "2025-04-04 20:11:35 - INFO - Epoch [10/10] Val Loss: 0.2855, Val Accuracy: 88.31%\n",
      "2025-04-04 20:11:35 - INFO - New best model at epoch 10 with val accuracy: 88.31%\n",
      "2025-04-04 20:11:36 - INFO - Test Loss: 0.2951, Test Accuracy: 88.38%\n",
      "2025-04-04 20:11:36 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-04 20:11:36 - INFO - {\n",
      "    \"world_size\": 2,\n",
      "    \"train_time\": 275.1992199420929,\n",
      "    \"avg_epoch_time\": 27.51992199420929,\n",
      "    \"val_accuracy\": 88.3089770354906,\n",
      "    \"test_accuracy\": 88.3785664578984,\n",
      "    \"test_loss\": 0.29506914924231353,\n",
      "    \"total_time\": 275.1992199420929,\n",
      "    \"train_losses\": [\n",
      "        0.6353901035516908,\n",
      "        0.6025732843429337,\n",
      "        0.568621723686704,\n",
      "        0.517621310927078,\n",
      "        0.45630984047594725,\n",
      "        0.4189116405496957,\n",
      "        0.38824075318268725,\n",
      "        0.37430600557469734,\n",
      "        0.35791079620555194,\n",
      "        0.3488222189513856\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        61.26035651518956,\n",
      "        66.13942589338019,\n",
      "        69.16896811448657,\n",
      "        73.97271738220772,\n",
      "        78.25759477780568,\n",
      "        79.91463720813458,\n",
      "        81.8562222780149,\n",
      "        82.60942338270985,\n",
      "        83.5132647083438,\n",
      "        83.8396518537116\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.5808497312256422,\n",
      "        0.5522465018157587,\n",
      "        0.49670582690335185,\n",
      "        0.4151200259320837,\n",
      "        0.3806615364410517,\n",
      "        0.35852783499447605,\n",
      "        0.3250030197446846,\n",
      "        0.30390571361626695,\n",
      "        0.2914407421875597,\n",
      "        0.2855457641552452\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        72.79053583855254,\n",
      "        75.01739735560194,\n",
      "        76.7223382045929,\n",
      "        81.62839248434238,\n",
      "        83.92484342379959,\n",
      "        84.23799582463465,\n",
      "        86.0821155184412,\n",
      "        87.36951983298539,\n",
      "        88.03061934585944,\n",
      "        88.3089770354906\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-04 20:11:36 - INFO - Model saved to models/training_using_gpus_2_best_model.pt\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes=1 --nproc_per_node=2 --rdzv_backend=c10d --rdzv_endpoint=127.0.0.1:29500 main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e07c43-37c0-4c0c-94e3-d6d8c3f1e659",
   "metadata": {},
   "source": [
    "### Calling the main function through the torchrun command to train the model in 4 GPUs with DDP, AMP and Model Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a818922c-5573-4dc7-a178-d2d4d0ae9e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0404 20:11:49.136000 1095118 site-packages/torch/distributed/run.py:793] \n",
      "W0404 20:11:49.136000 1095118 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0404 20:11:49.136000 1095118 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0404 20:11:49.136000 1095118 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Running on node: d1004Running on node: d1004Running on node: d1004\n",
      "\n",
      "\n",
      "Running with world_size: 4 (Rank: 0)Running with world_size: 4 (Rank: 2)\n",
      "\n",
      "Running with world_size: 4 (Rank: 3)\n",
      "Running on node: d1004\n",
      "Running with world_size: 4 (Rank: 1)\n",
      "2025-04-04 20:11:59 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 20:11:59 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 20:11:59 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 20:11:59 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 20:16:18 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-04 20:16:18 - INFO - [Rank 2] Data loaded.\n",
      "2025-04-04 20:16:18 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-04 20:16:18 - INFO - [Rank 3] Data loaded.\n",
      "2025-04-04 20:16:22 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-04 20:16:22 - INFO - Total layers: 129\n",
      "2025-04-04 20:16:22 - INFO - Total parameters: 22,494,274\n",
      "2025-04-04 20:16:26 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6746\n",
      "2025-04-04 20:16:27 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.7315\n",
      "2025-04-04 20:16:28 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.6218\n",
      "2025-04-04 20:16:29 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.5900\n",
      "2025-04-04 20:16:30 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.6077\n",
      "2025-04-04 20:16:31 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6250\n",
      "2025-04-04 20:16:32 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.5801\n",
      "2025-04-04 20:16:33 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.6533\n",
      "2025-04-04 20:16:34 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.5705\n",
      "2025-04-04 20:16:35 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.7005\n",
      "2025-04-04 20:16:36 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.6821\n",
      "2025-04-04 20:16:37 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.5653\n",
      "2025-04-04 20:16:38 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6346\n",
      "2025-04-04 20:16:39 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.5957\n",
      "2025-04-04 20:16:40 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5321\n",
      "2025-04-04 20:16:41 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.6153\n",
      "2025-04-04 20:16:41 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.6019\n",
      "2025-04-04 20:16:43 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5258\n",
      "2025-04-04 20:16:43 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.6157\n",
      "2025-04-04 20:16:45 - INFO - Epoch [1/10] Train Loss: 0.6354, Train Accuracy: 61.51%\n",
      "2025-04-04 20:16:47 - INFO - Epoch [1/10] Val Loss: 0.6012, Val Accuracy: 69.24%\n",
      "2025-04-04 20:16:47 - INFO - New best model at epoch 1 with val accuracy: 69.24%\n",
      "2025-04-04 20:16:48 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.6088\n",
      "2025-04-04 20:16:49 - INFO - Epoch [2/10], Batch [10/187], Loss: 0.5517\n",
      "2025-04-04 20:16:49 - INFO - Epoch [2/10], Batch [20/187], Loss: 0.6696\n",
      "2025-04-04 20:16:50 - INFO - Epoch [2/10], Batch [30/187], Loss: 0.8990\n",
      "2025-04-04 20:16:51 - INFO - Epoch [2/10], Batch [40/187], Loss: 0.5155\n",
      "2025-04-04 20:16:52 - INFO - Epoch [2/10], Batch [50/187], Loss: 0.5332\n",
      "2025-04-04 20:16:53 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.5286\n",
      "2025-04-04 20:16:54 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.5853\n",
      "2025-04-04 20:16:55 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.5493\n",
      "2025-04-04 20:16:56 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.7116\n",
      "2025-04-04 20:16:57 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.7599\n",
      "2025-04-04 20:16:58 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.6443\n",
      "2025-04-04 20:16:59 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.7446\n",
      "2025-04-04 20:17:00 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.6526\n",
      "2025-04-04 20:17:01 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.5463\n",
      "2025-04-04 20:17:02 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.5691\n",
      "2025-04-04 20:17:03 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.5675\n",
      "2025-04-04 20:17:04 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.5385\n",
      "2025-04-04 20:17:05 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.5698\n",
      "2025-04-04 20:17:05 - INFO - Epoch [2/10] Train Loss: 0.6038, Train Accuracy: 65.87%\n",
      "2025-04-04 20:17:07 - INFO - Epoch [2/10] Val Loss: 0.5311, Val Accuracy: 75.23%\n",
      "2025-04-04 20:17:07 - INFO - New best model at epoch 2 with val accuracy: 75.23%\n",
      "2025-04-04 20:17:07 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.7097\n",
      "2025-04-04 20:17:08 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.4818\n",
      "2025-04-04 20:17:09 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.4550\n",
      "2025-04-04 20:17:10 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.5839\n",
      "2025-04-04 20:17:11 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.6258\n",
      "2025-04-04 20:17:12 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.7211\n",
      "2025-04-04 20:17:13 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.6380\n",
      "2025-04-04 20:17:14 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.5205\n",
      "2025-04-04 20:17:15 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.5805\n",
      "2025-04-04 20:17:16 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.5666\n",
      "2025-04-04 20:17:17 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.6195\n",
      "2025-04-04 20:17:18 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.7919\n",
      "2025-04-04 20:17:18 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.5245\n",
      "2025-04-04 20:17:19 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5626\n",
      "2025-04-04 20:17:20 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.4384\n",
      "2025-04-04 20:17:21 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.4595\n",
      "2025-04-04 20:17:22 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.3491\n",
      "2025-04-04 20:17:23 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.5397\n",
      "2025-04-04 20:17:24 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.5856\n",
      "2025-04-04 20:17:25 - INFO - Epoch [3/10] Train Loss: 0.5587, Train Accuracy: 69.77%\n",
      "2025-04-04 20:17:26 - INFO - Epoch [3/10] Val Loss: 0.5595, Val Accuracy: 72.79%\n",
      "2025-04-04 20:17:26 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.6916\n",
      "2025-04-04 20:17:27 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.3909\n",
      "2025-04-04 20:17:28 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.4540\n",
      "2025-04-04 20:17:29 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.4821\n",
      "2025-04-04 20:17:30 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.5349\n",
      "2025-04-04 20:17:31 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.5488\n",
      "2025-04-04 20:17:32 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.5105\n",
      "2025-04-04 20:17:33 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.5179\n",
      "2025-04-04 20:17:34 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.4787\n",
      "2025-04-04 20:17:35 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.5118\n",
      "2025-04-04 20:17:36 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4374\n",
      "2025-04-04 20:17:37 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.5610\n",
      "2025-04-04 20:17:38 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.4558\n",
      "2025-04-04 20:17:39 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.4666\n",
      "2025-04-04 20:17:40 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.4519\n",
      "2025-04-04 20:17:41 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.5054\n",
      "2025-04-04 20:17:41 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.3870\n",
      "2025-04-04 20:17:42 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.4879\n",
      "2025-04-04 20:17:43 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.3606\n",
      "2025-04-04 20:17:44 - INFO - Epoch [4/10] Train Loss: 0.5034, Train Accuracy: 75.60%\n",
      "2025-04-04 20:17:45 - INFO - Epoch [4/10] Val Loss: 0.5828, Val Accuracy: 69.52%\n",
      "2025-04-04 20:17:46 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.5077\n",
      "2025-04-04 20:17:47 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.4800\n",
      "2025-04-04 20:17:48 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.4583\n",
      "2025-04-04 20:17:49 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.5439\n",
      "2025-04-04 20:17:50 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.6282\n",
      "2025-04-04 20:17:51 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.4146\n",
      "2025-04-04 20:17:52 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.4632\n",
      "2025-04-04 20:17:52 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.4585\n",
      "2025-04-04 20:17:53 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.3792\n",
      "2025-04-04 20:17:54 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.5414\n",
      "2025-04-04 20:17:55 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.4081\n",
      "2025-04-04 20:17:56 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3169\n",
      "2025-04-04 20:17:57 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.4497\n",
      "2025-04-04 20:17:58 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.3345\n",
      "2025-04-04 20:17:59 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.3193\n",
      "2025-04-04 20:18:00 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.4951\n",
      "2025-04-04 20:18:01 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.3261\n",
      "2025-04-04 20:18:02 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.3768\n",
      "2025-04-04 20:18:03 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.4649\n",
      "2025-04-04 20:18:03 - INFO - Epoch [5/10] Train Loss: 0.4497, Train Accuracy: 78.68%\n",
      "2025-04-04 20:18:04 - INFO - Epoch [5/10] Val Loss: 0.4036, Val Accuracy: 82.95%\n",
      "2025-04-04 20:18:04 - INFO - New best model at epoch 5 with val accuracy: 82.95%\n",
      "2025-04-04 20:18:05 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.4629\n",
      "2025-04-04 20:18:06 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.4344\n",
      "2025-04-04 20:18:07 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.5249\n",
      "2025-04-04 20:18:08 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.3997\n",
      "2025-04-04 20:18:09 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2492\n",
      "2025-04-04 20:18:10 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.5826\n",
      "2025-04-04 20:18:11 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.5447\n",
      "2025-04-04 20:18:12 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.5543\n",
      "2025-04-04 20:18:12 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.4047\n",
      "2025-04-04 20:18:13 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.4025\n",
      "2025-04-04 20:18:14 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.4539\n",
      "2025-04-04 20:18:15 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2890\n",
      "2025-04-04 20:18:16 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.7276\n",
      "2025-04-04 20:18:17 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.3732\n",
      "2025-04-04 20:18:18 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.3110\n",
      "2025-04-04 20:18:19 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.3659\n",
      "2025-04-04 20:18:20 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.3208\n",
      "2025-04-04 20:18:21 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.5300\n",
      "2025-04-04 20:18:22 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.3288\n",
      "2025-04-04 20:18:22 - INFO - Epoch [6/10] Train Loss: 0.4141, Train Accuracy: 80.38%\n",
      "2025-04-04 20:18:24 - INFO - Epoch [6/10] Val Loss: 0.3676, Val Accuracy: 83.23%\n",
      "2025-04-04 20:18:24 - INFO - New best model at epoch 6 with val accuracy: 83.23%\n",
      "2025-04-04 20:18:24 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.3115\n",
      "2025-04-04 20:18:25 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.4382\n",
      "2025-04-04 20:18:26 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.5259\n",
      "2025-04-04 20:18:27 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.3867\n",
      "2025-04-04 20:18:28 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.3450\n",
      "2025-04-04 20:18:29 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.4975\n",
      "2025-04-04 20:18:30 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.4336\n",
      "2025-04-04 20:18:31 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.5301\n",
      "2025-04-04 20:18:32 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.2665\n",
      "2025-04-04 20:18:33 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.2979\n",
      "2025-04-04 20:18:34 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.4105\n",
      "2025-04-04 20:18:35 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.3322\n",
      "2025-04-04 20:18:36 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.6023\n",
      "2025-04-04 20:18:37 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.4085\n",
      "2025-04-04 20:18:38 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.5231\n",
      "2025-04-04 20:18:38 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.4147\n",
      "2025-04-04 20:18:39 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.3671\n",
      "2025-04-04 20:18:40 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.5818\n",
      "2025-04-04 20:18:41 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2555\n",
      "2025-04-04 20:18:42 - INFO - Epoch [7/10] Train Loss: 0.3946, Train Accuracy: 81.07%\n",
      "2025-04-04 20:18:43 - INFO - Epoch [7/10] Val Loss: 0.3313, Val Accuracy: 85.73%\n",
      "2025-04-04 20:18:43 - INFO - New best model at epoch 7 with val accuracy: 85.73%\n",
      "2025-04-04 20:18:44 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.5147\n",
      "2025-04-04 20:18:45 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.4213\n",
      "2025-04-04 20:18:45 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.4535\n",
      "2025-04-04 20:18:46 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.2741\n",
      "2025-04-04 20:18:47 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.2820\n",
      "2025-04-04 20:18:48 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2558\n",
      "2025-04-04 20:18:49 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.2053\n",
      "2025-04-04 20:18:50 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.4048\n",
      "2025-04-04 20:18:51 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.5460\n",
      "2025-04-04 20:18:52 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.4790\n",
      "2025-04-04 20:18:53 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.3514\n",
      "2025-04-04 20:18:54 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.2763\n",
      "2025-04-04 20:18:55 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.2469\n",
      "2025-04-04 20:18:56 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.2632\n",
      "2025-04-04 20:18:57 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.3718\n",
      "2025-04-04 20:18:58 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.3687\n",
      "2025-04-04 20:18:59 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.3031\n",
      "2025-04-04 20:19:00 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3452\n",
      "2025-04-04 20:19:00 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.4195\n",
      "2025-04-04 20:19:01 - INFO - Epoch [8/10] Train Loss: 0.3584, Train Accuracy: 83.41%\n",
      "2025-04-04 20:19:02 - INFO - Epoch [8/10] Val Loss: 0.2918, Val Accuracy: 87.75%\n",
      "2025-04-04 20:19:02 - INFO - New best model at epoch 8 with val accuracy: 87.75%\n",
      "2025-04-04 20:19:03 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.2644\n",
      "2025-04-04 20:19:04 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.3997\n",
      "2025-04-04 20:19:05 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.1991\n",
      "2025-04-04 20:19:06 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.2616\n",
      "2025-04-04 20:19:07 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.3571\n",
      "2025-04-04 20:19:08 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2112\n",
      "2025-04-04 20:19:08 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.2010\n",
      "2025-04-04 20:19:09 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2972\n",
      "2025-04-04 20:19:11 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.2948\n",
      "2025-04-04 20:19:11 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.3266\n",
      "2025-04-04 20:19:12 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4104\n",
      "2025-04-04 20:19:13 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.2335\n",
      "2025-04-04 20:19:14 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.3088\n",
      "2025-04-04 20:19:15 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2593\n",
      "2025-04-04 20:19:16 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.4145\n",
      "2025-04-04 20:19:17 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.3046\n",
      "2025-04-04 20:19:18 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.3489\n",
      "2025-04-04 20:19:19 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.4187\n",
      "2025-04-04 20:19:20 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.3149\n",
      "2025-04-04 20:19:20 - INFO - Epoch [9/10] Train Loss: 0.3485, Train Accuracy: 83.65%\n",
      "2025-04-04 20:19:22 - INFO - Epoch [9/10] Val Loss: 0.2806, Val Accuracy: 88.17%\n",
      "2025-04-04 20:19:22 - INFO - New best model at epoch 9 with val accuracy: 88.17%\n",
      "2025-04-04 20:19:22 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.3990\n",
      "2025-04-04 20:19:23 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.4562\n",
      "2025-04-04 20:19:24 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.3995\n",
      "2025-04-04 20:19:25 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.3216\n",
      "2025-04-04 20:19:26 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.4217\n",
      "2025-04-04 20:19:27 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2401\n",
      "2025-04-04 20:19:28 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2273\n",
      "2025-04-04 20:19:29 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.3900\n",
      "2025-04-04 20:19:30 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.4524\n",
      "2025-04-04 20:19:31 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.5130\n",
      "2025-04-04 20:19:32 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.1921\n",
      "2025-04-04 20:19:33 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.3771\n",
      "2025-04-04 20:19:33 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.2501\n",
      "2025-04-04 20:19:34 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.2480\n",
      "2025-04-04 20:19:35 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.5998\n",
      "2025-04-04 20:19:36 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.4085\n",
      "2025-04-04 20:19:37 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.3360\n",
      "2025-04-04 20:19:38 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.3641\n",
      "2025-04-04 20:19:39 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.2918\n",
      "2025-04-04 20:19:40 - INFO - Epoch [10/10] Train Loss: 0.3361, Train Accuracy: 84.38%\n",
      "2025-04-04 20:19:41 - INFO - Epoch [10/10] Val Loss: 0.2730, Val Accuracy: 88.10%\n",
      "2025-04-04 20:19:42 - INFO - Test Loss: 0.3035, Test Accuracy: 86.79%\n",
      "2025-04-04 20:19:42 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-04 20:19:42 - INFO - {\n",
      "    \"world_size\": 4,\n",
      "    \"train_time\": 198.9313611984253,\n",
      "    \"avg_epoch_time\": 19.89313611984253,\n",
      "    \"val_accuracy\": 88.16979819067502,\n",
      "    \"test_accuracy\": 86.78720445062586,\n",
      "    \"test_loss\": 0.3034796742139506,\n",
      "    \"total_time\": 198.9313611984253,\n",
      "    \"train_losses\": [\n",
      "        0.6354442041887898,\n",
      "        0.6038203507387488,\n",
      "        0.5587076033508428,\n",
      "        0.5034305914874855,\n",
      "        0.4497242580597371,\n",
      "        0.4140833946351726,\n",
      "        0.39460167870361934,\n",
      "        0.3583885454483112,\n",
      "        0.34850505183431396,\n",
      "        0.33610638033894813\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        61.50627615062761,\n",
      "        65.8744769874477,\n",
      "        69.77405857740585,\n",
      "        75.59832635983264,\n",
      "        78.67782426778243,\n",
      "        80.38493723849372,\n",
      "        81.07112970711297,\n",
      "        83.4142259414226,\n",
      "        83.64853556485356,\n",
      "        84.38493723849372\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.6011505724243928,\n",
      "        0.5311214469253975,\n",
      "        0.5595132090934217,\n",
      "        0.5828365644151001,\n",
      "        0.4035547260368708,\n",
      "        0.36763012575124315,\n",
      "        0.3312854131187261,\n",
      "        0.2917904443598159,\n",
      "        0.28060775605356353,\n",
      "        0.2730240064025672\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        69.24147529575505,\n",
      "        75.22616562282533,\n",
      "        72.79053583855254,\n",
      "        69.51983298538622,\n",
      "        82.95059151009046,\n",
      "        83.22894919972164,\n",
      "        85.73416840640223,\n",
      "        87.75226165622826,\n",
      "        88.16979819067502,\n",
      "        88.10020876826722\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-04 20:19:43 - INFO - Model saved to models/training_using_gpus_4_best_model.pt\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes=1 --nproc_per_node=4 --rdzv_backend=c10d --rdzv_endpoint=127.0.0.1:29500 main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
