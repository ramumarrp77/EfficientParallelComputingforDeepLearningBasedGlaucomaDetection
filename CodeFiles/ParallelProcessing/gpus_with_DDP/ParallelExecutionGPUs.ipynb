{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12337238-44be-40ca-933f-e91e902c0fc0",
   "metadata": {},
   "source": [
    "# CSYE 7105 - High Perfoamnce Machine Learning & AI - Project - Team 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6e816-2a62-4b33-b192-ea26f0b936eb",
   "metadata": {},
   "source": [
    "## Analysis of Efficient Parallel Computing for Deep Learning-Based Glaucoma Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f441fb-8954-4bb7-b11b-0abb53d695eb",
   "metadata": {},
   "source": [
    "### Parallel Execution using GPUs with DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab9576-85ac-4b6b-bcb6-40f44b2fe84f",
   "metadata": {},
   "source": [
    "### Calling the main function through the torchrun command to train the model in 2 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dce895d-f04d-4c90-a4e9-b96d0330c056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0404 18:16:50.538000 1090676 site-packages/torch/distributed/run.py:793] \n",
      "W0404 18:16:50.538000 1090676 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0404 18:16:50.538000 1090676 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0404 18:16:50.538000 1090676 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "2025-04-04 18:17:00 - INFO - Running on node: d1004\n",
      "2025-04-04 18:17:00 - INFO - Running on node: d1004\n",
      "2025-04-04 18:17:00 - INFO - Running with world_size: 2 (Rank: 0)\n",
      "2025-04-04 18:17:00 - INFO - Running with world_size: 2 (Rank: 1)\n",
      "2025-04-04 18:17:00 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 18:17:00 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 18:19:27 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-04 18:19:27 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-04 18:19:28 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-04 18:19:28 - INFO - Total layers: 129\n",
      "2025-04-04 18:19:28 - INFO - Total parameters: 22,494,274\n",
      "2025-04-04 18:19:31 - INFO - Epoch [1/10], Batch [0/374], Loss: 0.7141\n",
      "2025-04-04 18:19:32 - INFO - Epoch [1/10], Batch [10/374], Loss: 0.6994\n",
      "2025-04-04 18:19:32 - INFO - Epoch [1/10], Batch [20/374], Loss: 0.6995\n",
      "2025-04-04 18:19:33 - INFO - Epoch [1/10], Batch [30/374], Loss: 0.6898\n",
      "2025-04-04 18:19:34 - INFO - Epoch [1/10], Batch [40/374], Loss: 0.6418\n",
      "2025-04-04 18:19:34 - INFO - Epoch [1/10], Batch [50/374], Loss: 0.6651\n",
      "2025-04-04 18:19:35 - INFO - Epoch [1/10], Batch [60/374], Loss: 0.6095\n",
      "2025-04-04 18:19:36 - INFO - Epoch [1/10], Batch [70/374], Loss: 0.7367\n",
      "2025-04-04 18:19:36 - INFO - Epoch [1/10], Batch [80/374], Loss: 0.6121\n",
      "2025-04-04 18:19:37 - INFO - Epoch [1/10], Batch [90/374], Loss: 0.6569\n",
      "2025-04-04 18:19:38 - INFO - Epoch [1/10], Batch [100/374], Loss: 0.6521\n",
      "2025-04-04 18:19:38 - INFO - Epoch [1/10], Batch [110/374], Loss: 0.6105\n",
      "2025-04-04 18:19:39 - INFO - Epoch [1/10], Batch [120/374], Loss: 0.5894\n",
      "2025-04-04 18:19:40 - INFO - Epoch [1/10], Batch [130/374], Loss: 0.6142\n",
      "2025-04-04 18:19:40 - INFO - Epoch [1/10], Batch [140/374], Loss: 0.6769\n",
      "2025-04-04 18:19:41 - INFO - Epoch [1/10], Batch [150/374], Loss: 0.7669\n",
      "2025-04-04 18:19:42 - INFO - Epoch [1/10], Batch [160/374], Loss: 0.5977\n",
      "2025-04-04 18:19:42 - INFO - Epoch [1/10], Batch [170/374], Loss: 0.6538\n",
      "2025-04-04 18:19:43 - INFO - Epoch [1/10], Batch [180/374], Loss: 0.5993\n",
      "2025-04-04 18:19:44 - INFO - Epoch [1/10], Batch [190/374], Loss: 0.6294\n",
      "2025-04-04 18:19:44 - INFO - Epoch [1/10], Batch [200/374], Loss: 0.7264\n",
      "2025-04-04 18:19:45 - INFO - Epoch [1/10], Batch [210/374], Loss: 0.7083\n",
      "2025-04-04 18:19:46 - INFO - Epoch [1/10], Batch [220/374], Loss: 0.6325\n",
      "2025-04-04 18:19:46 - INFO - Epoch [1/10], Batch [230/374], Loss: 0.6213\n",
      "2025-04-04 18:19:47 - INFO - Epoch [1/10], Batch [240/374], Loss: 0.6378\n",
      "2025-04-04 18:19:48 - INFO - Epoch [1/10], Batch [250/374], Loss: 0.5909\n",
      "2025-04-04 18:19:49 - INFO - Epoch [1/10], Batch [260/374], Loss: 0.6022\n",
      "2025-04-04 18:19:49 - INFO - Epoch [1/10], Batch [270/374], Loss: 0.5620\n",
      "2025-04-04 18:19:50 - INFO - Epoch [1/10], Batch [280/374], Loss: 0.5374\n",
      "2025-04-04 18:19:51 - INFO - Epoch [1/10], Batch [290/374], Loss: 0.6203\n",
      "2025-04-04 18:19:51 - INFO - Epoch [1/10], Batch [300/374], Loss: 0.5665\n",
      "2025-04-04 18:19:52 - INFO - Epoch [1/10], Batch [310/374], Loss: 0.6443\n",
      "2025-04-04 18:19:53 - INFO - Epoch [1/10], Batch [320/374], Loss: 0.7561\n",
      "2025-04-04 18:19:53 - INFO - Epoch [1/10], Batch [330/374], Loss: 0.7385\n",
      "2025-04-04 18:19:54 - INFO - Epoch [1/10], Batch [340/374], Loss: 0.5876\n",
      "2025-04-04 18:19:55 - INFO - Epoch [1/10], Batch [350/374], Loss: 0.6922\n",
      "2025-04-04 18:19:55 - INFO - Epoch [1/10], Batch [360/374], Loss: 0.5937\n",
      "2025-04-04 18:19:56 - INFO - Epoch [1/10], Batch [370/374], Loss: 0.5238\n",
      "2025-04-04 18:19:57 - INFO - Epoch [1/10] Train Loss: 0.6457, Train Accuracy: 60.35%\n",
      "2025-04-04 18:19:59 - INFO - Epoch [1/10] Val Loss: 0.5886, Val Accuracy: 72.23%\n",
      "2025-04-04 18:19:59 - INFO - New best model at epoch 1 with val accuracy: 72.23%\n",
      "2025-04-04 18:20:00 - INFO - Epoch [2/10], Batch [0/374], Loss: 0.5933\n",
      "2025-04-04 18:20:00 - INFO - Epoch [2/10], Batch [10/374], Loss: 0.5978\n",
      "2025-04-04 18:20:01 - INFO - Epoch [2/10], Batch [20/374], Loss: 0.5154\n",
      "2025-04-04 18:20:02 - INFO - Epoch [2/10], Batch [30/374], Loss: 0.6117\n",
      "2025-04-04 18:20:02 - INFO - Epoch [2/10], Batch [40/374], Loss: 0.7490\n",
      "2025-04-04 18:20:03 - INFO - Epoch [2/10], Batch [50/374], Loss: 0.5180\n",
      "2025-04-04 18:20:04 - INFO - Epoch [2/10], Batch [60/374], Loss: 0.7715\n",
      "2025-04-04 18:20:04 - INFO - Epoch [2/10], Batch [70/374], Loss: 0.7290\n",
      "2025-04-04 18:20:05 - INFO - Epoch [2/10], Batch [80/374], Loss: 0.5581\n",
      "2025-04-04 18:20:06 - INFO - Epoch [2/10], Batch [90/374], Loss: 0.6608\n",
      "2025-04-04 18:20:06 - INFO - Epoch [2/10], Batch [100/374], Loss: 0.5129\n",
      "2025-04-04 18:20:07 - INFO - Epoch [2/10], Batch [110/374], Loss: 0.6036\n",
      "2025-04-04 18:20:08 - INFO - Epoch [2/10], Batch [120/374], Loss: 0.4742\n",
      "2025-04-04 18:20:08 - INFO - Epoch [2/10], Batch [130/374], Loss: 0.5588\n",
      "2025-04-04 18:20:09 - INFO - Epoch [2/10], Batch [140/374], Loss: 0.6375\n",
      "2025-04-04 18:20:10 - INFO - Epoch [2/10], Batch [150/374], Loss: 0.5445\n",
      "2025-04-04 18:20:11 - INFO - Epoch [2/10], Batch [160/374], Loss: 0.6354\n",
      "2025-04-04 18:20:11 - INFO - Epoch [2/10], Batch [170/374], Loss: 0.6594\n",
      "2025-04-04 18:20:12 - INFO - Epoch [2/10], Batch [180/374], Loss: 0.6117\n",
      "2025-04-04 18:20:13 - INFO - Epoch [2/10], Batch [190/374], Loss: 0.5572\n",
      "2025-04-04 18:20:13 - INFO - Epoch [2/10], Batch [200/374], Loss: 0.6952\n",
      "2025-04-04 18:20:14 - INFO - Epoch [2/10], Batch [210/374], Loss: 0.5924\n",
      "2025-04-04 18:20:15 - INFO - Epoch [2/10], Batch [220/374], Loss: 0.7743\n",
      "2025-04-04 18:20:15 - INFO - Epoch [2/10], Batch [230/374], Loss: 0.5966\n",
      "2025-04-04 18:20:16 - INFO - Epoch [2/10], Batch [240/374], Loss: 0.6893\n",
      "2025-04-04 18:20:17 - INFO - Epoch [2/10], Batch [250/374], Loss: 0.7507\n",
      "2025-04-04 18:20:17 - INFO - Epoch [2/10], Batch [260/374], Loss: 0.6502\n",
      "2025-04-04 18:20:18 - INFO - Epoch [2/10], Batch [270/374], Loss: 0.5308\n",
      "2025-04-04 18:20:19 - INFO - Epoch [2/10], Batch [280/374], Loss: 0.6160\n",
      "2025-04-04 18:20:19 - INFO - Epoch [2/10], Batch [290/374], Loss: 0.6076\n",
      "2025-04-04 18:20:20 - INFO - Epoch [2/10], Batch [300/374], Loss: 0.5542\n",
      "2025-04-04 18:20:21 - INFO - Epoch [2/10], Batch [310/374], Loss: 0.6101\n",
      "2025-04-04 18:20:21 - INFO - Epoch [2/10], Batch [320/374], Loss: 0.7038\n",
      "2025-04-04 18:20:22 - INFO - Epoch [2/10], Batch [330/374], Loss: 0.5431\n",
      "2025-04-04 18:20:23 - INFO - Epoch [2/10], Batch [340/374], Loss: 0.5093\n",
      "2025-04-04 18:20:23 - INFO - Epoch [2/10], Batch [350/374], Loss: 0.5875\n",
      "2025-04-04 18:20:24 - INFO - Epoch [2/10], Batch [360/374], Loss: 0.7251\n",
      "2025-04-04 18:20:25 - INFO - Epoch [2/10], Batch [370/374], Loss: 0.6499\n",
      "2025-04-04 18:20:25 - INFO - Epoch [2/10] Train Loss: 0.6137, Train Accuracy: 65.08%\n",
      "2025-04-04 18:20:27 - INFO - Epoch [2/10] Val Loss: 0.5592, Val Accuracy: 74.11%\n",
      "2025-04-04 18:20:27 - INFO - New best model at epoch 2 with val accuracy: 74.11%\n",
      "2025-04-04 18:20:27 - INFO - Epoch [3/10], Batch [0/374], Loss: 0.8303\n",
      "2025-04-04 18:20:28 - INFO - Epoch [3/10], Batch [10/374], Loss: 0.6577\n",
      "2025-04-04 18:20:29 - INFO - Epoch [3/10], Batch [20/374], Loss: 0.6633\n",
      "2025-04-04 18:20:29 - INFO - Epoch [3/10], Batch [30/374], Loss: 0.5537\n",
      "2025-04-04 18:20:30 - INFO - Epoch [3/10], Batch [40/374], Loss: 0.4706\n",
      "2025-04-04 18:20:31 - INFO - Epoch [3/10], Batch [50/374], Loss: 0.6942\n",
      "2025-04-04 18:20:31 - INFO - Epoch [3/10], Batch [60/374], Loss: 0.6380\n",
      "2025-04-04 18:20:32 - INFO - Epoch [3/10], Batch [70/374], Loss: 0.5132\n",
      "2025-04-04 18:20:33 - INFO - Epoch [3/10], Batch [80/374], Loss: 0.6585\n",
      "2025-04-04 18:20:33 - INFO - Epoch [3/10], Batch [90/374], Loss: 0.5497\n",
      "2025-04-04 18:20:34 - INFO - Epoch [3/10], Batch [100/374], Loss: 0.5445\n",
      "2025-04-04 18:20:35 - INFO - Epoch [3/10], Batch [110/374], Loss: 0.5890\n",
      "2025-04-04 18:20:35 - INFO - Epoch [3/10], Batch [120/374], Loss: 0.6060\n",
      "2025-04-04 18:20:36 - INFO - Epoch [3/10], Batch [130/374], Loss: 0.5868\n",
      "2025-04-04 18:20:37 - INFO - Epoch [3/10], Batch [140/374], Loss: 0.5714\n",
      "2025-04-04 18:20:37 - INFO - Epoch [3/10], Batch [150/374], Loss: 0.5349\n",
      "2025-04-04 18:20:38 - INFO - Epoch [3/10], Batch [160/374], Loss: 0.6928\n",
      "2025-04-04 18:20:39 - INFO - Epoch [3/10], Batch [170/374], Loss: 0.7201\n",
      "2025-04-04 18:20:39 - INFO - Epoch [3/10], Batch [180/374], Loss: 0.6462\n",
      "2025-04-04 18:20:40 - INFO - Epoch [3/10], Batch [190/374], Loss: 0.6813\n",
      "2025-04-04 18:20:41 - INFO - Epoch [3/10], Batch [200/374], Loss: 0.5887\n",
      "2025-04-04 18:20:41 - INFO - Epoch [3/10], Batch [210/374], Loss: 0.7486\n",
      "2025-04-04 18:20:42 - INFO - Epoch [3/10], Batch [220/374], Loss: 0.6267\n",
      "2025-04-04 18:20:43 - INFO - Epoch [3/10], Batch [230/374], Loss: 0.5883\n",
      "2025-04-04 18:20:43 - INFO - Epoch [3/10], Batch [240/374], Loss: 0.5583\n",
      "2025-04-04 18:20:44 - INFO - Epoch [3/10], Batch [250/374], Loss: 0.6025\n",
      "2025-04-04 18:20:45 - INFO - Epoch [3/10], Batch [260/374], Loss: 0.5775\n",
      "2025-04-04 18:20:46 - INFO - Epoch [3/10], Batch [270/374], Loss: 0.5196\n",
      "2025-04-04 18:20:46 - INFO - Epoch [3/10], Batch [280/374], Loss: 0.5807\n",
      "2025-04-04 18:20:47 - INFO - Epoch [3/10], Batch [290/374], Loss: 0.6140\n",
      "2025-04-04 18:20:48 - INFO - Epoch [3/10], Batch [300/374], Loss: 0.4639\n",
      "2025-04-04 18:20:48 - INFO - Epoch [3/10], Batch [310/374], Loss: 0.6284\n",
      "2025-04-04 18:20:49 - INFO - Epoch [3/10], Batch [320/374], Loss: 0.5168\n",
      "2025-04-04 18:20:50 - INFO - Epoch [3/10], Batch [330/374], Loss: 0.6365\n",
      "2025-04-04 18:20:50 - INFO - Epoch [3/10], Batch [340/374], Loss: 0.5102\n",
      "2025-04-04 18:20:51 - INFO - Epoch [3/10], Batch [350/374], Loss: 0.6757\n",
      "2025-04-04 18:20:52 - INFO - Epoch [3/10], Batch [360/374], Loss: 0.5772\n",
      "2025-04-04 18:20:52 - INFO - Epoch [3/10], Batch [370/374], Loss: 0.4737\n",
      "2025-04-04 18:20:52 - INFO - Epoch [3/10] Train Loss: 0.5789, Train Accuracy: 67.87%\n",
      "2025-04-04 18:20:54 - INFO - Epoch [3/10] Val Loss: 0.5201, Val Accuracy: 76.20%\n",
      "2025-04-04 18:20:54 - INFO - New best model at epoch 3 with val accuracy: 76.20%\n",
      "2025-04-04 18:20:55 - INFO - Epoch [4/10], Batch [0/374], Loss: 0.5741\n",
      "2025-04-04 18:20:55 - INFO - Epoch [4/10], Batch [10/374], Loss: 0.5808\n",
      "2025-04-04 18:20:56 - INFO - Epoch [4/10], Batch [20/374], Loss: 0.4880\n",
      "2025-04-04 18:20:57 - INFO - Epoch [4/10], Batch [30/374], Loss: 0.5359\n",
      "2025-04-04 18:20:57 - INFO - Epoch [4/10], Batch [40/374], Loss: 0.5035\n",
      "2025-04-04 18:20:58 - INFO - Epoch [4/10], Batch [50/374], Loss: 0.5705\n",
      "2025-04-04 18:20:59 - INFO - Epoch [4/10], Batch [60/374], Loss: 0.5931\n",
      "2025-04-04 18:20:59 - INFO - Epoch [4/10], Batch [70/374], Loss: 0.5701\n",
      "2025-04-04 18:21:00 - INFO - Epoch [4/10], Batch [80/374], Loss: 0.7088\n",
      "2025-04-04 18:21:01 - INFO - Epoch [4/10], Batch [90/374], Loss: 0.4796\n",
      "2025-04-04 18:21:02 - INFO - Epoch [4/10], Batch [100/374], Loss: 0.4592\n",
      "2025-04-04 18:21:02 - INFO - Epoch [4/10], Batch [110/374], Loss: 0.5511\n",
      "2025-04-04 18:21:03 - INFO - Epoch [4/10], Batch [120/374], Loss: 0.5705\n",
      "2025-04-04 18:21:04 - INFO - Epoch [4/10], Batch [130/374], Loss: 0.5825\n",
      "2025-04-04 18:21:04 - INFO - Epoch [4/10], Batch [140/374], Loss: 0.4654\n",
      "2025-04-04 18:21:05 - INFO - Epoch [4/10], Batch [150/374], Loss: 0.5976\n",
      "2025-04-04 18:21:06 - INFO - Epoch [4/10], Batch [160/374], Loss: 0.6438\n",
      "2025-04-04 18:21:06 - INFO - Epoch [4/10], Batch [170/374], Loss: 0.4226\n",
      "2025-04-04 18:21:07 - INFO - Epoch [4/10], Batch [180/374], Loss: 0.6333\n",
      "2025-04-04 18:21:08 - INFO - Epoch [4/10], Batch [190/374], Loss: 0.4968\n",
      "2025-04-04 18:21:08 - INFO - Epoch [4/10], Batch [200/374], Loss: 0.5694\n",
      "2025-04-04 18:21:09 - INFO - Epoch [4/10], Batch [210/374], Loss: 0.4092\n",
      "2025-04-04 18:21:10 - INFO - Epoch [4/10], Batch [220/374], Loss: 0.4456\n",
      "2025-04-04 18:21:10 - INFO - Epoch [4/10], Batch [230/374], Loss: 0.4991\n",
      "2025-04-04 18:21:11 - INFO - Epoch [4/10], Batch [240/374], Loss: 0.6586\n",
      "2025-04-04 18:21:12 - INFO - Epoch [4/10], Batch [250/374], Loss: 0.5635\n",
      "2025-04-04 18:21:13 - INFO - Epoch [4/10], Batch [260/374], Loss: 0.6505\n",
      "2025-04-04 18:21:13 - INFO - Epoch [4/10], Batch [270/374], Loss: 0.4961\n",
      "2025-04-04 18:21:14 - INFO - Epoch [4/10], Batch [280/374], Loss: 0.4871\n",
      "2025-04-04 18:21:15 - INFO - Epoch [4/10], Batch [290/374], Loss: 0.4304\n",
      "2025-04-04 18:21:15 - INFO - Epoch [4/10], Batch [300/374], Loss: 0.4955\n",
      "2025-04-04 18:21:16 - INFO - Epoch [4/10], Batch [310/374], Loss: 0.3954\n",
      "2025-04-04 18:21:17 - INFO - Epoch [4/10], Batch [320/374], Loss: 0.4744\n",
      "2025-04-04 18:21:17 - INFO - Epoch [4/10], Batch [330/374], Loss: 0.4225\n",
      "2025-04-04 18:21:18 - INFO - Epoch [4/10], Batch [340/374], Loss: 0.4207\n",
      "2025-04-04 18:21:19 - INFO - Epoch [4/10], Batch [350/374], Loss: 0.4938\n",
      "2025-04-04 18:21:19 - INFO - Epoch [4/10], Batch [360/374], Loss: 0.4195\n",
      "2025-04-04 18:21:20 - INFO - Epoch [4/10], Batch [370/374], Loss: 0.5300\n",
      "2025-04-04 18:21:20 - INFO - Epoch [4/10] Train Loss: 0.5270, Train Accuracy: 73.46%\n",
      "2025-04-04 18:21:22 - INFO - Epoch [4/10] Val Loss: 0.4491, Val Accuracy: 79.75%\n",
      "2025-04-04 18:21:22 - INFO - New best model at epoch 4 with val accuracy: 79.75%\n",
      "2025-04-04 18:21:22 - INFO - Epoch [5/10], Batch [0/374], Loss: 0.5040\n",
      "2025-04-04 18:21:23 - INFO - Epoch [5/10], Batch [10/374], Loss: 0.3705\n",
      "2025-04-04 18:21:24 - INFO - Epoch [5/10], Batch [20/374], Loss: 0.5199\n",
      "2025-04-04 18:21:24 - INFO - Epoch [5/10], Batch [30/374], Loss: 0.3397\n",
      "2025-04-04 18:21:25 - INFO - Epoch [5/10], Batch [40/374], Loss: 0.4142\n",
      "2025-04-04 18:21:26 - INFO - Epoch [5/10], Batch [50/374], Loss: 0.3057\n",
      "2025-04-04 18:21:26 - INFO - Epoch [5/10], Batch [60/374], Loss: 0.4835\n",
      "2025-04-04 18:21:27 - INFO - Epoch [5/10], Batch [70/374], Loss: 0.4323\n",
      "2025-04-04 18:21:28 - INFO - Epoch [5/10], Batch [80/374], Loss: 0.4129\n",
      "2025-04-04 18:21:28 - INFO - Epoch [5/10], Batch [90/374], Loss: 0.3722\n",
      "2025-04-04 18:21:29 - INFO - Epoch [5/10], Batch [100/374], Loss: 0.4201\n",
      "2025-04-04 18:21:30 - INFO - Epoch [5/10], Batch [110/374], Loss: 0.4936\n",
      "2025-04-04 18:21:30 - INFO - Epoch [5/10], Batch [120/374], Loss: 0.4172\n",
      "2025-04-04 18:21:31 - INFO - Epoch [5/10], Batch [130/374], Loss: 0.4883\n",
      "2025-04-04 18:21:32 - INFO - Epoch [5/10], Batch [140/374], Loss: 0.4137\n",
      "2025-04-04 18:21:32 - INFO - Epoch [5/10], Batch [150/374], Loss: 0.5189\n",
      "2025-04-04 18:21:33 - INFO - Epoch [5/10], Batch [160/374], Loss: 0.4112\n",
      "2025-04-04 18:21:34 - INFO - Epoch [5/10], Batch [170/374], Loss: 0.5037\n",
      "2025-04-04 18:21:35 - INFO - Epoch [5/10], Batch [180/374], Loss: 0.5092\n",
      "2025-04-04 18:21:35 - INFO - Epoch [5/10], Batch [190/374], Loss: 0.5910\n",
      "2025-04-04 18:21:36 - INFO - Epoch [5/10], Batch [200/374], Loss: 0.6293\n",
      "2025-04-04 18:21:37 - INFO - Epoch [5/10], Batch [210/374], Loss: 0.2980\n",
      "2025-04-04 18:21:37 - INFO - Epoch [5/10], Batch [220/374], Loss: 0.4019\n",
      "2025-04-04 18:21:38 - INFO - Epoch [5/10], Batch [230/374], Loss: 0.6013\n",
      "2025-04-04 18:21:39 - INFO - Epoch [5/10], Batch [240/374], Loss: 0.4640\n",
      "2025-04-04 18:21:39 - INFO - Epoch [5/10], Batch [250/374], Loss: 0.6238\n",
      "2025-04-04 18:21:40 - INFO - Epoch [5/10], Batch [260/374], Loss: 0.3568\n",
      "2025-04-04 18:21:41 - INFO - Epoch [5/10], Batch [270/374], Loss: 0.5002\n",
      "2025-04-04 18:21:41 - INFO - Epoch [5/10], Batch [280/374], Loss: 0.4753\n",
      "2025-04-04 18:21:42 - INFO - Epoch [5/10], Batch [290/374], Loss: 0.5253\n",
      "2025-04-04 18:21:43 - INFO - Epoch [5/10], Batch [300/374], Loss: 0.5183\n",
      "2025-04-04 18:21:43 - INFO - Epoch [5/10], Batch [310/374], Loss: 0.3538\n",
      "2025-04-04 18:21:44 - INFO - Epoch [5/10], Batch [320/374], Loss: 0.3190\n",
      "2025-04-04 18:21:45 - INFO - Epoch [5/10], Batch [330/374], Loss: 0.4019\n",
      "2025-04-04 18:21:45 - INFO - Epoch [5/10], Batch [340/374], Loss: 0.3634\n",
      "2025-04-04 18:21:46 - INFO - Epoch [5/10], Batch [350/374], Loss: 0.2995\n",
      "2025-04-04 18:21:47 - INFO - Epoch [5/10], Batch [360/374], Loss: 0.5556\n",
      "2025-04-04 18:21:48 - INFO - Epoch [5/10], Batch [370/374], Loss: 0.6212\n",
      "2025-04-04 18:21:48 - INFO - Epoch [5/10] Train Loss: 0.4697, Train Accuracy: 77.22%\n",
      "2025-04-04 18:21:50 - INFO - Epoch [5/10] Val Loss: 0.4199, Val Accuracy: 80.76%\n",
      "2025-04-04 18:21:50 - INFO - New best model at epoch 5 with val accuracy: 80.76%\n",
      "2025-04-04 18:21:50 - INFO - Epoch [6/10], Batch [0/374], Loss: 0.4981\n",
      "2025-04-04 18:21:51 - INFO - Epoch [6/10], Batch [10/374], Loss: 0.5088\n",
      "2025-04-04 18:21:51 - INFO - Epoch [6/10], Batch [20/374], Loss: 0.5110\n",
      "2025-04-04 18:21:52 - INFO - Epoch [6/10], Batch [30/374], Loss: 0.4144\n",
      "2025-04-04 18:21:53 - INFO - Epoch [6/10], Batch [40/374], Loss: 0.4797\n",
      "2025-04-04 18:21:53 - INFO - Epoch [6/10], Batch [50/374], Loss: 0.3919\n",
      "2025-04-04 18:21:54 - INFO - Epoch [6/10], Batch [60/374], Loss: 0.3737\n",
      "2025-04-04 18:21:55 - INFO - Epoch [6/10], Batch [70/374], Loss: 0.4572\n",
      "2025-04-04 18:21:55 - INFO - Epoch [6/10], Batch [80/374], Loss: 0.3885\n",
      "2025-04-04 18:21:56 - INFO - Epoch [6/10], Batch [90/374], Loss: 0.3971\n",
      "2025-04-04 18:21:57 - INFO - Epoch [6/10], Batch [100/374], Loss: 0.3576\n",
      "2025-04-04 18:21:57 - INFO - Epoch [6/10], Batch [110/374], Loss: 0.3920\n",
      "2025-04-04 18:21:58 - INFO - Epoch [6/10], Batch [120/374], Loss: 0.5224\n",
      "2025-04-04 18:21:59 - INFO - Epoch [6/10], Batch [130/374], Loss: 0.3236\n",
      "2025-04-04 18:22:00 - INFO - Epoch [6/10], Batch [140/374], Loss: 0.3935\n",
      "2025-04-04 18:22:00 - INFO - Epoch [6/10], Batch [150/374], Loss: 0.4091\n",
      "2025-04-04 18:22:01 - INFO - Epoch [6/10], Batch [160/374], Loss: 0.3640\n",
      "2025-04-04 18:22:02 - INFO - Epoch [6/10], Batch [170/374], Loss: 0.3191\n",
      "2025-04-04 18:22:02 - INFO - Epoch [6/10], Batch [180/374], Loss: 0.4729\n",
      "2025-04-04 18:22:03 - INFO - Epoch [6/10], Batch [190/374], Loss: 0.2902\n",
      "2025-04-04 18:22:04 - INFO - Epoch [6/10], Batch [200/374], Loss: 0.3321\n",
      "2025-04-04 18:22:04 - INFO - Epoch [6/10], Batch [210/374], Loss: 0.3580\n",
      "2025-04-04 18:22:05 - INFO - Epoch [6/10], Batch [220/374], Loss: 0.5066\n",
      "2025-04-04 18:22:06 - INFO - Epoch [6/10], Batch [230/374], Loss: 0.4385\n",
      "2025-04-04 18:22:06 - INFO - Epoch [6/10], Batch [240/374], Loss: 0.6433\n",
      "2025-04-04 18:22:07 - INFO - Epoch [6/10], Batch [250/374], Loss: 0.3855\n",
      "2025-04-04 18:22:08 - INFO - Epoch [6/10], Batch [260/374], Loss: 0.3815\n",
      "2025-04-04 18:22:08 - INFO - Epoch [6/10], Batch [270/374], Loss: 0.2901\n",
      "2025-04-04 18:22:09 - INFO - Epoch [6/10], Batch [280/374], Loss: 0.2789\n",
      "2025-04-04 18:22:10 - INFO - Epoch [6/10], Batch [290/374], Loss: 0.2769\n",
      "2025-04-04 18:22:10 - INFO - Epoch [6/10], Batch [300/374], Loss: 0.4415\n",
      "2025-04-04 18:22:11 - INFO - Epoch [6/10], Batch [310/374], Loss: 0.4212\n",
      "2025-04-04 18:22:12 - INFO - Epoch [6/10], Batch [320/374], Loss: 0.4836\n",
      "2025-04-04 18:22:12 - INFO - Epoch [6/10], Batch [330/374], Loss: 0.5175\n",
      "2025-04-04 18:22:13 - INFO - Epoch [6/10], Batch [340/374], Loss: 0.3379\n",
      "2025-04-04 18:22:14 - INFO - Epoch [6/10], Batch [350/374], Loss: 0.4156\n",
      "2025-04-04 18:22:14 - INFO - Epoch [6/10], Batch [360/374], Loss: 0.5953\n",
      "2025-04-04 18:22:15 - INFO - Epoch [6/10], Batch [370/374], Loss: 0.5745\n",
      "2025-04-04 18:22:15 - INFO - Epoch [6/10] Train Loss: 0.4371, Train Accuracy: 79.24%\n",
      "2025-04-04 18:22:17 - INFO - Epoch [6/10] Val Loss: 0.3649, Val Accuracy: 84.03%\n",
      "2025-04-04 18:22:17 - INFO - New best model at epoch 6 with val accuracy: 84.03%\n",
      "2025-04-04 18:22:18 - INFO - Epoch [7/10], Batch [0/374], Loss: 0.3438\n",
      "2025-04-04 18:22:18 - INFO - Epoch [7/10], Batch [10/374], Loss: 0.4218\n",
      "2025-04-04 18:22:19 - INFO - Epoch [7/10], Batch [20/374], Loss: 0.3221\n",
      "2025-04-04 18:22:20 - INFO - Epoch [7/10], Batch [30/374], Loss: 0.3471\n",
      "2025-04-04 18:22:20 - INFO - Epoch [7/10], Batch [40/374], Loss: 0.6581\n",
      "2025-04-04 18:22:21 - INFO - Epoch [7/10], Batch [50/374], Loss: 0.4497\n",
      "2025-04-04 18:22:22 - INFO - Epoch [7/10], Batch [60/374], Loss: 0.6104\n",
      "2025-04-04 18:22:22 - INFO - Epoch [7/10], Batch [70/374], Loss: 0.2961\n",
      "2025-04-04 18:22:23 - INFO - Epoch [7/10], Batch [80/374], Loss: 0.3968\n",
      "2025-04-04 18:22:24 - INFO - Epoch [7/10], Batch [90/374], Loss: 0.2428\n",
      "2025-04-04 18:22:24 - INFO - Epoch [7/10], Batch [100/374], Loss: 0.4741\n",
      "2025-04-04 18:22:25 - INFO - Epoch [7/10], Batch [110/374], Loss: 0.3476\n",
      "2025-04-04 18:22:26 - INFO - Epoch [7/10], Batch [120/374], Loss: 0.4863\n",
      "2025-04-04 18:22:26 - INFO - Epoch [7/10], Batch [130/374], Loss: 0.3445\n",
      "2025-04-04 18:22:27 - INFO - Epoch [7/10], Batch [140/374], Loss: 0.2588\n",
      "2025-04-04 18:22:28 - INFO - Epoch [7/10], Batch [150/374], Loss: 0.2655\n",
      "2025-04-04 18:22:28 - INFO - Epoch [7/10], Batch [160/374], Loss: 0.4868\n",
      "2025-04-04 18:22:29 - INFO - Epoch [7/10], Batch [170/374], Loss: 0.3767\n",
      "2025-04-04 18:22:30 - INFO - Epoch [7/10], Batch [180/374], Loss: 0.4635\n",
      "2025-04-04 18:22:31 - INFO - Epoch [7/10], Batch [190/374], Loss: 0.4814\n",
      "2025-04-04 18:22:31 - INFO - Epoch [7/10], Batch [200/374], Loss: 0.5756\n",
      "2025-04-04 18:22:32 - INFO - Epoch [7/10], Batch [210/374], Loss: 0.3080\n",
      "2025-04-04 18:22:33 - INFO - Epoch [7/10], Batch [220/374], Loss: 0.3583\n",
      "2025-04-04 18:22:33 - INFO - Epoch [7/10], Batch [230/374], Loss: 0.3658\n",
      "2025-04-04 18:22:34 - INFO - Epoch [7/10], Batch [240/374], Loss: 0.4140\n",
      "2025-04-04 18:22:35 - INFO - Epoch [7/10], Batch [250/374], Loss: 0.3497\n",
      "2025-04-04 18:22:35 - INFO - Epoch [7/10], Batch [260/374], Loss: 0.3773\n",
      "2025-04-04 18:22:36 - INFO - Epoch [7/10], Batch [270/374], Loss: 0.4800\n",
      "2025-04-04 18:22:37 - INFO - Epoch [7/10], Batch [280/374], Loss: 0.3439\n",
      "2025-04-04 18:22:37 - INFO - Epoch [7/10], Batch [290/374], Loss: 0.2912\n",
      "2025-04-04 18:22:38 - INFO - Epoch [7/10], Batch [300/374], Loss: 0.5443\n",
      "2025-04-04 18:22:39 - INFO - Epoch [7/10], Batch [310/374], Loss: 0.4599\n",
      "2025-04-04 18:22:39 - INFO - Epoch [7/10], Batch [320/374], Loss: 0.5479\n",
      "2025-04-04 18:22:40 - INFO - Epoch [7/10], Batch [330/374], Loss: 0.4335\n",
      "2025-04-04 18:22:41 - INFO - Epoch [7/10], Batch [340/374], Loss: 0.4160\n",
      "2025-04-04 18:22:41 - INFO - Epoch [7/10], Batch [350/374], Loss: 0.4124\n",
      "2025-04-04 18:22:42 - INFO - Epoch [7/10], Batch [360/374], Loss: 0.3809\n",
      "2025-04-04 18:22:43 - INFO - Epoch [7/10], Batch [370/374], Loss: 0.5702\n",
      "2025-04-04 18:22:43 - INFO - Epoch [7/10] Train Loss: 0.4114, Train Accuracy: 80.36%\n",
      "2025-04-04 18:22:45 - INFO - Epoch [7/10] Val Loss: 0.3385, Val Accuracy: 85.59%\n",
      "2025-04-04 18:22:45 - INFO - New best model at epoch 7 with val accuracy: 85.59%\n",
      "2025-04-04 18:22:45 - INFO - Epoch [8/10], Batch [0/374], Loss: 0.5244\n",
      "2025-04-04 18:22:46 - INFO - Epoch [8/10], Batch [10/374], Loss: 0.2981\n",
      "2025-04-04 18:22:47 - INFO - Epoch [8/10], Batch [20/374], Loss: 0.5737\n",
      "2025-04-04 18:22:47 - INFO - Epoch [8/10], Batch [30/374], Loss: 0.3017\n",
      "2025-04-04 18:22:48 - INFO - Epoch [8/10], Batch [40/374], Loss: 0.4062\n",
      "2025-04-04 18:22:49 - INFO - Epoch [8/10], Batch [50/374], Loss: 0.3960\n",
      "2025-04-04 18:22:49 - INFO - Epoch [8/10], Batch [60/374], Loss: 0.3944\n",
      "2025-04-04 18:22:50 - INFO - Epoch [8/10], Batch [70/374], Loss: 0.4496\n",
      "2025-04-04 18:22:51 - INFO - Epoch [8/10], Batch [80/374], Loss: 0.3485\n",
      "2025-04-04 18:22:51 - INFO - Epoch [8/10], Batch [90/374], Loss: 0.3208\n",
      "2025-04-04 18:22:52 - INFO - Epoch [8/10], Batch [100/374], Loss: 0.5254\n",
      "2025-04-04 18:22:53 - INFO - Epoch [8/10], Batch [110/374], Loss: 0.3553\n",
      "2025-04-04 18:22:53 - INFO - Epoch [8/10], Batch [120/374], Loss: 0.2938\n",
      "2025-04-04 18:22:54 - INFO - Epoch [8/10], Batch [130/374], Loss: 0.2707\n",
      "2025-04-04 18:22:55 - INFO - Epoch [8/10], Batch [140/374], Loss: 0.4436\n",
      "2025-04-04 18:22:56 - INFO - Epoch [8/10], Batch [150/374], Loss: 0.5506\n",
      "2025-04-04 18:22:56 - INFO - Epoch [8/10], Batch [160/374], Loss: 0.6207\n",
      "2025-04-04 18:22:57 - INFO - Epoch [8/10], Batch [170/374], Loss: 0.3931\n",
      "2025-04-04 18:22:58 - INFO - Epoch [8/10], Batch [180/374], Loss: 0.5790\n",
      "2025-04-04 18:22:58 - INFO - Epoch [8/10], Batch [190/374], Loss: 0.5138\n",
      "2025-04-04 18:22:59 - INFO - Epoch [8/10], Batch [200/374], Loss: 0.4105\n",
      "2025-04-04 18:23:00 - INFO - Epoch [8/10], Batch [210/374], Loss: 0.4527\n",
      "2025-04-04 18:23:00 - INFO - Epoch [8/10], Batch [220/374], Loss: 0.2772\n",
      "2025-04-04 18:23:01 - INFO - Epoch [8/10], Batch [230/374], Loss: 0.2769\n",
      "2025-04-04 18:23:02 - INFO - Epoch [8/10], Batch [240/374], Loss: 0.2383\n",
      "2025-04-04 18:23:02 - INFO - Epoch [8/10], Batch [250/374], Loss: 0.2215\n",
      "2025-04-04 18:23:03 - INFO - Epoch [8/10], Batch [260/374], Loss: 0.3393\n",
      "2025-04-04 18:23:04 - INFO - Epoch [8/10], Batch [270/374], Loss: 0.2730\n",
      "2025-04-04 18:23:04 - INFO - Epoch [8/10], Batch [280/374], Loss: 0.4549\n",
      "2025-04-04 18:23:05 - INFO - Epoch [8/10], Batch [290/374], Loss: 0.4318\n",
      "2025-04-04 18:23:06 - INFO - Epoch [8/10], Batch [300/374], Loss: 0.4149\n",
      "2025-04-04 18:23:06 - INFO - Epoch [8/10], Batch [310/374], Loss: 0.2966\n",
      "2025-04-04 18:23:07 - INFO - Epoch [8/10], Batch [320/374], Loss: 0.6974\n",
      "2025-04-04 18:23:08 - INFO - Epoch [8/10], Batch [330/374], Loss: 0.2822\n",
      "2025-04-04 18:23:08 - INFO - Epoch [8/10], Batch [340/374], Loss: 0.3772\n",
      "2025-04-04 18:23:09 - INFO - Epoch [8/10], Batch [350/374], Loss: 0.4630\n",
      "2025-04-04 18:23:10 - INFO - Epoch [8/10], Batch [360/374], Loss: 0.5212\n",
      "2025-04-04 18:23:10 - INFO - Epoch [8/10], Batch [370/374], Loss: 0.4634\n",
      "2025-04-04 18:23:11 - INFO - Epoch [8/10] Train Loss: 0.3889, Train Accuracy: 81.50%\n",
      "2025-04-04 18:23:12 - INFO - Epoch [8/10] Val Loss: 0.3252, Val Accuracy: 85.80%\n",
      "2025-04-04 18:23:12 - INFO - New best model at epoch 8 with val accuracy: 85.80%\n",
      "2025-04-04 18:23:13 - INFO - Epoch [9/10], Batch [0/374], Loss: 0.3852\n",
      "2025-04-04 18:23:14 - INFO - Epoch [9/10], Batch [10/374], Loss: 0.6130\n",
      "2025-04-04 18:23:14 - INFO - Epoch [9/10], Batch [20/374], Loss: 0.4639\n",
      "2025-04-04 18:23:15 - INFO - Epoch [9/10], Batch [30/374], Loss: 0.4816\n",
      "2025-04-04 18:23:16 - INFO - Epoch [9/10], Batch [40/374], Loss: 0.2927\n",
      "2025-04-04 18:23:16 - INFO - Epoch [9/10], Batch [50/374], Loss: 0.6183\n",
      "2025-04-04 18:23:17 - INFO - Epoch [9/10], Batch [60/374], Loss: 0.5405\n",
      "2025-04-04 18:23:18 - INFO - Epoch [9/10], Batch [70/374], Loss: 0.4020\n",
      "2025-04-04 18:23:18 - INFO - Epoch [9/10], Batch [80/374], Loss: 0.3246\n",
      "2025-04-04 18:23:19 - INFO - Epoch [9/10], Batch [90/374], Loss: 0.4092\n",
      "2025-04-04 18:23:20 - INFO - Epoch [9/10], Batch [100/374], Loss: 0.3453\n",
      "2025-04-04 18:23:20 - INFO - Epoch [9/10], Batch [110/374], Loss: 0.2968\n",
      "2025-04-04 18:23:21 - INFO - Epoch [9/10], Batch [120/374], Loss: 0.2700\n",
      "2025-04-04 18:23:22 - INFO - Epoch [9/10], Batch [130/374], Loss: 0.3035\n",
      "2025-04-04 18:23:22 - INFO - Epoch [9/10], Batch [140/374], Loss: 0.3060\n",
      "2025-04-04 18:23:23 - INFO - Epoch [9/10], Batch [150/374], Loss: 0.3038\n",
      "2025-04-04 18:23:24 - INFO - Epoch [9/10], Batch [160/374], Loss: 0.2994\n",
      "2025-04-04 18:23:24 - INFO - Epoch [9/10], Batch [170/374], Loss: 0.2889\n",
      "2025-04-04 18:23:25 - INFO - Epoch [9/10], Batch [180/374], Loss: 0.3627\n",
      "2025-04-04 18:23:26 - INFO - Epoch [9/10], Batch [190/374], Loss: 0.3878\n",
      "2025-04-04 18:23:26 - INFO - Epoch [9/10], Batch [200/374], Loss: 0.3792\n",
      "2025-04-04 18:23:27 - INFO - Epoch [9/10], Batch [210/374], Loss: 0.2847\n",
      "2025-04-04 18:23:28 - INFO - Epoch [9/10], Batch [220/374], Loss: 0.3216\n",
      "2025-04-04 18:23:29 - INFO - Epoch [9/10], Batch [230/374], Loss: 0.2326\n",
      "2025-04-04 18:23:29 - INFO - Epoch [9/10], Batch [240/374], Loss: 0.3735\n",
      "2025-04-04 18:23:30 - INFO - Epoch [9/10], Batch [250/374], Loss: 0.3128\n",
      "2025-04-04 18:23:31 - INFO - Epoch [9/10], Batch [260/374], Loss: 0.2635\n",
      "2025-04-04 18:23:31 - INFO - Epoch [9/10], Batch [270/374], Loss: 0.2610\n",
      "2025-04-04 18:23:32 - INFO - Epoch [9/10], Batch [280/374], Loss: 0.3055\n",
      "2025-04-04 18:23:33 - INFO - Epoch [9/10], Batch [290/374], Loss: 0.4037\n",
      "2025-04-04 18:23:33 - INFO - Epoch [9/10], Batch [300/374], Loss: 0.3617\n",
      "2025-04-04 18:23:34 - INFO - Epoch [9/10], Batch [310/374], Loss: 0.3514\n",
      "2025-04-04 18:23:35 - INFO - Epoch [9/10], Batch [320/374], Loss: 0.3810\n",
      "2025-04-04 18:23:35 - INFO - Epoch [9/10], Batch [330/374], Loss: 0.3760\n",
      "2025-04-04 18:23:36 - INFO - Epoch [9/10], Batch [340/374], Loss: 0.2439\n",
      "2025-04-04 18:23:37 - INFO - Epoch [9/10], Batch [350/374], Loss: 0.5980\n",
      "2025-04-04 18:23:37 - INFO - Epoch [9/10], Batch [360/374], Loss: 0.3156\n",
      "2025-04-04 18:23:38 - INFO - Epoch [9/10], Batch [370/374], Loss: 0.2383\n",
      "2025-04-04 18:23:38 - INFO - Epoch [9/10] Train Loss: 0.3697, Train Accuracy: 82.84%\n",
      "2025-04-04 18:23:40 - INFO - Epoch [9/10] Val Loss: 0.2968, Val Accuracy: 86.92%\n",
      "2025-04-04 18:23:40 - INFO - New best model at epoch 9 with val accuracy: 86.92%\n",
      "2025-04-04 18:23:40 - INFO - Epoch [10/10], Batch [0/374], Loss: 0.4066\n",
      "2025-04-04 18:23:41 - INFO - Epoch [10/10], Batch [10/374], Loss: 0.3069\n",
      "2025-04-04 18:23:42 - INFO - Epoch [10/10], Batch [20/374], Loss: 0.4671\n",
      "2025-04-04 18:23:43 - INFO - Epoch [10/10], Batch [30/374], Loss: 0.4303\n",
      "2025-04-04 18:23:43 - INFO - Epoch [10/10], Batch [40/374], Loss: 0.2095\n",
      "2025-04-04 18:23:44 - INFO - Epoch [10/10], Batch [50/374], Loss: 0.3307\n",
      "2025-04-04 18:23:45 - INFO - Epoch [10/10], Batch [60/374], Loss: 0.3162\n",
      "2025-04-04 18:23:45 - INFO - Epoch [10/10], Batch [70/374], Loss: 0.2876\n",
      "2025-04-04 18:23:46 - INFO - Epoch [10/10], Batch [80/374], Loss: 0.3206\n",
      "2025-04-04 18:23:47 - INFO - Epoch [10/10], Batch [90/374], Loss: 0.3027\n",
      "2025-04-04 18:23:47 - INFO - Epoch [10/10], Batch [100/374], Loss: 0.4345\n",
      "2025-04-04 18:23:48 - INFO - Epoch [10/10], Batch [110/374], Loss: 0.2570\n",
      "2025-04-04 18:23:49 - INFO - Epoch [10/10], Batch [120/374], Loss: 0.3220\n",
      "2025-04-04 18:23:49 - INFO - Epoch [10/10], Batch [130/374], Loss: 0.3248\n",
      "2025-04-04 18:23:50 - INFO - Epoch [10/10], Batch [140/374], Loss: 0.3216\n",
      "2025-04-04 18:23:51 - INFO - Epoch [10/10], Batch [150/374], Loss: 0.2324\n",
      "2025-04-04 18:23:51 - INFO - Epoch [10/10], Batch [160/374], Loss: 0.2776\n",
      "2025-04-04 18:23:52 - INFO - Epoch [10/10], Batch [170/374], Loss: 0.2764\n",
      "2025-04-04 18:23:53 - INFO - Epoch [10/10], Batch [180/374], Loss: 0.5642\n",
      "2025-04-04 18:23:53 - INFO - Epoch [10/10], Batch [190/374], Loss: 0.3539\n",
      "2025-04-04 18:23:54 - INFO - Epoch [10/10], Batch [200/374], Loss: 0.3176\n",
      "2025-04-04 18:23:55 - INFO - Epoch [10/10], Batch [210/374], Loss: 0.3753\n",
      "2025-04-04 18:23:55 - INFO - Epoch [10/10], Batch [220/374], Loss: 0.3586\n",
      "2025-04-04 18:23:56 - INFO - Epoch [10/10], Batch [230/374], Loss: 0.2382\n",
      "2025-04-04 18:23:57 - INFO - Epoch [10/10], Batch [240/374], Loss: 0.3614\n",
      "2025-04-04 18:23:58 - INFO - Epoch [10/10], Batch [250/374], Loss: 0.5211\n",
      "2025-04-04 18:23:58 - INFO - Epoch [10/10], Batch [260/374], Loss: 0.3341\n",
      "2025-04-04 18:23:59 - INFO - Epoch [10/10], Batch [270/374], Loss: 0.4572\n",
      "2025-04-04 18:24:00 - INFO - Epoch [10/10], Batch [280/374], Loss: 0.3703\n",
      "2025-04-04 18:24:00 - INFO - Epoch [10/10], Batch [290/374], Loss: 0.4214\n",
      "2025-04-04 18:24:01 - INFO - Epoch [10/10], Batch [300/374], Loss: 0.4053\n",
      "2025-04-04 18:24:02 - INFO - Epoch [10/10], Batch [310/374], Loss: 0.3555\n",
      "2025-04-04 18:24:02 - INFO - Epoch [10/10], Batch [320/374], Loss: 0.2820\n",
      "2025-04-04 18:24:03 - INFO - Epoch [10/10], Batch [330/374], Loss: 0.5975\n",
      "2025-04-04 18:24:04 - INFO - Epoch [10/10], Batch [340/374], Loss: 0.3750\n",
      "2025-04-04 18:24:04 - INFO - Epoch [10/10], Batch [350/374], Loss: 0.2607\n",
      "2025-04-04 18:24:05 - INFO - Epoch [10/10], Batch [360/374], Loss: 0.5084\n",
      "2025-04-04 18:24:06 - INFO - Epoch [10/10], Batch [370/374], Loss: 0.5572\n",
      "2025-04-04 18:24:06 - INFO - Epoch [10/10] Train Loss: 0.3655, Train Accuracy: 83.30%\n",
      "2025-04-04 18:24:08 - INFO - Epoch [10/10] Val Loss: 0.2940, Val Accuracy: 87.09%\n",
      "2025-04-04 18:24:08 - INFO - New best model at epoch 10 with val accuracy: 87.09%\n",
      "2025-04-04 18:24:09 - INFO - Test Loss: 0.3121, Test Accuracy: 86.99%\n",
      "2025-04-04 18:24:09 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-04 18:24:09 - INFO - {\n",
      "    \"world_size\": 2,\n",
      "    \"train_time\": 279.4308559894562,\n",
      "    \"avg_epoch_time\": 27.94308559894562,\n",
      "    \"val_accuracy\": 87.09116214335421,\n",
      "    \"test_accuracy\": 86.98677800974252,\n",
      "    \"test_loss\": 0.3121020334332704,\n",
      "    \"total_time\": 279.4308559894562,\n",
      "    \"train_losses\": [\n",
      "        0.6456588382491089,\n",
      "        0.6137480153824177,\n",
      "        0.5788744733402226,\n",
      "        0.5270019430567086,\n",
      "        0.4696963345274206,\n",
      "        0.43707803508279436,\n",
      "        0.4114341286448058,\n",
      "        0.38886874149719414,\n",
      "        0.3696538248075303,\n",
      "        0.36549226143428176\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        60.348146288392336,\n",
      "        65.08494434680726,\n",
      "        67.8717884341786,\n",
      "        73.4622144112478,\n",
      "        77.2198510335593,\n",
      "        79.23675621390912,\n",
      "        80.35818896978827,\n",
      "        81.49635952799397,\n",
      "        82.83538371411834,\n",
      "        83.29567327809859\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.5885921503075644,\n",
      "        0.5592073912245579,\n",
      "        0.5200968342849424,\n",
      "        0.449126938273696,\n",
      "        0.41994956930063626,\n",
      "        0.36486120896365964,\n",
      "        0.33851759472736154,\n",
      "        0.32515311365585486,\n",
      "        0.29677169963396,\n",
      "        0.29398198489024563\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        72.23382045929019,\n",
      "        74.11273486430062,\n",
      "        76.20041753653445,\n",
      "        79.74947807933194,\n",
      "        80.75852470424495,\n",
      "        84.02922755741128,\n",
      "        85.59498956158664,\n",
      "        85.80375782881002,\n",
      "        86.91718858733472,\n",
      "        87.09116214335421\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-04 18:24:10 - INFO - Model saved to models/training_using_gpus_2_best_model.pt\n",
      "2025-04-04 18:24:12 - INFO - Loss plot saved as plots/training_using_gpus_2_loss.png\n",
      "2025-04-04 18:24:12 - INFO - Accuracy plot saved as plots/training_using_gpus_2_accuracy.png\n",
      "2025-04-04 18:24:12 - INFO - Runtime parameters saved as metrics/training_using_gpus_2_params.json\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6101eb-4a7d-4e09-b180-4191a57a3724",
   "metadata": {},
   "source": [
    "### Calling the main function through the torchrun command to train the model in 4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308298b0-3bb2-41ad-b373-97f74cc1d0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0404 18:24:21.764000 1091005 site-packages/torch/distributed/run.py:793] \n",
      "W0404 18:24:21.764000 1091005 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0404 18:24:21.764000 1091005 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0404 18:24:21.764000 1091005 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "2025-04-04 18:24:31 - INFO - Running on node: d1004\n",
      "2025-04-04 18:24:31 - INFO - Running with world_size: 4 (Rank: 1)\n",
      "2025-04-04 18:24:31 - INFO - Running on node: d1004\n",
      "2025-04-04 18:24:31 - INFO - Running with world_size: 4 (Rank: 2)\n",
      "2025-04-04 18:24:31 - INFO - Running on node: d1004\n",
      "2025-04-04 18:24:31 - INFO - Running with world_size: 4 (Rank: 0)\n",
      "2025-04-04 18:24:31 - INFO - Running on node: d1004\n",
      "2025-04-04 18:24:31 - INFO - Running with world_size: 4 (Rank: 3)\n",
      "2025-04-04 18:24:31 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 18:24:31 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 18:24:31 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 18:24:31 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 18:28:01 - INFO - [Rank 3] Data loaded.\n",
      "2025-04-04 18:28:01 - INFO - [Rank 2] Data loaded.\n",
      "2025-04-04 18:28:01 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-04 18:28:01 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-04 18:28:04 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-04 18:28:04 - INFO - Total layers: 129\n",
      "2025-04-04 18:28:04 - INFO - Total parameters: 22,494,274\n",
      "2025-04-04 18:28:10 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6685\n",
      "2025-04-04 18:28:11 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.6515\n",
      "2025-04-04 18:28:12 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.6748\n",
      "2025-04-04 18:28:13 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.6250\n",
      "2025-04-04 18:28:14 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.6253\n",
      "2025-04-04 18:28:15 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6555\n",
      "2025-04-04 18:28:16 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.6267\n",
      "2025-04-04 18:28:17 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.6563\n",
      "2025-04-04 18:28:18 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.6208\n",
      "2025-04-04 18:28:19 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.7907\n",
      "2025-04-04 18:28:20 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.6546\n",
      "2025-04-04 18:28:21 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.5849\n",
      "2025-04-04 18:28:22 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6316\n",
      "2025-04-04 18:28:23 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.6455\n",
      "2025-04-04 18:28:24 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5686\n",
      "2025-04-04 18:28:25 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.6041\n",
      "2025-04-04 18:28:26 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.5684\n",
      "2025-04-04 18:28:27 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5245\n",
      "2025-04-04 18:28:28 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.6097\n",
      "2025-04-04 18:28:29 - INFO - Epoch [1/10] Train Loss: 0.6402, Train Accuracy: 61.26%\n",
      "2025-04-04 18:28:33 - INFO - Epoch [1/10] Val Loss: 0.5916, Val Accuracy: 70.08%\n",
      "2025-04-04 18:28:33 - INFO - New best model at epoch 1 with val accuracy: 70.08%\n",
      "2025-04-04 18:28:34 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.5746\n",
      "2025-04-04 18:28:35 - INFO - Epoch [2/10], Batch [10/187], Loss: 0.5902\n",
      "2025-04-04 18:28:36 - INFO - Epoch [2/10], Batch [20/187], Loss: 0.6183\n",
      "2025-04-04 18:28:37 - INFO - Epoch [2/10], Batch [30/187], Loss: 0.9128\n",
      "2025-04-04 18:28:38 - INFO - Epoch [2/10], Batch [40/187], Loss: 0.5037\n",
      "2025-04-04 18:28:39 - INFO - Epoch [2/10], Batch [50/187], Loss: 0.5799\n",
      "2025-04-04 18:28:40 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.5844\n",
      "2025-04-04 18:28:41 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.6139\n",
      "2025-04-04 18:28:42 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.6872\n",
      "2025-04-04 18:28:43 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.6060\n",
      "2025-04-04 18:28:44 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.8325\n",
      "2025-04-04 18:28:45 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.6705\n",
      "2025-04-04 18:28:46 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.7154\n",
      "2025-04-04 18:28:47 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.7254\n",
      "2025-04-04 18:28:48 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.6279\n",
      "2025-04-04 18:28:49 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.5820\n",
      "2025-04-04 18:28:50 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.6136\n",
      "2025-04-04 18:28:51 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.5410\n",
      "2025-04-04 18:28:52 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.6360\n",
      "2025-04-04 18:28:52 - INFO - Epoch [2/10] Train Loss: 0.6093, Train Accuracy: 65.22%\n",
      "2025-04-04 18:28:54 - INFO - Epoch [2/10] Val Loss: 0.5804, Val Accuracy: 75.43%\n",
      "2025-04-04 18:28:54 - INFO - New best model at epoch 2 with val accuracy: 75.43%\n",
      "2025-04-04 18:28:55 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.7268\n",
      "2025-04-04 18:28:56 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.5199\n",
      "2025-04-04 18:28:57 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.4787\n",
      "2025-04-04 18:28:58 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.6871\n",
      "2025-04-04 18:28:59 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.6576\n",
      "2025-04-04 18:29:00 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.7067\n",
      "2025-04-04 18:29:01 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.6315\n",
      "2025-04-04 18:29:02 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.5128\n",
      "2025-04-04 18:29:03 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.5840\n",
      "2025-04-04 18:29:04 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.5299\n",
      "2025-04-04 18:29:05 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.5333\n",
      "2025-04-04 18:29:06 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.7369\n",
      "2025-04-04 18:29:07 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.5672\n",
      "2025-04-04 18:29:08 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5416\n",
      "2025-04-04 18:29:09 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.4892\n",
      "2025-04-04 18:29:10 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.5386\n",
      "2025-04-04 18:29:11 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.4511\n",
      "2025-04-04 18:29:11 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.5188\n",
      "2025-04-04 18:29:12 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.5527\n",
      "2025-04-04 18:29:12 - INFO - Epoch [3/10] Train Loss: 0.5762, Train Accuracy: 67.88%\n",
      "2025-04-04 18:29:14 - INFO - Epoch [3/10] Val Loss: 0.5384, Val Accuracy: 74.11%\n",
      "2025-04-04 18:29:15 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.6406\n",
      "2025-04-04 18:29:16 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.4401\n",
      "2025-04-04 18:29:17 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.5387\n",
      "2025-04-04 18:29:18 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.5397\n",
      "2025-04-04 18:29:19 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.6372\n",
      "2025-04-04 18:29:20 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.5886\n",
      "2025-04-04 18:29:21 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.5032\n",
      "2025-04-04 18:29:22 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.4530\n",
      "2025-04-04 18:29:23 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.5440\n",
      "2025-04-04 18:29:24 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.6783\n",
      "2025-04-04 18:29:25 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4899\n",
      "2025-04-04 18:29:26 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.5222\n",
      "2025-04-04 18:29:27 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.5401\n",
      "2025-04-04 18:29:28 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.5053\n",
      "2025-04-04 18:29:29 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.4982\n",
      "2025-04-04 18:29:30 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.5072\n",
      "2025-04-04 18:29:31 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.5893\n",
      "2025-04-04 18:29:32 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.5412\n",
      "2025-04-04 18:29:32 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.4472\n",
      "2025-04-04 18:29:33 - INFO - Epoch [4/10] Train Loss: 0.5482, Train Accuracy: 70.56%\n",
      "2025-04-04 18:29:35 - INFO - Epoch [4/10] Val Loss: 0.5007, Val Accuracy: 78.91%\n",
      "2025-04-04 18:29:35 - INFO - New best model at epoch 4 with val accuracy: 78.91%\n",
      "2025-04-04 18:29:35 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.5337\n",
      "2025-04-04 18:29:37 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.5778\n",
      "2025-04-04 18:29:38 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.4585\n",
      "2025-04-04 18:29:39 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.5176\n",
      "2025-04-04 18:29:40 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.5910\n",
      "2025-04-04 18:29:41 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.4328\n",
      "2025-04-04 18:29:42 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.5034\n",
      "2025-04-04 18:29:42 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.4312\n",
      "2025-04-04 18:29:43 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.4294\n",
      "2025-04-04 18:29:44 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.5739\n",
      "2025-04-04 18:29:45 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.5885\n",
      "2025-04-04 18:29:46 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3128\n",
      "2025-04-04 18:29:47 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.4632\n",
      "2025-04-04 18:29:48 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.2890\n",
      "2025-04-04 18:29:49 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.4268\n",
      "2025-04-04 18:29:50 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.5168\n",
      "2025-04-04 18:29:51 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.3939\n",
      "2025-04-04 18:29:52 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.3715\n",
      "2025-04-04 18:29:53 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.5239\n",
      "2025-04-04 18:29:53 - INFO - Epoch [5/10] Train Loss: 0.4759, Train Accuracy: 77.24%\n",
      "2025-04-04 18:29:55 - INFO - Epoch [5/10] Val Loss: 0.4035, Val Accuracy: 82.81%\n",
      "2025-04-04 18:29:55 - INFO - New best model at epoch 5 with val accuracy: 82.81%\n",
      "2025-04-04 18:29:56 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.4936\n",
      "2025-04-04 18:29:57 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.5744\n",
      "2025-04-04 18:29:58 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.5521\n",
      "2025-04-04 18:29:59 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.4465\n",
      "2025-04-04 18:30:00 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2790\n",
      "2025-04-04 18:30:01 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.4739\n",
      "2025-04-04 18:30:02 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.3442\n",
      "2025-04-04 18:30:03 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.5410\n",
      "2025-04-04 18:30:04 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.3849\n",
      "2025-04-04 18:30:05 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.3880\n",
      "2025-04-04 18:30:06 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.5198\n",
      "2025-04-04 18:30:07 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2567\n",
      "2025-04-04 18:30:08 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.8108\n",
      "2025-04-04 18:30:09 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.3114\n",
      "2025-04-04 18:30:10 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.3182\n",
      "2025-04-04 18:30:10 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.3907\n",
      "2025-04-04 18:30:11 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.4091\n",
      "2025-04-04 18:30:12 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.5588\n",
      "2025-04-04 18:30:13 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.3730\n",
      "2025-04-04 18:30:14 - INFO - Epoch [6/10] Train Loss: 0.4253, Train Accuracy: 79.78%\n",
      "2025-04-04 18:30:15 - INFO - Epoch [6/10] Val Loss: 0.3627, Val Accuracy: 84.97%\n",
      "2025-04-04 18:30:15 - INFO - New best model at epoch 6 with val accuracy: 84.97%\n",
      "2025-04-04 18:30:16 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.3245\n",
      "2025-04-04 18:30:17 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.5257\n",
      "2025-04-04 18:30:18 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.6140\n",
      "2025-04-04 18:30:19 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.3510\n",
      "2025-04-04 18:30:20 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.3500\n",
      "2025-04-04 18:30:21 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.3629\n",
      "2025-04-04 18:30:22 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.4925\n",
      "2025-04-04 18:30:23 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.4614\n",
      "2025-04-04 18:30:24 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.3233\n",
      "2025-04-04 18:30:25 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.3216\n",
      "2025-04-04 18:30:26 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.5093\n",
      "2025-04-04 18:30:27 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.3375\n",
      "2025-04-04 18:30:28 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.5763\n",
      "2025-04-04 18:30:29 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.2586\n",
      "2025-04-04 18:30:30 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.6378\n",
      "2025-04-04 18:30:31 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.4656\n",
      "2025-04-04 18:30:32 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.3479\n",
      "2025-04-04 18:30:33 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.6402\n",
      "2025-04-04 18:30:34 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2862\n",
      "2025-04-04 18:30:34 - INFO - Epoch [7/10] Train Loss: 0.4033, Train Accuracy: 80.80%\n",
      "2025-04-04 18:30:36 - INFO - Epoch [7/10] Val Loss: 0.3281, Val Accuracy: 86.43%\n",
      "2025-04-04 18:30:36 - INFO - New best model at epoch 7 with val accuracy: 86.43%\n",
      "2025-04-04 18:30:37 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.5705\n",
      "2025-04-04 18:30:38 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.3827\n",
      "2025-04-04 18:30:39 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.4311\n",
      "2025-04-04 18:30:40 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.3221\n",
      "2025-04-04 18:30:41 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.2603\n",
      "2025-04-04 18:30:42 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2404\n",
      "2025-04-04 18:30:43 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.2147\n",
      "2025-04-04 18:30:44 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.4174\n",
      "2025-04-04 18:30:45 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.5604\n",
      "2025-04-04 18:30:46 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.4298\n",
      "2025-04-04 18:30:46 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.4129\n",
      "2025-04-04 18:30:47 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.2824\n",
      "2025-04-04 18:30:48 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.2506\n",
      "2025-04-04 18:30:49 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.3110\n",
      "2025-04-04 18:30:50 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.4443\n",
      "2025-04-04 18:30:51 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.4442\n",
      "2025-04-04 18:30:52 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.3248\n",
      "2025-04-04 18:30:53 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3441\n",
      "2025-04-04 18:30:54 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.3452\n",
      "2025-04-04 18:30:54 - INFO - Epoch [8/10] Train Loss: 0.3704, Train Accuracy: 82.59%\n",
      "2025-04-04 18:30:56 - INFO - Epoch [8/10] Val Loss: 0.3055, Val Accuracy: 86.92%\n",
      "2025-04-04 18:30:56 - INFO - New best model at epoch 8 with val accuracy: 86.92%\n",
      "2025-04-04 18:30:57 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.2833\n",
      "2025-04-04 18:30:58 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.2914\n",
      "2025-04-04 18:30:59 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.2122\n",
      "2025-04-04 18:31:00 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.3848\n",
      "2025-04-04 18:31:01 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.4331\n",
      "2025-04-04 18:31:02 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2756\n",
      "2025-04-04 18:31:03 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.1973\n",
      "2025-04-04 18:31:04 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2243\n",
      "2025-04-04 18:31:05 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.3506\n",
      "2025-04-04 18:31:06 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.3008\n",
      "2025-04-04 18:31:07 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4119\n",
      "2025-04-04 18:31:08 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.2073\n",
      "2025-04-04 18:31:09 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.2509\n",
      "2025-04-04 18:31:10 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2427\n",
      "2025-04-04 18:31:11 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.4031\n",
      "2025-04-04 18:31:12 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.2664\n",
      "2025-04-04 18:31:13 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.4755\n",
      "2025-04-04 18:31:14 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.3199\n",
      "2025-04-04 18:31:14 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.2902\n",
      "2025-04-04 18:31:15 - INFO - Epoch [9/10] Train Loss: 0.3605, Train Accuracy: 82.91%\n",
      "2025-04-04 18:31:16 - INFO - Epoch [9/10] Val Loss: 0.2888, Val Accuracy: 88.10%\n",
      "2025-04-04 18:31:16 - INFO - New best model at epoch 9 with val accuracy: 88.10%\n",
      "2025-04-04 18:31:17 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.4659\n",
      "2025-04-04 18:31:18 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.5182\n",
      "2025-04-04 18:31:19 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.2972\n",
      "2025-04-04 18:31:20 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.2652\n",
      "2025-04-04 18:31:21 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.5091\n",
      "2025-04-04 18:31:22 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2964\n",
      "2025-04-04 18:31:23 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2780\n",
      "2025-04-04 18:31:24 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.4474\n",
      "2025-04-04 18:31:25 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.3588\n",
      "2025-04-04 18:31:26 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.4196\n",
      "2025-04-04 18:31:27 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.2101\n",
      "2025-04-04 18:31:28 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.2898\n",
      "2025-04-04 18:31:29 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.2445\n",
      "2025-04-04 18:31:30 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.2844\n",
      "2025-04-04 18:31:31 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.5018\n",
      "2025-04-04 18:31:32 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.2959\n",
      "2025-04-04 18:31:33 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.2649\n",
      "2025-04-04 18:31:34 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.3984\n",
      "2025-04-04 18:31:35 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.3520\n",
      "2025-04-04 18:31:35 - INFO - Epoch [10/10] Train Loss: 0.3423, Train Accuracy: 83.73%\n",
      "2025-04-04 18:31:37 - INFO - Epoch [10/10] Val Loss: 0.2781, Val Accuracy: 88.03%\n",
      "2025-04-04 18:31:41 - INFO - Test Loss: 0.3068, Test Accuracy: 86.79%\n",
      "2025-04-04 18:31:41 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-04 18:31:41 - INFO - {\n",
      "    \"world_size\": 4,\n",
      "    \"train_time\": 212.2620713710785,\n",
      "    \"avg_epoch_time\": 21.22620713710785,\n",
      "    \"val_accuracy\": 88.10020876826722,\n",
      "    \"test_accuracy\": 86.78720445062586,\n",
      "    \"test_loss\": 0.306791549100332,\n",
      "    \"total_time\": 212.2620713710785,\n",
      "    \"train_losses\": [\n",
      "        0.640213508027368,\n",
      "        0.6093268294214703,\n",
      "        0.5762142176009621,\n",
      "        0.5481988203874691,\n",
      "        0.4758872297219153,\n",
      "        0.4252999133046202,\n",
      "        0.40328851476374034,\n",
      "        0.3703911739488027,\n",
      "        0.3604969360938132,\n",
      "        0.34226615855384573\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        61.25523012552301,\n",
      "        65.22175732217573,\n",
      "        67.88284518828452,\n",
      "        70.56066945606695,\n",
      "        77.23849372384937,\n",
      "        79.78242677824268,\n",
      "        80.80334728033473,\n",
      "        82.59414225941423,\n",
      "        82.91213389121339,\n",
      "        83.73221757322176\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.5916417552600242,\n",
      "        0.5803933919264197,\n",
      "        0.5383723988330604,\n",
      "        0.5006647317377991,\n",
      "        0.4035295301124132,\n",
      "        0.36273427124063257,\n",
      "        0.3281303976008522,\n",
      "        0.30551797670047154,\n",
      "        0.28880073008806073,\n",
      "        0.2780912941997053\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        70.07654836464857,\n",
      "        75.43493389004871,\n",
      "        74.11273486430062,\n",
      "        78.91440501043841,\n",
      "        82.81141266527487,\n",
      "        84.9686847599165,\n",
      "        86.43006263048017,\n",
      "        86.91718858733472,\n",
      "        88.10020876826722,\n",
      "        88.03061934585944\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-04 18:31:41 - INFO - Model saved to models/training_using_gpus_4_best_model.pt\n",
      "2025-04-04 18:31:45 - INFO - Loss plot saved as plots/training_using_gpus_4_loss.png\n",
      "2025-04-04 18:31:45 - INFO - Accuracy plot saved as plots/training_using_gpus_4_accuracy.png\n",
      "2025-04-04 18:31:45 - INFO - Runtime parameters saved as metrics/training_using_gpus_4_params.json\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=4 main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015f1ac-3e29-4071-bcf6-40738a422be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=6 main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
