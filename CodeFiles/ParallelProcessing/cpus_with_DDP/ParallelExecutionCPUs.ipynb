{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244ac713-661c-439b-830a-f9d1a990aca6",
   "metadata": {},
   "source": [
    "# CSYE 7105 - High Perfoamnce Machine Learning & AI - Project - Team 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a65463e-e4d0-40c8-8501-143398191253",
   "metadata": {},
   "source": [
    "## Analysis of Efficient Parallel Computing for Deep Learning-Based Glaucoma Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a15ff4-f868-4cb3-82e2-dfb6d041a616",
   "metadata": {},
   "source": [
    "### Parallel Execution using CPU with DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a67e6-bfb1-4b0c-9125-2be8085bedee",
   "metadata": {},
   "source": [
    "#### Workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f65a2a-8455-41a5-9b57-237a04938155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 23:28:40,810 - INFO - [Rank 1] Passed initial barrier\n",
      "2025-04-04 23:28:40,810 - INFO - [Rank 0] Passed initial barrier\n",
      "2025-04-04 23:28:40,824 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 23:28:40,824 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-04 23:28:57,236 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-04 23:28:57,236 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-04 23:28:57,677 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-04 23:28:57,677 - INFO - Total layers: 129\n",
      "2025-04-04 23:28:57,677 - INFO - Total parameters: 22,494,274\n",
      "2025-04-04 23:29:05,733 - INFO - Epoch [1/10], Batch [0/374], Loss: 0.6760\n",
      "2025-04-04 23:30:09,285 - INFO - Epoch [1/10], Batch [10/374], Loss: 0.6599\n",
      "2025-04-04 23:31:08,744 - INFO - Epoch [1/10], Batch [20/374], Loss: 0.7180\n",
      "2025-04-04 23:32:07,959 - INFO - Epoch [1/10], Batch [30/374], Loss: 0.6635\n",
      "2025-04-04 23:33:08,674 - INFO - Epoch [1/10], Batch [40/374], Loss: 0.6516\n",
      "2025-04-04 23:34:09,090 - INFO - Epoch [1/10], Batch [50/374], Loss: 0.6806\n",
      "2025-04-04 23:35:10,126 - INFO - Epoch [1/10], Batch [60/374], Loss: 0.6450\n",
      "2025-04-04 23:36:10,490 - INFO - Epoch [1/10], Batch [70/374], Loss: 0.6863\n",
      "2025-04-04 23:37:11,061 - INFO - Epoch [1/10], Batch [80/374], Loss: 0.6290\n",
      "2025-04-04 23:38:10,772 - INFO - Epoch [1/10], Batch [90/374], Loss: 0.7215\n",
      "2025-04-04 23:39:10,864 - INFO - Epoch [1/10], Batch [100/374], Loss: 0.6507\n",
      "2025-04-04 23:40:10,777 - INFO - Epoch [1/10], Batch [110/374], Loss: 0.6000\n",
      "2025-04-04 23:41:10,265 - INFO - Epoch [1/10], Batch [120/374], Loss: 0.6026\n",
      "2025-04-04 23:42:09,582 - INFO - Epoch [1/10], Batch [130/374], Loss: 0.6735\n",
      "2025-04-04 23:43:09,540 - INFO - Epoch [1/10], Batch [140/374], Loss: 0.6667\n",
      "2025-04-04 23:44:09,389 - INFO - Epoch [1/10], Batch [150/374], Loss: 0.7315\n",
      "2025-04-04 23:45:09,142 - INFO - Epoch [1/10], Batch [160/374], Loss: 0.6107\n",
      "2025-04-04 23:46:09,096 - INFO - Epoch [1/10], Batch [170/374], Loss: 0.6883\n",
      "2025-04-04 23:47:08,256 - INFO - Epoch [1/10], Batch [180/374], Loss: 0.6086\n",
      "2025-04-04 23:48:07,514 - INFO - Epoch [1/10], Batch [190/374], Loss: 0.6761\n",
      "2025-04-04 23:49:08,590 - INFO - Epoch [1/10], Batch [200/374], Loss: 0.6779\n",
      "2025-04-04 23:50:09,779 - INFO - Epoch [1/10], Batch [210/374], Loss: 0.7650\n",
      "2025-04-04 23:51:09,638 - INFO - Epoch [1/10], Batch [220/374], Loss: 0.6516\n",
      "2025-04-04 23:52:08,899 - INFO - Epoch [1/10], Batch [230/374], Loss: 0.6346\n",
      "2025-04-04 23:53:07,751 - INFO - Epoch [1/10], Batch [240/374], Loss: 0.7204\n",
      "2025-04-04 23:54:07,565 - INFO - Epoch [1/10], Batch [250/374], Loss: 0.5511\n",
      "2025-04-04 23:55:06,743 - INFO - Epoch [1/10], Batch [260/374], Loss: 0.5713\n",
      "2025-04-04 23:56:06,493 - INFO - Epoch [1/10], Batch [270/374], Loss: 0.6098\n",
      "2025-04-04 23:57:06,377 - INFO - Epoch [1/10], Batch [280/374], Loss: 0.5854\n",
      "2025-04-04 23:58:06,548 - INFO - Epoch [1/10], Batch [290/374], Loss: 0.6265\n",
      "2025-04-04 23:59:06,915 - INFO - Epoch [1/10], Batch [300/374], Loss: 0.5970\n",
      "2025-04-05 00:00:06,373 - INFO - Epoch [1/10], Batch [310/374], Loss: 0.5327\n",
      "2025-04-05 00:01:06,569 - INFO - Epoch [1/10], Batch [320/374], Loss: 0.6891\n",
      "2025-04-05 00:02:06,094 - INFO - Epoch [1/10], Batch [330/374], Loss: 0.6378\n",
      "2025-04-05 00:03:05,671 - INFO - Epoch [1/10], Batch [340/374], Loss: 0.5755\n",
      "2025-04-05 00:04:05,701 - INFO - Epoch [1/10], Batch [350/374], Loss: 0.6582\n",
      "2025-04-05 00:05:05,203 - INFO - Epoch [1/10], Batch [360/374], Loss: 0.6477\n",
      "2025-04-05 00:06:05,525 - INFO - Epoch [1/10], Batch [370/374], Loss: 0.5742\n",
      "2025-04-05 00:06:20,137 - INFO - Epoch [1/10] Train Loss: 0.6494, Train Accuracy: 60.23%\n",
      "2025-04-05 00:09:03,787 - INFO - Epoch [1/10] Val Loss: 0.6207, Val Accuracy: 69.90%\n",
      "2025-04-05 00:09:03,791 - INFO - New best model at epoch 1 with val accuracy: 69.90%\n",
      "2025-04-05 00:09:10,332 - INFO - Epoch [2/10], Batch [0/374], Loss: 0.6476\n",
      "2025-04-05 00:10:10,588 - INFO - Epoch [2/10], Batch [10/374], Loss: 0.6168\n",
      "2025-04-05 00:11:09,533 - INFO - Epoch [2/10], Batch [20/374], Loss: 0.5836\n",
      "2025-04-05 00:12:09,180 - INFO - Epoch [2/10], Batch [30/374], Loss: 0.6016\n",
      "2025-04-05 00:13:09,369 - INFO - Epoch [2/10], Batch [40/374], Loss: 0.7590\n",
      "2025-04-05 00:14:09,960 - INFO - Epoch [2/10], Batch [50/374], Loss: 0.5421\n",
      "2025-04-05 00:15:10,269 - INFO - Epoch [2/10], Batch [60/374], Loss: 0.7318\n",
      "2025-04-05 00:16:10,328 - INFO - Epoch [2/10], Batch [70/374], Loss: 0.6597\n",
      "2025-04-05 00:17:08,934 - INFO - Epoch [2/10], Batch [80/374], Loss: 0.5618\n",
      "2025-04-05 00:18:07,843 - INFO - Epoch [2/10], Batch [90/374], Loss: 0.7018\n",
      "2025-04-05 00:19:07,295 - INFO - Epoch [2/10], Batch [100/374], Loss: 0.5608\n",
      "2025-04-05 00:20:06,263 - INFO - Epoch [2/10], Batch [110/374], Loss: 0.6045\n",
      "2025-04-05 00:21:05,602 - INFO - Epoch [2/10], Batch [120/374], Loss: 0.5458\n",
      "2025-04-05 00:22:04,497 - INFO - Epoch [2/10], Batch [130/374], Loss: 0.5742\n",
      "2025-04-05 00:23:03,935 - INFO - Epoch [2/10], Batch [140/374], Loss: 0.6314\n",
      "2025-04-05 00:24:03,347 - INFO - Epoch [2/10], Batch [150/374], Loss: 0.5536\n",
      "2025-04-05 00:25:02,375 - INFO - Epoch [2/10], Batch [160/374], Loss: 0.5731\n",
      "2025-04-05 00:26:01,511 - INFO - Epoch [2/10], Batch [170/374], Loss: 0.6980\n",
      "2025-04-05 00:27:01,053 - INFO - Epoch [2/10], Batch [180/374], Loss: 0.5081\n",
      "2025-04-05 00:28:00,696 - INFO - Epoch [2/10], Batch [190/374], Loss: 0.4722\n",
      "2025-04-05 00:29:00,626 - INFO - Epoch [2/10], Batch [200/374], Loss: 0.7237\n",
      "2025-04-05 00:30:00,817 - INFO - Epoch [2/10], Batch [210/374], Loss: 0.6307\n",
      "2025-04-05 00:31:01,054 - INFO - Epoch [2/10], Batch [220/374], Loss: 0.5803\n",
      "2025-04-05 00:32:01,901 - INFO - Epoch [2/10], Batch [230/374], Loss: 0.6314\n",
      "2025-04-05 00:33:00,956 - INFO - Epoch [2/10], Batch [240/374], Loss: 0.6833\n",
      "2025-04-05 00:34:00,420 - INFO - Epoch [2/10], Batch [250/374], Loss: 0.8291\n",
      "2025-04-05 00:35:00,630 - INFO - Epoch [2/10], Batch [260/374], Loss: 0.7387\n",
      "2025-04-05 00:36:00,941 - INFO - Epoch [2/10], Batch [270/374], Loss: 0.5340\n",
      "2025-04-05 00:37:00,308 - INFO - Epoch [2/10], Batch [280/374], Loss: 0.6656\n",
      "2025-04-05 00:37:59,740 - INFO - Epoch [2/10], Batch [290/374], Loss: 0.5710\n",
      "2025-04-05 00:39:00,237 - INFO - Epoch [2/10], Batch [300/374], Loss: 0.6142\n",
      "2025-04-05 00:40:00,193 - INFO - Epoch [2/10], Batch [310/374], Loss: 0.5850\n",
      "2025-04-05 00:40:59,841 - INFO - Epoch [2/10], Batch [320/374], Loss: 0.6446\n",
      "2025-04-05 00:42:00,069 - INFO - Epoch [2/10], Batch [330/374], Loss: 0.5870\n",
      "2025-04-05 00:43:00,168 - INFO - Epoch [2/10], Batch [340/374], Loss: 0.5840\n",
      "2025-04-05 00:44:00,226 - INFO - Epoch [2/10], Batch [350/374], Loss: 0.6232\n",
      "2025-04-05 00:45:00,633 - INFO - Epoch [2/10], Batch [360/374], Loss: 0.6075\n",
      "2025-04-05 00:46:00,606 - INFO - Epoch [2/10], Batch [370/374], Loss: 0.7074\n",
      "2025-04-05 00:46:15,035 - INFO - Epoch [2/10] Train Loss: 0.6192, Train Accuracy: 65.22%\n",
      "2025-04-05 00:48:52,531 - INFO - Epoch [2/10] Val Loss: 0.5542, Val Accuracy: 74.29%\n",
      "2025-04-05 00:48:52,534 - INFO - New best model at epoch 2 with val accuracy: 74.29%\n",
      "2025-04-05 00:48:58,764 - INFO - Epoch [3/10], Batch [0/374], Loss: 0.8245\n",
      "2025-04-05 00:49:58,794 - INFO - Epoch [3/10], Batch [10/374], Loss: 0.6766\n",
      "2025-04-05 00:50:58,955 - INFO - Epoch [3/10], Batch [20/374], Loss: 0.6503\n",
      "2025-04-05 00:51:58,865 - INFO - Epoch [3/10], Batch [30/374], Loss: 0.5435\n",
      "2025-04-05 00:52:58,686 - INFO - Epoch [3/10], Batch [40/374], Loss: 0.5588\n",
      "2025-04-05 00:53:57,963 - INFO - Epoch [3/10], Batch [50/374], Loss: 0.5990\n",
      "2025-04-05 00:54:56,865 - INFO - Epoch [3/10], Batch [60/374], Loss: 0.7991\n",
      "2025-04-05 00:55:56,411 - INFO - Epoch [3/10], Batch [70/374], Loss: 0.5246\n",
      "2025-04-05 00:56:54,956 - INFO - Epoch [3/10], Batch [80/374], Loss: 0.6961\n",
      "2025-04-05 00:57:53,155 - INFO - Epoch [3/10], Batch [90/374], Loss: 0.5544\n",
      "2025-04-05 00:58:51,513 - INFO - Epoch [3/10], Batch [100/374], Loss: 0.6027\n",
      "2025-04-05 00:59:50,631 - INFO - Epoch [3/10], Batch [110/374], Loss: 0.5998\n",
      "2025-04-05 01:00:50,114 - INFO - Epoch [3/10], Batch [120/374], Loss: 0.5403\n",
      "2025-04-05 01:01:50,355 - INFO - Epoch [3/10], Batch [130/374], Loss: 0.6350\n",
      "2025-04-05 01:02:50,486 - INFO - Epoch [3/10], Batch [140/374], Loss: 0.5453\n",
      "2025-04-05 01:03:50,957 - INFO - Epoch [3/10], Batch [150/374], Loss: 0.5837\n",
      "2025-04-05 01:04:51,636 - INFO - Epoch [3/10], Batch [160/374], Loss: 0.6439\n",
      "2025-04-05 01:05:52,091 - INFO - Epoch [3/10], Batch [170/374], Loss: 0.6924\n",
      "2025-04-05 01:06:51,501 - INFO - Epoch [3/10], Batch [180/374], Loss: 0.6450\n",
      "2025-04-05 01:07:51,603 - INFO - Epoch [3/10], Batch [190/374], Loss: 0.6515\n",
      "2025-04-05 01:08:51,655 - INFO - Epoch [3/10], Batch [200/374], Loss: 0.5991\n",
      "2025-04-05 01:09:52,111 - INFO - Epoch [3/10], Batch [210/374], Loss: 0.7209\n",
      "2025-04-05 01:10:52,206 - INFO - Epoch [3/10], Batch [220/374], Loss: 0.5985\n",
      "2025-04-05 01:11:52,216 - INFO - Epoch [3/10], Batch [230/374], Loss: 0.6760\n",
      "2025-04-05 01:12:51,998 - INFO - Epoch [3/10], Batch [240/374], Loss: 0.6081\n",
      "2025-04-05 01:13:51,605 - INFO - Epoch [3/10], Batch [250/374], Loss: 0.5724\n",
      "2025-04-05 01:14:51,843 - INFO - Epoch [3/10], Batch [260/374], Loss: 0.5628\n",
      "2025-04-05 01:15:52,197 - INFO - Epoch [3/10], Batch [270/374], Loss: 0.4904\n",
      "2025-04-05 01:16:52,791 - INFO - Epoch [3/10], Batch [280/374], Loss: 0.6655\n",
      "2025-04-05 01:17:53,939 - INFO - Epoch [3/10], Batch [290/374], Loss: 0.6303\n",
      "2025-04-05 01:18:55,338 - INFO - Epoch [3/10], Batch [300/374], Loss: 0.4772\n",
      "2025-04-05 01:19:55,413 - INFO - Epoch [3/10], Batch [310/374], Loss: 0.6011\n",
      "2025-04-05 01:20:55,743 - INFO - Epoch [3/10], Batch [320/374], Loss: 0.5495\n",
      "2025-04-05 01:21:55,929 - INFO - Epoch [3/10], Batch [330/374], Loss: 0.5638\n",
      "2025-04-05 01:22:55,835 - INFO - Epoch [3/10], Batch [340/374], Loss: 0.4782\n",
      "2025-04-05 01:23:55,841 - INFO - Epoch [3/10], Batch [350/374], Loss: 0.6773\n",
      "2025-04-05 01:24:55,870 - INFO - Epoch [3/10], Batch [360/374], Loss: 0.5653\n",
      "2025-04-05 01:25:55,979 - INFO - Epoch [3/10], Batch [370/374], Loss: 0.4299\n",
      "2025-04-05 01:26:10,464 - INFO - Epoch [3/10] Train Loss: 0.5893, Train Accuracy: 67.39%\n",
      "2025-04-05 01:28:47,843 - INFO - Epoch [3/10] Val Loss: 0.5207, Val Accuracy: 75.82%\n",
      "2025-04-05 01:28:47,846 - INFO - New best model at epoch 3 with val accuracy: 75.82%\n",
      "2025-04-05 01:28:55,220 - INFO - Epoch [4/10], Batch [0/374], Loss: 0.6069\n",
      "2025-04-05 01:29:53,456 - INFO - Epoch [4/10], Batch [10/374], Loss: 0.5116\n",
      "2025-04-05 01:30:52,404 - INFO - Epoch [4/10], Batch [20/374], Loss: 0.4631\n",
      "2025-04-05 01:31:52,113 - INFO - Epoch [4/10], Batch [30/374], Loss: 0.5712\n",
      "2025-04-05 01:32:50,987 - INFO - Epoch [4/10], Batch [40/374], Loss: 0.5133\n",
      "2025-04-05 01:33:50,493 - INFO - Epoch [4/10], Batch [50/374], Loss: 0.5267\n",
      "2025-04-05 01:34:50,272 - INFO - Epoch [4/10], Batch [60/374], Loss: 0.5852\n",
      "2025-04-05 01:35:49,946 - INFO - Epoch [4/10], Batch [70/374], Loss: 0.5542\n",
      "2025-04-05 01:36:51,092 - INFO - Epoch [4/10], Batch [80/374], Loss: 0.5982\n",
      "2025-04-05 01:37:51,818 - INFO - Epoch [4/10], Batch [90/374], Loss: 0.5513\n",
      "2025-04-05 01:38:52,310 - INFO - Epoch [4/10], Batch [100/374], Loss: 0.5296\n",
      "2025-04-05 01:39:52,295 - INFO - Epoch [4/10], Batch [110/374], Loss: 0.7234\n",
      "2025-04-05 01:40:52,263 - INFO - Epoch [4/10], Batch [120/374], Loss: 0.4928\n",
      "2025-04-05 01:41:52,498 - INFO - Epoch [4/10], Batch [130/374], Loss: 0.5751\n",
      "2025-04-05 01:42:53,927 - INFO - Epoch [4/10], Batch [140/374], Loss: 0.4277\n",
      "2025-04-05 01:43:54,830 - INFO - Epoch [4/10], Batch [150/374], Loss: 0.5925\n",
      "2025-04-05 01:44:53,377 - INFO - Epoch [4/10], Batch [160/374], Loss: 0.7010\n",
      "2025-04-05 01:45:51,742 - INFO - Epoch [4/10], Batch [170/374], Loss: 0.5070\n",
      "2025-04-05 01:46:50,509 - INFO - Epoch [4/10], Batch [180/374], Loss: 0.6276\n",
      "2025-04-05 01:47:49,319 - INFO - Epoch [4/10], Batch [190/374], Loss: 0.4356\n",
      "2025-04-05 01:48:47,932 - INFO - Epoch [4/10], Batch [200/374], Loss: 0.5933\n",
      "2025-04-05 01:49:47,610 - INFO - Epoch [4/10], Batch [210/374], Loss: 0.4545\n",
      "2025-04-05 01:50:47,121 - INFO - Epoch [4/10], Batch [220/374], Loss: 0.4613\n",
      "2025-04-05 01:51:45,605 - INFO - Epoch [4/10], Batch [230/374], Loss: 0.4648\n",
      "2025-04-05 01:52:45,935 - INFO - Epoch [4/10], Batch [240/374], Loss: 0.6062\n",
      "2025-04-05 01:53:47,198 - INFO - Epoch [4/10], Batch [250/374], Loss: 0.6437\n",
      "2025-04-05 01:54:47,679 - INFO - Epoch [4/10], Batch [260/374], Loss: 0.5406\n",
      "2025-04-05 01:55:47,408 - INFO - Epoch [4/10], Batch [270/374], Loss: 0.4473\n",
      "2025-04-05 01:56:46,289 - INFO - Epoch [4/10], Batch [280/374], Loss: 0.4914\n",
      "2025-04-05 01:57:45,083 - INFO - Epoch [4/10], Batch [290/374], Loss: 0.4150\n",
      "2025-04-05 01:58:44,912 - INFO - Epoch [4/10], Batch [300/374], Loss: 0.4905\n",
      "2025-04-05 01:59:44,247 - INFO - Epoch [4/10], Batch [310/374], Loss: 0.3306\n",
      "2025-04-05 02:00:43,644 - INFO - Epoch [4/10], Batch [320/374], Loss: 0.6063\n",
      "2025-04-05 02:01:44,354 - INFO - Epoch [4/10], Batch [330/374], Loss: 0.4496\n",
      "2025-04-05 02:02:44,414 - INFO - Epoch [4/10], Batch [340/374], Loss: 0.5047\n",
      "2025-04-05 02:03:44,335 - INFO - Epoch [4/10], Batch [350/374], Loss: 0.4073\n",
      "2025-04-05 02:04:43,609 - INFO - Epoch [4/10], Batch [360/374], Loss: 0.3404\n",
      "2025-04-05 02:05:42,761 - INFO - Epoch [4/10], Batch [370/374], Loss: 0.5263\n",
      "2025-04-05 02:05:57,240 - INFO - Epoch [4/10] Train Loss: 0.5392, Train Accuracy: 72.52%\n",
      "2025-04-05 02:08:42,141 - INFO - Epoch [4/10] Val Loss: 0.5256, Val Accuracy: 76.83%\n",
      "2025-04-05 02:08:42,144 - INFO - New best model at epoch 4 with val accuracy: 76.83%\n",
      "2025-04-05 02:08:48,285 - INFO - Epoch [5/10], Batch [0/374], Loss: 0.5372\n",
      "2025-04-05 02:09:47,825 - INFO - Epoch [5/10], Batch [10/374], Loss: 0.3794\n",
      "2025-04-05 02:10:47,519 - INFO - Epoch [5/10], Batch [20/374], Loss: 0.4274\n",
      "2025-04-05 02:11:47,901 - INFO - Epoch [5/10], Batch [30/374], Loss: 0.3677\n",
      "2025-04-05 02:12:48,274 - INFO - Epoch [5/10], Batch [40/374], Loss: 0.3575\n",
      "2025-04-05 02:13:49,500 - INFO - Epoch [5/10], Batch [50/374], Loss: 0.3638\n",
      "2025-04-05 02:14:50,394 - INFO - Epoch [5/10], Batch [60/374], Loss: 0.5695\n",
      "2025-04-05 02:15:50,565 - INFO - Epoch [5/10], Batch [70/374], Loss: 0.4102\n",
      "2025-04-05 02:16:50,652 - INFO - Epoch [5/10], Batch [80/374], Loss: 0.3381\n",
      "2025-04-05 02:17:51,301 - INFO - Epoch [5/10], Batch [90/374], Loss: 0.4315\n",
      "2025-04-05 02:18:50,190 - INFO - Epoch [5/10], Batch [100/374], Loss: 0.4152\n",
      "2025-04-05 02:20:48,737 - INFO - Epoch [5/10], Batch [120/374], Loss: 0.5008\n",
      "2025-04-05 02:21:47,848 - INFO - Epoch [5/10], Batch [130/374], Loss: 0.6164\n",
      "2025-04-05 02:22:47,354 - INFO - Epoch [5/10], Batch [140/374], Loss: 0.4626\n",
      "2025-04-05 02:23:46,229 - INFO - Epoch [5/10], Batch [150/374], Loss: 0.5808\n",
      "2025-04-05 02:24:45,260 - INFO - Epoch [5/10], Batch [160/374], Loss: 0.4423\n",
      "2025-04-05 02:25:44,487 - INFO - Epoch [5/10], Batch [170/374], Loss: 0.4545\n",
      "2025-04-05 02:26:44,560 - INFO - Epoch [5/10], Batch [180/374], Loss: 0.3960\n",
      "2025-04-05 02:27:43,998 - INFO - Epoch [5/10], Batch [190/374], Loss: 0.5714\n",
      "2025-04-05 02:28:43,745 - INFO - Epoch [5/10], Batch [200/374], Loss: 0.5631\n",
      "2025-04-05 02:29:42,908 - INFO - Epoch [5/10], Batch [210/374], Loss: 0.3296\n",
      "2025-04-05 02:30:41,955 - INFO - Epoch [5/10], Batch [220/374], Loss: 0.4197\n",
      "2025-04-05 02:31:41,938 - INFO - Epoch [5/10], Batch [230/374], Loss: 0.4810\n",
      "2025-04-05 02:32:41,810 - INFO - Epoch [5/10], Batch [240/374], Loss: 0.4151\n",
      "2025-04-05 02:33:41,034 - INFO - Epoch [5/10], Batch [250/374], Loss: 0.6918\n",
      "2025-04-05 02:34:39,811 - INFO - Epoch [5/10], Batch [260/374], Loss: 0.3201\n",
      "2025-04-05 02:35:38,603 - INFO - Epoch [5/10], Batch [270/374], Loss: 0.5241\n",
      "2025-04-05 02:36:38,144 - INFO - Epoch [5/10], Batch [280/374], Loss: 0.4849\n",
      "2025-04-05 02:37:37,238 - INFO - Epoch [5/10], Batch [290/374], Loss: 0.6168\n",
      "2025-04-05 02:38:36,587 - INFO - Epoch [5/10], Batch [300/374], Loss: 0.5494\n",
      "2025-04-05 02:39:35,730 - INFO - Epoch [5/10], Batch [310/374], Loss: 0.2686\n",
      "2025-04-05 02:40:35,404 - INFO - Epoch [5/10], Batch [320/374], Loss: 0.3762\n",
      "2025-04-05 02:41:35,326 - INFO - Epoch [5/10], Batch [330/374], Loss: 0.4640\n",
      "2025-04-05 02:42:34,779 - INFO - Epoch [5/10], Batch [340/374], Loss: 0.3751\n"
     ]
    }
   ],
   "source": [
    "!python main.py --world_size 2 --num_threads 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922437f-e15c-4ceb-9978-5e9e048fbd80",
   "metadata": {},
   "source": [
    "### Workers = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69059b7a-c63f-4c8e-bc3b-5c519768e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --world_size 4 --num_threads 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f47ac-c38f-4ad1-9215-4035d630a744",
   "metadata": {},
   "source": [
    "### Workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d91d83-7eb0-43c5-89f9-b35391fef992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 14:18:51,257 - INFO - [Rank 3] Passed initial barrier\n",
      "2025-04-05 14:18:51,257 - INFO - [Rank 0] Passed initial barrier\n",
      "2025-04-05 14:18:51,257 - INFO - [Rank 1] Passed initial barrier\n",
      "2025-04-05 14:18:51,257 - INFO - [Rank 2] Passed initial barrier\n",
      "2025-04-05 14:18:51,291 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 14:18:51,291 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 14:18:51,291 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 14:18:51,291 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 14:21:10,320 - INFO - [Rank 3] Data loaded.\n",
      "2025-04-05 14:21:10,320 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-05 14:21:10,320 - INFO - [Rank 2] Data loaded.\n",
      "2025-04-05 14:21:10,320 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-05 14:21:10,810 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-05 14:21:10,810 - INFO - Total layers: 129\n",
      "2025-04-05 14:21:10,810 - INFO - Total parameters: 22,494,274\n",
      "2025-04-05 14:21:15,365 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6736\n",
      "2025-04-05 14:21:52,560 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.7030\n",
      "2025-04-05 14:22:28,162 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.5778\n",
      "2025-04-05 14:23:03,506 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.6837\n",
      "2025-04-05 14:23:39,128 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.6068\n",
      "2025-04-05 14:24:15,139 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6618\n",
      "2025-04-05 14:24:52,332 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.5881\n",
      "2025-04-05 14:25:29,374 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.6528\n",
      "2025-04-05 14:26:05,733 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.6641\n",
      "2025-04-05 14:26:41,717 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.7639\n",
      "2025-04-05 14:27:17,946 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.6302\n",
      "2025-04-05 14:27:54,472 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.5757\n",
      "2025-04-05 14:28:30,464 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6032\n",
      "2025-04-05 14:29:06,562 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.6475\n",
      "2025-04-05 14:29:42,738 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5536\n",
      "2025-04-05 14:30:19,126 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.5867\n",
      "2025-04-05 14:30:55,192 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.7121\n",
      "2025-04-05 14:31:31,014 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5220\n",
      "2025-04-05 14:32:08,041 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.5607\n",
      "2025-04-05 14:32:28,841 - INFO - Epoch [1/10] Train Loss: 0.6427, Train Accuracy: 61.07%\n",
      "2025-04-05 14:33:13,306 - INFO - Epoch [1/10] Val Loss: 0.6031, Val Accuracy: 70.08%\n",
      "2025-04-05 14:33:13,308 - INFO - New best model at epoch 1 with val accuracy: 70.08%\n",
      "2025-04-05 14:33:21,067 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.6017\n",
      "2025-04-05 14:33:57,595 - INFO - Epoch [2/10], Batch [10/187], Loss: 0.5844\n",
      "2025-04-05 14:34:32,926 - INFO - Epoch [2/10], Batch [20/187], Loss: 0.5620\n",
      "2025-04-05 14:35:09,204 - INFO - Epoch [2/10], Batch [30/187], Loss: 0.8731\n",
      "2025-04-05 14:35:45,757 - INFO - Epoch [2/10], Batch [40/187], Loss: 0.5260\n",
      "2025-04-05 14:36:21,154 - INFO - Epoch [2/10], Batch [50/187], Loss: 0.5197\n",
      "2025-04-05 14:36:56,048 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.5263\n",
      "2025-04-05 14:37:31,373 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.6291\n",
      "2025-04-05 14:38:07,183 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.6604\n",
      "2025-04-05 14:38:41,706 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.6862\n",
      "2025-04-05 14:39:17,217 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.6292\n",
      "2025-04-05 14:39:53,467 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.6390\n",
      "2025-04-05 14:40:28,290 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.6734\n",
      "2025-04-05 14:41:03,563 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.6740\n",
      "2025-04-05 14:41:38,646 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.6190\n",
      "2025-04-05 14:42:13,620 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.5282\n",
      "2025-04-05 14:42:48,785 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.6628\n",
      "2025-04-05 14:43:23,436 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.4604\n",
      "2025-04-05 14:43:58,888 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.5713\n",
      "2025-04-05 14:44:18,635 - INFO - Epoch [2/10] Train Loss: 0.5983, Train Accuracy: 68.18%\n",
      "2025-04-05 14:45:02,355 - INFO - Epoch [2/10] Val Loss: 0.5706, Val Accuracy: 75.43%\n",
      "2025-04-05 14:45:02,358 - INFO - New best model at epoch 2 with val accuracy: 75.43%\n",
      "2025-04-05 14:45:07,237 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.5414\n",
      "2025-04-05 14:45:43,318 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.5954\n",
      "2025-04-05 14:46:18,068 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.4025\n",
      "2025-04-05 14:46:53,476 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.4975\n",
      "2025-04-05 14:47:27,955 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.5613\n",
      "2025-04-05 14:48:01,957 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.5970\n",
      "2025-04-05 14:48:37,402 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.6265\n",
      "2025-04-05 14:49:15,076 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.4186\n",
      "2025-04-05 14:49:51,844 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.7722\n",
      "2025-04-05 14:50:27,448 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.4317\n",
      "2025-04-05 14:51:02,904 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.5441\n",
      "2025-04-05 14:51:38,234 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.6972\n",
      "2025-04-05 14:52:13,663 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.4753\n",
      "2025-04-05 14:52:48,823 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5040\n",
      "2025-04-05 14:53:24,535 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.4683\n",
      "2025-04-05 14:53:59,054 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.4890\n",
      "2025-04-05 14:54:32,885 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.3678\n",
      "2025-04-05 14:55:06,923 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.5576\n",
      "2025-04-05 14:55:41,211 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.5585\n",
      "2025-04-05 14:56:00,502 - INFO - Epoch [3/10] Train Loss: 0.5132, Train Accuracy: 75.16%\n",
      "2025-04-05 14:56:44,470 - INFO - Epoch [3/10] Val Loss: 0.4866, Val Accuracy: 78.43%\n",
      "2025-04-05 14:56:44,472 - INFO - New best model at epoch 3 with val accuracy: 78.43%\n",
      "2025-04-05 14:56:49,643 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.7194\n",
      "2025-04-05 14:57:23,893 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.2890\n",
      "2025-04-05 14:57:57,826 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.4559\n",
      "2025-04-05 14:58:32,028 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.4137\n",
      "2025-04-05 14:59:06,432 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.4671\n",
      "2025-04-05 14:59:40,663 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.4394\n",
      "2025-04-05 15:00:14,533 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.4981\n",
      "2025-04-05 15:00:48,372 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.4499\n",
      "2025-04-05 15:01:22,565 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.3386\n",
      "2025-04-05 15:01:56,682 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.5284\n",
      "2025-04-05 15:02:30,801 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4325\n",
      "2025-04-05 15:03:06,297 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.4117\n",
      "2025-04-05 15:03:41,316 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.5263\n",
      "2025-04-05 15:04:16,679 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.5328\n",
      "2025-04-05 15:04:53,052 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.3448\n",
      "2025-04-05 15:05:27,950 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.4024\n",
      "2025-04-05 15:06:04,392 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.3474\n",
      "2025-04-05 15:06:39,372 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.4359\n",
      "2025-04-05 15:07:16,855 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.3430\n",
      "2025-04-05 15:07:37,764 - INFO - Epoch [4/10] Train Loss: 0.4474, Train Accuracy: 78.59%\n",
      "2025-04-05 15:08:22,481 - INFO - Epoch [4/10] Val Loss: 0.3582, Val Accuracy: 84.62%\n",
      "2025-04-05 15:08:22,483 - INFO - New best model at epoch 4 with val accuracy: 84.62%\n",
      "2025-04-05 15:08:26,172 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.4966\n",
      "2025-04-05 15:09:02,513 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.5232\n",
      "2025-04-05 15:09:37,757 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.3635\n",
      "2025-04-05 15:10:12,531 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.4574\n",
      "2025-04-05 15:10:47,006 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.5065\n",
      "2025-04-05 15:11:21,214 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.2664\n",
      "2025-04-05 15:11:55,492 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.4270\n",
      "2025-04-05 15:12:30,411 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.2869\n",
      "2025-04-05 15:13:05,652 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.3330\n",
      "2025-04-05 15:13:41,763 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.4757\n",
      "2025-04-05 15:14:17,052 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.4676\n",
      "2025-04-05 15:14:51,304 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3476\n",
      "2025-04-05 15:15:25,133 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.5172\n",
      "2025-04-05 15:15:59,568 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.2000\n",
      "2025-04-05 15:16:34,799 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.4369\n",
      "2025-04-05 15:17:09,414 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.4726\n",
      "2025-04-05 15:17:44,426 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.2995\n",
      "2025-04-05 15:18:19,646 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.4062\n",
      "2025-04-05 15:18:55,222 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.4891\n",
      "2025-04-05 15:19:15,361 - INFO - Epoch [5/10] Train Loss: 0.4001, Train Accuracy: 80.92%\n",
      "2025-04-05 15:19:59,650 - INFO - Epoch [5/10] Val Loss: 0.3449, Val Accuracy: 85.04%\n",
      "2025-04-05 15:19:59,652 - INFO - New best model at epoch 5 with val accuracy: 85.04%\n",
      "2025-04-05 15:20:05,158 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.4028\n",
      "2025-04-05 15:20:40,203 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.4050\n",
      "2025-04-05 15:21:14,144 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.5398\n",
      "2025-04-05 15:21:47,966 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.3811\n",
      "2025-04-05 15:22:23,171 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2555\n",
      "2025-04-05 15:23:00,158 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.4605\n",
      "2025-04-05 15:23:36,274 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.3931\n",
      "2025-04-05 15:24:12,418 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.4621\n",
      "2025-04-05 15:24:47,890 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.3092\n",
      "2025-04-05 15:25:22,953 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.3538\n",
      "2025-04-05 15:25:57,613 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.4296\n",
      "2025-04-05 15:26:31,949 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2368\n",
      "2025-04-05 15:27:06,480 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.6630\n",
      "2025-04-05 15:27:40,881 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.2952\n",
      "2025-04-05 15:28:15,194 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.2592\n",
      "2025-04-05 15:28:49,467 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.2873\n",
      "2025-04-05 15:29:23,499 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.3674\n",
      "2025-04-05 15:29:57,438 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.4676\n",
      "2025-04-05 15:30:32,307 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.3319\n",
      "2025-04-05 15:30:52,275 - INFO - Epoch [6/10] Train Loss: 0.3736, Train Accuracy: 82.81%\n",
      "2025-04-05 15:31:36,151 - INFO - Epoch [6/10] Val Loss: 0.3157, Val Accuracy: 87.54%\n",
      "2025-04-05 15:31:36,154 - INFO - New best model at epoch 6 with val accuracy: 87.54%\n",
      "2025-04-05 15:31:42,206 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.2002\n",
      "2025-04-05 15:32:17,896 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.4601\n",
      "2025-04-05 15:32:52,829 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.4402\n",
      "2025-04-05 15:33:28,016 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.2834\n",
      "2025-04-05 15:34:02,381 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.3420\n",
      "2025-04-05 15:34:36,840 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.3517\n",
      "2025-04-05 15:35:11,349 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.5420\n",
      "2025-04-05 15:35:46,498 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.4231\n",
      "2025-04-05 15:36:21,222 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.2877\n",
      "2025-04-05 15:36:55,787 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.2075\n",
      "2025-04-05 15:37:32,704 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.3114\n",
      "2025-04-05 15:38:09,423 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.4154\n",
      "2025-04-05 15:38:47,195 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.4218\n",
      "2025-04-05 15:39:22,963 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.2731\n",
      "2025-04-05 15:39:59,253 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.6546\n",
      "2025-04-05 15:40:34,349 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.4327\n",
      "2025-04-05 15:41:09,411 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.3505\n",
      "2025-04-05 15:41:45,175 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.4790\n",
      "2025-04-05 15:42:20,006 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2567\n",
      "2025-04-05 15:42:40,232 - INFO - Epoch [7/10] Train Loss: 0.3596, Train Accuracy: 83.40%\n",
      "2025-04-05 15:43:25,711 - INFO - Epoch [7/10] Val Loss: 0.3440, Val Accuracy: 84.27%\n",
      "2025-04-05 15:43:31,203 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.4294\n",
      "2025-04-05 15:44:08,074 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.3674\n",
      "2025-04-05 15:44:46,560 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.3787\n",
      "2025-04-05 15:45:23,363 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.3103\n",
      "2025-04-05 15:46:00,555 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.2238\n",
      "2025-04-05 15:46:36,417 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2007\n",
      "2025-04-05 15:47:12,231 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.1411\n",
      "2025-04-05 15:47:48,087 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.3399\n",
      "2025-04-05 15:48:24,010 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.3974\n",
      "2025-04-05 15:49:01,016 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.3394\n",
      "2025-04-05 15:49:36,879 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.4458\n",
      "2025-04-05 15:50:14,004 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.3382\n",
      "2025-04-05 15:50:51,359 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.2147\n",
      "2025-04-05 15:51:28,231 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.2361\n",
      "2025-04-05 15:52:05,806 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.3176\n",
      "2025-04-05 15:52:41,936 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.4663\n",
      "2025-04-05 15:53:19,537 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.3816\n",
      "2025-04-05 15:53:56,131 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3362\n",
      "2025-04-05 15:54:31,960 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.3830\n",
      "2025-04-05 15:54:52,341 - INFO - Epoch [8/10] Train Loss: 0.3371, Train Accuracy: 85.19%\n",
      "2025-04-05 15:55:38,549 - INFO - Epoch [8/10] Val Loss: 0.2788, Val Accuracy: 88.52%\n",
      "2025-04-05 15:55:38,551 - INFO - New best model at epoch 8 with val accuracy: 88.52%\n",
      "2025-04-05 15:55:43,878 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.2874\n",
      "2025-04-05 15:56:20,529 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.5092\n",
      "2025-04-05 15:56:56,900 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.1998\n",
      "2025-04-05 15:57:33,071 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.3122\n",
      "2025-04-05 15:58:09,685 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.3433\n",
      "2025-04-05 15:58:47,854 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2136\n",
      "2025-04-05 15:59:25,355 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.1431\n",
      "2025-04-05 16:00:03,147 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2238\n",
      "2025-04-05 16:00:39,483 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.2879\n",
      "2025-04-05 16:01:15,272 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.3542\n",
      "2025-04-05 16:01:51,223 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4198\n",
      "2025-04-05 16:02:26,994 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.2025\n",
      "2025-04-05 16:03:02,312 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.2731\n",
      "2025-04-05 16:03:38,521 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2149\n",
      "2025-04-05 16:04:14,752 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.4273\n",
      "2025-04-05 16:04:49,960 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.3053\n",
      "2025-04-05 16:05:24,579 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.3469\n",
      "2025-04-05 16:05:58,585 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.4079\n",
      "2025-04-05 16:06:32,562 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.2937\n",
      "2025-04-05 16:06:51,943 - INFO - Epoch [9/10] Train Loss: 0.3278, Train Accuracy: 85.24%\n",
      "2025-04-05 16:07:36,146 - INFO - Epoch [9/10] Val Loss: 0.2614, Val Accuracy: 89.49%\n",
      "2025-04-05 16:07:36,148 - INFO - New best model at epoch 9 with val accuracy: 89.49%\n",
      "2025-04-05 16:07:41,170 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.4573\n",
      "2025-04-05 16:08:15,218 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.5552\n",
      "2025-04-05 16:08:49,132 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.3005\n",
      "2025-04-05 16:09:23,110 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.3288\n",
      "2025-04-05 16:09:57,089 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.3985\n",
      "2025-04-05 16:10:31,236 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2519\n",
      "2025-04-05 16:11:04,606 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2042\n",
      "2025-04-05 16:11:38,038 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.2998\n",
      "2025-04-05 16:12:11,601 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.4004\n",
      "2025-04-05 16:12:45,033 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.3524\n",
      "2025-04-05 16:13:18,576 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.1782\n",
      "2025-04-05 16:13:52,169 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.2691\n",
      "2025-04-05 16:14:25,799 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.2945\n",
      "2025-04-05 16:14:59,381 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.2609\n",
      "2025-04-05 16:15:32,880 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.4999\n",
      "2025-04-05 16:16:06,448 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.4483\n",
      "2025-04-05 16:16:40,148 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.2265\n",
      "2025-04-05 16:17:14,030 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.3664\n",
      "2025-04-05 16:17:47,680 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.2569\n",
      "2025-04-05 16:18:06,885 - INFO - Epoch [10/10] Train Loss: 0.3118, Train Accuracy: 85.69%\n",
      "2025-04-05 16:18:51,425 - INFO - Epoch [10/10] Val Loss: 0.2530, Val Accuracy: 89.21%\n",
      "2025-04-05 16:19:14,173 - INFO - Test Loss: 0.2879, Test Accuracy: 88.87%\n",
      "2025-04-05 16:19:14,174 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-05 16:19:14,175 - INFO - {\n",
      "    \"world_size\": 4,\n",
      "    \"train_time\": 7060.608250141144,\n",
      "    \"avg_epoch_time\": 706.0608250141144,\n",
      "    \"val_accuracy\": 89.4919972164231,\n",
      "    \"test_accuracy\": 88.87343532684284,\n",
      "    \"test_loss\": 0.2878904207525399,\n",
      "    \"total_time\": 7060.608250141144,\n",
      "    \"train_losses\": [\n",
      "        0.6426999837284806,\n",
      "        0.5983306241534245,\n",
      "        0.5132324242342466,\n",
      "        0.44736579509958563,\n",
      "        0.4000942371380379,\n",
      "        0.3735516778669597,\n",
      "        0.359630262946484,\n",
      "        0.3370587286116189,\n",
      "        0.32779783366614307,\n",
      "        0.3118303519761712\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        61.07112970711297,\n",
      "        68.18410041841004,\n",
      "        75.16317991631799,\n",
      "        78.59414225941423,\n",
      "        80.92050209205021,\n",
      "        82.81171548117155,\n",
      "        83.39748953974896,\n",
      "        85.18828451882845,\n",
      "        85.23849372384937,\n",
      "        85.69037656903765\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.6030554726626529,\n",
      "        0.5705916254405935,\n",
      "        0.4865935298412644,\n",
      "        0.35818259439786937,\n",
      "        0.3448646287164841,\n",
      "        0.3156548889046671,\n",
      "        0.34402062503845227,\n",
      "        0.278816649751192,\n",
      "        0.26138379255266925,\n",
      "        0.2529710421731427\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        70.07654836464857,\n",
      "        75.43493389004871,\n",
      "        78.42727905358386,\n",
      "        84.62073764787752,\n",
      "        85.03827418232429,\n",
      "        87.54349338900487,\n",
      "        84.27279053583855,\n",
      "        88.51774530271399,\n",
      "        89.4919972164231,\n",
      "        89.21363952679192\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-05 16:19:14,399 - INFO - Model saved to models/training_using_cpus_4_best_model.pt\n",
      "2025-04-05 16:19:19,513 - INFO - Loss plot saved as plots/training_using_cpus_4_loss.png\n",
      "2025-04-05 16:19:19,621 - INFO - Accuracy plot saved as plots/training_using_cpus_4_accuracy.png\n",
      "2025-04-05 16:19:19,638 - INFO - Runtime parameters saved as metrics/training_using_cpus_4_params.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py --world_size 4 --num_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f87c5-f677-4813-9d36-88603a2c9f61",
   "metadata": {},
   "source": [
    "### Workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c654abca-fb30-4ea0-b1d5-1edf15a75b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "[Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "[Rank 1] Data loaded.\n",
      "[Rank 0] Data loaded.\n",
      "Model architecture: MedicalCNN\n",
      "Total layers: 129\n",
      "Total parameters: 22,494,274\n",
      "Epoch [1/10], Batch [0/374], Loss: 0.6760\n",
      "Epoch [1/10], Batch [10/374], Loss: 0.6668\n",
      "Epoch [1/10], Batch [20/374], Loss: 0.6710\n",
      "Epoch [1/10], Batch [30/374], Loss: 0.6832\n",
      "Epoch [1/10], Batch [40/374], Loss: 0.6123\n",
      "Epoch [1/10], Batch [50/374], Loss: 0.7173\n",
      "Epoch [1/10], Batch [60/374], Loss: 0.6026\n",
      "Epoch [1/10], Batch [70/374], Loss: 0.6402\n",
      "Epoch [1/10], Batch [80/374], Loss: 0.5743\n",
      "Epoch [1/10], Batch [90/374], Loss: 0.7215\n",
      "Epoch [1/10], Batch [100/374], Loss: 0.6646\n",
      "Epoch [1/10], Batch [110/374], Loss: 0.5912\n",
      "Epoch [1/10], Batch [120/374], Loss: 0.6117\n",
      "Epoch [1/10], Batch [130/374], Loss: 0.6286\n",
      "Epoch [1/10], Batch [140/374], Loss: 0.7176\n",
      "Epoch [1/10], Batch [150/374], Loss: 0.7413\n",
      "Epoch [1/10], Batch [160/374], Loss: 0.6253\n",
      "Epoch [1/10], Batch [170/374], Loss: 0.6028\n",
      "Epoch [1/10], Batch [180/374], Loss: 0.6318\n",
      "Epoch [1/10], Batch [190/374], Loss: 0.6040\n",
      "Epoch [1/10], Batch [200/374], Loss: 0.7123\n",
      "Epoch [1/10], Batch [210/374], Loss: 0.7449\n",
      "Epoch [1/10], Batch [220/374], Loss: 0.6383\n",
      "Epoch [1/10], Batch [230/374], Loss: 0.6311\n",
      "Epoch [1/10], Batch [240/374], Loss: 0.5673\n",
      "Epoch [1/10], Batch [250/374], Loss: 0.5804\n",
      "Epoch [1/10], Batch [260/374], Loss: 0.6006\n",
      "Epoch [1/10], Batch [270/374], Loss: 0.5642\n",
      "Epoch [1/10], Batch [280/374], Loss: 0.5260\n",
      "Epoch [1/10], Batch [290/374], Loss: 0.5660\n",
      "Epoch [1/10], Batch [300/374], Loss: 0.5929\n",
      "Epoch [1/10], Batch [310/374], Loss: 0.5998\n",
      "Epoch [1/10], Batch [320/374], Loss: 0.8101\n",
      "Epoch [1/10], Batch [330/374], Loss: 0.7577\n",
      "Epoch [1/10], Batch [340/374], Loss: 0.5835\n",
      "Epoch [1/10], Batch [350/374], Loss: 0.6793\n",
      "Epoch [1/10], Batch [360/374], Loss: 0.7575\n",
      "Epoch [1/10], Batch [370/374], Loss: 0.5803\n",
      "Epoch [1/10] Train Loss: 0.6417, Train Accuracy: 61.24%\n",
      "Epoch [1/10] Val Loss: 0.6419, Val Accuracy: 67.36%\n",
      "New best model at epoch 1 with val accuracy: 67.36%\n",
      "Epoch [2/10], Batch [0/374], Loss: 0.6297\n",
      "Epoch [2/10], Batch [10/374], Loss: 0.5767\n",
      "Epoch [2/10], Batch [20/374], Loss: 0.4861\n",
      "Epoch [2/10], Batch [30/374], Loss: 0.6014\n",
      "Epoch [2/10], Batch [40/374], Loss: 0.8125\n",
      "Epoch [2/10], Batch [50/374], Loss: 0.6014\n",
      "Epoch [2/10], Batch [60/374], Loss: 0.7028\n",
      "Epoch [2/10], Batch [70/374], Loss: 0.6933\n",
      "Epoch [2/10], Batch [80/374], Loss: 0.5443\n",
      "Epoch [2/10], Batch [90/374], Loss: 0.5921\n",
      "Epoch [2/10], Batch [100/374], Loss: 0.5712\n",
      "Epoch [2/10], Batch [110/374], Loss: 0.6367\n",
      "Epoch [2/10], Batch [120/374], Loss: 0.4773\n",
      "Epoch [2/10], Batch [130/374], Loss: 0.5408\n",
      "Epoch [2/10], Batch [140/374], Loss: 0.6834\n",
      "Epoch [2/10], Batch [150/374], Loss: 0.5516\n",
      "Epoch [2/10], Batch [160/374], Loss: 0.5867\n",
      "Epoch [2/10], Batch [170/374], Loss: 0.7115\n",
      "Epoch [2/10], Batch [180/374], Loss: 0.6219\n",
      "Epoch [2/10], Batch [190/374], Loss: 0.5289\n",
      "Epoch [2/10], Batch [200/374], Loss: 0.7280\n",
      "Epoch [2/10], Batch [210/374], Loss: 0.6014\n",
      "Epoch [2/10], Batch [220/374], Loss: 0.7413\n",
      "Epoch [2/10], Batch [230/374], Loss: 0.5707\n",
      "Epoch [2/10], Batch [240/374], Loss: 0.7575\n",
      "Epoch [2/10], Batch [250/374], Loss: 0.7128\n",
      "Epoch [2/10], Batch [260/374], Loss: 0.7240\n",
      "Epoch [2/10], Batch [270/374], Loss: 0.5098\n",
      "Epoch [2/10], Batch [280/374], Loss: 0.5794\n",
      "Epoch [2/10], Batch [290/374], Loss: 0.6354\n",
      "Epoch [2/10], Batch [300/374], Loss: 0.5310\n",
      "Epoch [2/10], Batch [310/374], Loss: 0.6127\n",
      "Epoch [2/10], Batch [320/374], Loss: 0.6303\n",
      "Epoch [2/10], Batch [330/374], Loss: 0.5253\n",
      "Epoch [2/10], Batch [340/374], Loss: 0.5474\n",
      "Epoch [2/10], Batch [350/374], Loss: 0.5803\n",
      "Epoch [2/10], Batch [360/374], Loss: 0.6354\n",
      "Epoch [2/10], Batch [370/374], Loss: 0.7366\n",
      "Epoch [2/10] Train Loss: 0.6192, Train Accuracy: 65.00%\n",
      "Epoch [2/10] Val Loss: 0.5972, Val Accuracy: 73.87%\n",
      "New best model at epoch 2 with val accuracy: 73.87%\n",
      "Epoch [3/10], Batch [0/374], Loss: 0.7974\n",
      "Epoch [3/10], Batch [10/374], Loss: 0.6391\n",
      "Epoch [3/10], Batch [20/374], Loss: 0.6489\n",
      "Epoch [3/10], Batch [30/374], Loss: 0.5504\n",
      "Epoch [3/10], Batch [40/374], Loss: 0.5325\n",
      "Epoch [3/10], Batch [50/374], Loss: 0.7611\n",
      "Epoch [3/10], Batch [60/374], Loss: 0.6121\n",
      "Epoch [3/10], Batch [70/374], Loss: 0.5529\n",
      "Epoch [3/10], Batch [80/374], Loss: 0.6858\n",
      "Epoch [3/10], Batch [90/374], Loss: 0.6134\n",
      "Epoch [3/10], Batch [100/374], Loss: 0.5607\n",
      "Epoch [3/10], Batch [110/374], Loss: 0.5609\n",
      "Epoch [3/10], Batch [120/374], Loss: 0.5015\n",
      "Epoch [3/10], Batch [130/374], Loss: 0.5819\n",
      "Epoch [3/10], Batch [140/374], Loss: 0.5929\n",
      "Epoch [3/10], Batch [150/374], Loss: 0.6039\n",
      "Epoch [3/10], Batch [160/374], Loss: 0.6661\n",
      "Epoch [3/10], Batch [170/374], Loss: 0.7214\n",
      "Epoch [3/10], Batch [180/374], Loss: 0.5860\n",
      "Epoch [3/10], Batch [190/374], Loss: 0.6328\n",
      "Epoch [3/10], Batch [200/374], Loss: 0.5604\n",
      "Epoch [3/10], Batch [210/374], Loss: 0.8366\n",
      "Epoch [3/10], Batch [220/374], Loss: 0.6461\n",
      "Epoch [3/10], Batch [230/374], Loss: 0.5813\n",
      "Epoch [3/10], Batch [240/374], Loss: 0.5346\n",
      "Epoch [3/10], Batch [250/374], Loss: 0.5493\n",
      "Epoch [3/10], Batch [260/374], Loss: 0.5171\n",
      "Epoch [3/10], Batch [270/374], Loss: 0.4980\n",
      "Epoch [3/10], Batch [280/374], Loss: 0.6560\n",
      "Epoch [3/10], Batch [290/374], Loss: 0.5288\n",
      "Epoch [3/10], Batch [300/374], Loss: 0.4546\n",
      "Epoch [3/10], Batch [310/374], Loss: 0.6094\n",
      "Epoch [3/10], Batch [320/374], Loss: 0.5085\n",
      "Epoch [3/10], Batch [330/374], Loss: 0.5509\n",
      "Epoch [3/10], Batch [340/374], Loss: 0.5186\n",
      "Epoch [3/10], Batch [350/374], Loss: 0.6691\n",
      "Epoch [3/10], Batch [360/374], Loss: 0.5008\n",
      "Epoch [3/10], Batch [370/374], Loss: 0.4605\n",
      "Epoch [3/10] Train Loss: 0.5815, Train Accuracy: 67.68%\n",
      "Epoch [3/10] Val Loss: 0.5094, Val Accuracy: 76.06%\n",
      "New best model at epoch 3 with val accuracy: 76.06%\n",
      "Epoch [4/10], Batch [0/374], Loss: 0.5381\n",
      "Epoch [4/10], Batch [10/374], Loss: 0.5643\n",
      "Epoch [4/10], Batch [20/374], Loss: 0.4477\n",
      "Epoch [4/10], Batch [30/374], Loss: 0.4794\n",
      "Epoch [4/10], Batch [40/374], Loss: 0.5880\n",
      "Epoch [4/10], Batch [50/374], Loss: 0.5247\n",
      "Epoch [4/10], Batch [60/374], Loss: 0.6356\n",
      "Epoch [4/10], Batch [70/374], Loss: 0.5852\n",
      "Epoch [4/10], Batch [80/374], Loss: 0.7423\n",
      "Epoch [4/10], Batch [90/374], Loss: 0.5886\n",
      "Epoch [4/10], Batch [100/374], Loss: 0.5940\n",
      "Epoch [4/10], Batch [110/374], Loss: 0.6591\n",
      "Epoch [4/10], Batch [120/374], Loss: 0.5166\n",
      "Epoch [4/10], Batch [130/374], Loss: 0.6237\n",
      "Epoch [4/10], Batch [140/374], Loss: 0.4370\n",
      "Epoch [4/10], Batch [150/374], Loss: 0.5751\n",
      "Epoch [4/10], Batch [160/374], Loss: 0.7055\n",
      "Epoch [4/10], Batch [170/374], Loss: 0.4801\n",
      "Epoch [4/10], Batch [180/374], Loss: 0.6282\n",
      "Epoch [4/10], Batch [190/374], Loss: 0.5346\n",
      "Epoch [4/10], Batch [200/374], Loss: 0.5371\n",
      "Epoch [4/10], Batch [210/374], Loss: 0.3919\n",
      "Epoch [4/10], Batch [220/374], Loss: 0.4723\n",
      "Epoch [4/10], Batch [230/374], Loss: 0.5396\n",
      "Epoch [4/10], Batch [240/374], Loss: 0.4919\n",
      "Epoch [4/10], Batch [250/374], Loss: 0.5461\n",
      "Epoch [4/10], Batch [260/374], Loss: 0.6612\n",
      "Epoch [4/10], Batch [270/374], Loss: 0.5387\n",
      "Epoch [4/10], Batch [280/374], Loss: 0.4941\n",
      "Epoch [4/10], Batch [290/374], Loss: 0.5047\n",
      "Epoch [4/10], Batch [300/374], Loss: 0.5160\n",
      "Epoch [4/10], Batch [310/374], Loss: 0.4543\n",
      "Epoch [4/10], Batch [320/374], Loss: 0.5779\n",
      "Epoch [4/10], Batch [330/374], Loss: 0.4306\n",
      "Epoch [4/10], Batch [340/374], Loss: 0.4123\n",
      "Epoch [4/10], Batch [350/374], Loss: 0.5817\n",
      "Epoch [4/10], Batch [360/374], Loss: 0.3391\n",
      "Epoch [4/10], Batch [370/374], Loss: 0.4451\n",
      "Epoch [4/10] Train Loss: 0.5375, Train Accuracy: 72.06%\n",
      "Epoch [4/10] Val Loss: 0.5032, Val Accuracy: 75.82%\n",
      "Epoch [5/10], Batch [0/374], Loss: 0.4809\n",
      "Epoch [5/10], Batch [10/374], Loss: 0.3655\n",
      "Epoch [5/10], Batch [20/374], Loss: 0.5462\n",
      "Epoch [5/10], Batch [30/374], Loss: 0.4158\n",
      "Epoch [5/10], Batch [40/374], Loss: 0.3733\n",
      "Epoch [5/10], Batch [50/374], Loss: 0.3477\n",
      "Epoch [5/10], Batch [60/374], Loss: 0.5807\n",
      "Epoch [5/10], Batch [70/374], Loss: 0.3194\n",
      "Epoch [5/10], Batch [80/374], Loss: 0.3665\n",
      "Epoch [5/10], Batch [90/374], Loss: 0.4201\n",
      "Epoch [5/10], Batch [100/374], Loss: 0.3509\n",
      "Epoch [5/10], Batch [110/374], Loss: 0.4919\n",
      "Epoch [5/10], Batch [120/374], Loss: 0.4493\n",
      "Epoch [5/10], Batch [130/374], Loss: 0.5643\n",
      "Epoch [5/10], Batch [140/374], Loss: 0.4018\n",
      "Epoch [5/10], Batch [150/374], Loss: 0.3726\n",
      "Epoch [5/10], Batch [160/374], Loss: 0.4028\n",
      "Epoch [5/10], Batch [170/374], Loss: 0.3490\n",
      "Epoch [5/10], Batch [180/374], Loss: 0.3952\n",
      "Epoch [5/10], Batch [190/374], Loss: 0.5721\n",
      "Epoch [5/10], Batch [200/374], Loss: 0.5485\n",
      "Epoch [5/10], Batch [210/374], Loss: 0.3537\n",
      "Epoch [5/10], Batch [220/374], Loss: 0.3697\n",
      "Epoch [5/10], Batch [230/374], Loss: 0.4643\n",
      "Epoch [5/10], Batch [240/374], Loss: 0.5095\n",
      "Epoch [5/10], Batch [250/374], Loss: 0.7652\n",
      "Epoch [5/10], Batch [260/374], Loss: 0.3409\n",
      "Epoch [5/10], Batch [270/374], Loss: 0.5184\n",
      "Epoch [5/10], Batch [280/374], Loss: 0.4472\n",
      "Epoch [5/10], Batch [290/374], Loss: 0.5519\n",
      "Epoch [5/10], Batch [300/374], Loss: 0.5375\n",
      "Epoch [5/10], Batch [310/374], Loss: 0.3554\n",
      "Epoch [5/10], Batch [320/374], Loss: 0.3735\n",
      "Epoch [5/10], Batch [330/374], Loss: 0.4247\n",
      "Epoch [5/10], Batch [340/374], Loss: 0.4580\n",
      "Epoch [5/10], Batch [350/374], Loss: 0.3754\n",
      "Epoch [5/10], Batch [360/374], Loss: 0.3716\n",
      "Epoch [5/10], Batch [370/374], Loss: 0.6153\n",
      "Epoch [5/10] Train Loss: 0.4690, Train Accuracy: 77.37%\n",
      "Epoch [5/10] Val Loss: 0.4146, Val Accuracy: 81.49%\n",
      "New best model at epoch 5 with val accuracy: 81.49%\n",
      "Epoch [6/10], Batch [0/374], Loss: 0.4543\n",
      "Epoch [6/10], Batch [10/374], Loss: 0.5251\n",
      "Epoch [6/10], Batch [20/374], Loss: 0.4452\n",
      "Epoch [6/10], Batch [30/374], Loss: 0.4825\n",
      "Epoch [6/10], Batch [40/374], Loss: 0.5687\n",
      "Epoch [6/10], Batch [50/374], Loss: 0.3249\n",
      "Epoch [6/10], Batch [60/374], Loss: 0.4797\n",
      "Epoch [6/10], Batch [70/374], Loss: 0.6316\n",
      "Epoch [6/10], Batch [80/374], Loss: 0.4051\n",
      "Epoch [6/10], Batch [90/374], Loss: 0.4876\n",
      "Epoch [6/10], Batch [100/374], Loss: 0.3976\n",
      "Epoch [6/10], Batch [110/374], Loss: 0.4114\n",
      "Epoch [6/10], Batch [120/374], Loss: 0.3910\n",
      "Epoch [6/10], Batch [130/374], Loss: 0.4178\n",
      "Epoch [6/10], Batch [140/374], Loss: 0.3730\n",
      "Epoch [6/10], Batch [150/374], Loss: 0.2978\n",
      "Epoch [6/10], Batch [160/374], Loss: 0.3580\n",
      "Epoch [6/10], Batch [170/374], Loss: 0.3491\n",
      "Epoch [6/10], Batch [180/374], Loss: 0.4600\n",
      "Epoch [6/10], Batch [190/374], Loss: 0.3045\n",
      "Epoch [6/10], Batch [200/374], Loss: 0.4037\n",
      "Epoch [6/10], Batch [210/374], Loss: 0.3660\n",
      "Epoch [6/10], Batch [220/374], Loss: 0.4454\n",
      "Epoch [6/10], Batch [230/374], Loss: 0.3319\n",
      "Epoch [6/10], Batch [240/374], Loss: 0.7355\n",
      "Epoch [6/10], Batch [250/374], Loss: 0.5043\n",
      "Epoch [6/10], Batch [260/374], Loss: 0.3364\n",
      "Epoch [6/10], Batch [270/374], Loss: 0.3519\n",
      "Epoch [6/10], Batch [280/374], Loss: 0.3192\n",
      "Epoch [6/10], Batch [290/374], Loss: 0.2875\n",
      "Epoch [6/10], Batch [300/374], Loss: 0.4117\n",
      "Epoch [6/10], Batch [310/374], Loss: 0.3827\n",
      "Epoch [6/10], Batch [320/374], Loss: 0.4529\n",
      "Epoch [6/10], Batch [330/374], Loss: 0.5451\n",
      "Epoch [6/10], Batch [340/374], Loss: 0.3297\n",
      "Epoch [6/10], Batch [350/374], Loss: 0.3988\n",
      "Epoch [6/10], Batch [360/374], Loss: 0.5905\n",
      "Epoch [6/10], Batch [370/374], Loss: 0.5323\n",
      "Epoch [6/10] Train Loss: 0.4281, Train Accuracy: 79.38%\n",
      "Epoch [6/10] Val Loss: 0.3608, Val Accuracy: 84.66%\n",
      "New best model at epoch 6 with val accuracy: 84.66%\n",
      "Epoch [7/10], Batch [0/374], Loss: 0.3397\n",
      "Epoch [7/10], Batch [10/374], Loss: 0.4027\n",
      "Epoch [7/10], Batch [20/374], Loss: 0.3517\n",
      "Epoch [7/10], Batch [30/374], Loss: 0.3876\n",
      "Epoch [7/10], Batch [40/374], Loss: 0.5368\n",
      "Epoch [7/10], Batch [50/374], Loss: 0.4488\n",
      "Epoch [7/10], Batch [60/374], Loss: 0.5094\n",
      "Epoch [7/10], Batch [70/374], Loss: 0.3149\n",
      "Epoch [7/10], Batch [80/374], Loss: 0.5167\n",
      "Epoch [7/10], Batch [90/374], Loss: 0.2522\n",
      "Epoch [7/10], Batch [100/374], Loss: 0.3660\n",
      "Epoch [7/10], Batch [110/374], Loss: 0.4359\n",
      "Epoch [7/10], Batch [120/374], Loss: 0.4791\n",
      "Epoch [7/10], Batch [130/374], Loss: 0.3471\n",
      "Epoch [7/10], Batch [140/374], Loss: 0.3572\n",
      "Epoch [7/10], Batch [150/374], Loss: 0.2264\n",
      "Epoch [7/10], Batch [160/374], Loss: 0.3636\n",
      "Epoch [7/10], Batch [170/374], Loss: 0.2473\n",
      "Epoch [7/10], Batch [180/374], Loss: 0.3720\n",
      "Epoch [7/10], Batch [190/374], Loss: 0.3835\n",
      "Epoch [7/10], Batch [200/374], Loss: 0.2783\n",
      "Epoch [7/10], Batch [210/374], Loss: 0.2173\n",
      "Epoch [7/10], Batch [220/374], Loss: 0.4224\n",
      "Epoch [7/10], Batch [230/374], Loss: 0.4233\n",
      "Epoch [7/10], Batch [240/374], Loss: 0.4732\n",
      "Epoch [7/10], Batch [250/374], Loss: 0.2948\n",
      "Epoch [7/10], Batch [260/374], Loss: 0.3206\n",
      "Epoch [7/10], Batch [270/374], Loss: 0.4418\n",
      "Epoch [7/10], Batch [280/374], Loss: 0.4062\n",
      "Epoch [7/10], Batch [290/374], Loss: 0.2320\n",
      "Epoch [7/10], Batch [300/374], Loss: 0.4773\n",
      "Epoch [7/10], Batch [310/374], Loss: 0.3223\n",
      "Epoch [7/10], Batch [320/374], Loss: 0.5181\n",
      "Epoch [7/10], Batch [330/374], Loss: 0.2986\n",
      "Epoch [7/10], Batch [340/374], Loss: 0.4275\n",
      "Epoch [7/10], Batch [350/374], Loss: 0.4814\n",
      "Epoch [7/10], Batch [360/374], Loss: 0.3312\n",
      "Epoch [7/10], Batch [370/374], Loss: 0.4834\n",
      "Epoch [7/10] Train Loss: 0.3952, Train Accuracy: 81.62%\n",
      "Epoch [7/10] Val Loss: 0.3348, Val Accuracy: 85.35%\n",
      "New best model at epoch 7 with val accuracy: 85.35%\n",
      "Epoch [8/10], Batch [0/374], Loss: 0.5670\n",
      "Epoch [8/10], Batch [10/374], Loss: 0.3283\n",
      "Epoch [8/10], Batch [20/374], Loss: 0.4781\n",
      "Epoch [8/10], Batch [30/374], Loss: 0.2401\n",
      "Epoch [8/10], Batch [40/374], Loss: 0.4073\n",
      "Epoch [8/10], Batch [50/374], Loss: 0.3076\n",
      "Epoch [8/10], Batch [60/374], Loss: 0.3091\n",
      "Epoch [8/10], Batch [70/374], Loss: 0.4039\n",
      "Epoch [8/10], Batch [80/374], Loss: 0.2131\n",
      "Epoch [8/10], Batch [90/374], Loss: 0.2918\n",
      "Epoch [8/10], Batch [100/374], Loss: 0.4926\n",
      "Epoch [8/10], Batch [110/374], Loss: 0.2930\n",
      "Epoch [8/10], Batch [120/374], Loss: 0.2969\n",
      "Epoch [8/10], Batch [130/374], Loss: 0.2911\n",
      "Epoch [8/10], Batch [140/374], Loss: 0.4081\n",
      "Epoch [8/10], Batch [150/374], Loss: 0.5930\n",
      "Epoch [8/10], Batch [160/374], Loss: 0.5947\n",
      "Epoch [8/10], Batch [170/374], Loss: 0.3019\n",
      "Epoch [8/10], Batch [180/374], Loss: 0.5957\n",
      "Epoch [8/10], Batch [190/374], Loss: 0.4358\n",
      "Epoch [8/10], Batch [200/374], Loss: 0.4193\n",
      "Epoch [8/10], Batch [210/374], Loss: 0.4232\n",
      "Epoch [8/10], Batch [220/374], Loss: 0.2879\n",
      "Epoch [8/10], Batch [230/374], Loss: 0.3424\n",
      "Epoch [8/10], Batch [240/374], Loss: 0.2093\n",
      "Epoch [8/10], Batch [250/374], Loss: 0.2779\n",
      "Epoch [8/10], Batch [260/374], Loss: 0.3647\n",
      "Epoch [8/10], Batch [270/374], Loss: 0.3277\n",
      "Epoch [8/10], Batch [280/374], Loss: 0.3760\n",
      "Epoch [8/10], Batch [290/374], Loss: 0.4944\n",
      "Epoch [8/10], Batch [300/374], Loss: 0.3796\n",
      "Epoch [8/10], Batch [310/374], Loss: 0.2583\n",
      "Epoch [8/10], Batch [320/374], Loss: 0.5232\n",
      "Epoch [8/10], Batch [330/374], Loss: 0.3272\n",
      "Epoch [8/10], Batch [340/374], Loss: 0.3507\n",
      "Epoch [8/10], Batch [350/374], Loss: 0.4452\n",
      "Epoch [8/10], Batch [360/374], Loss: 0.4626\n",
      "Epoch [8/10], Batch [370/374], Loss: 0.3449\n",
      "Epoch [8/10] Train Loss: 0.3720, Train Accuracy: 82.94%\n",
      "Epoch [8/10] Val Loss: 0.3107, Val Accuracy: 86.43%\n",
      "New best model at epoch 8 with val accuracy: 86.43%\n",
      "Epoch [9/10], Batch [0/374], Loss: 0.4185\n",
      "Epoch [9/10], Batch [10/374], Loss: 0.6680\n",
      "Epoch [9/10], Batch [20/374], Loss: 0.4850\n",
      "Epoch [9/10], Batch [30/374], Loss: 0.3677\n",
      "Epoch [9/10], Batch [40/374], Loss: 0.3278\n",
      "Epoch [9/10], Batch [50/374], Loss: 0.5868\n",
      "Epoch [9/10], Batch [60/374], Loss: 0.4591\n",
      "Epoch [9/10], Batch [70/374], Loss: 0.3537\n",
      "Epoch [9/10], Batch [80/374], Loss: 0.3872\n",
      "Epoch [9/10], Batch [90/374], Loss: 0.4318\n",
      "Epoch [9/10], Batch [100/374], Loss: 0.3200\n",
      "Epoch [9/10], Batch [110/374], Loss: 0.2968\n",
      "Epoch [9/10], Batch [120/374], Loss: 0.2471\n",
      "Epoch [9/10], Batch [130/374], Loss: 0.3079\n",
      "Epoch [9/10], Batch [140/374], Loss: 0.3814\n",
      "Epoch [9/10], Batch [150/374], Loss: 0.3004\n",
      "Epoch [9/10], Batch [160/374], Loss: 0.3219\n",
      "Epoch [9/10], Batch [170/374], Loss: 0.2627\n",
      "Epoch [9/10], Batch [180/374], Loss: 0.3506\n",
      "Epoch [9/10], Batch [190/374], Loss: 0.3968\n",
      "Epoch [9/10], Batch [200/374], Loss: 0.2968\n",
      "Epoch [9/10], Batch [210/374], Loss: 0.3474\n",
      "Epoch [9/10], Batch [220/374], Loss: 0.3202\n",
      "Epoch [9/10], Batch [230/374], Loss: 0.2274\n",
      "Epoch [9/10], Batch [240/374], Loss: 0.3748\n",
      "Epoch [9/10], Batch [250/374], Loss: 0.3112\n",
      "Epoch [9/10], Batch [260/374], Loss: 0.2684\n",
      "Epoch [9/10], Batch [270/374], Loss: 0.3044\n",
      "Epoch [9/10], Batch [280/374], Loss: 0.4172\n",
      "Epoch [9/10], Batch [290/374], Loss: 0.3592\n",
      "Epoch [9/10], Batch [300/374], Loss: 0.3792\n",
      "Epoch [9/10], Batch [310/374], Loss: 0.2860\n",
      "Epoch [9/10], Batch [320/374], Loss: 0.4254\n",
      "Epoch [9/10], Batch [330/374], Loss: 0.3746\n",
      "Epoch [9/10], Batch [340/374], Loss: 0.2469\n",
      "Epoch [9/10], Batch [350/374], Loss: 0.5396\n",
      "Epoch [9/10], Batch [360/374], Loss: 0.2313\n",
      "Epoch [9/10], Batch [370/374], Loss: 0.2111\n",
      "Epoch [9/10] Train Loss: 0.3569, Train Accuracy: 83.27%\n",
      "Epoch [9/10] Val Loss: 0.2956, Val Accuracy: 87.16%\n",
      "New best model at epoch 9 with val accuracy: 87.16%\n",
      "Epoch [10/10], Batch [0/374], Loss: 0.3134\n",
      "Epoch [10/10], Batch [10/374], Loss: 0.3639\n",
      "Epoch [10/10], Batch [20/374], Loss: 0.5344\n",
      "Epoch [10/10], Batch [30/374], Loss: 0.5612\n",
      "Epoch [10/10], Batch [40/374], Loss: 0.2412\n",
      "Epoch [10/10], Batch [50/374], Loss: 0.3118\n",
      "Epoch [10/10], Batch [60/374], Loss: 0.2344\n",
      "Epoch [10/10], Batch [70/374], Loss: 0.3333\n",
      "Epoch [10/10], Batch [80/374], Loss: 0.3303\n",
      "Epoch [10/10], Batch [90/374], Loss: 0.2477\n",
      "Epoch [10/10], Batch [100/374], Loss: 0.4066\n",
      "Epoch [10/10], Batch [110/374], Loss: 0.2286\n",
      "Epoch [10/10], Batch [120/374], Loss: 0.4327\n",
      "Epoch [10/10], Batch [130/374], Loss: 0.3033\n",
      "Epoch [10/10], Batch [140/374], Loss: 0.2839\n",
      "Epoch [10/10], Batch [150/374], Loss: 0.2823\n",
      "Epoch [10/10], Batch [160/374], Loss: 0.2635\n",
      "Epoch [10/10], Batch [170/374], Loss: 0.3273\n",
      "Epoch [10/10], Batch [180/374], Loss: 0.5105\n",
      "Epoch [10/10], Batch [190/374], Loss: 0.3845\n",
      "Epoch [10/10], Batch [200/374], Loss: 0.3117\n",
      "Epoch [10/10], Batch [210/374], Loss: 0.3006\n",
      "Epoch [10/10], Batch [220/374], Loss: 0.3144\n",
      "Epoch [10/10], Batch [230/374], Loss: 0.2168\n",
      "Epoch [10/10], Batch [240/374], Loss: 0.3201\n",
      "Epoch [10/10], Batch [250/374], Loss: 0.4179\n",
      "Epoch [10/10], Batch [260/374], Loss: 0.3711\n",
      "Epoch [10/10], Batch [270/374], Loss: 0.2765\n",
      "Epoch [10/10], Batch [280/374], Loss: 0.5275\n",
      "Epoch [10/10], Batch [290/374], Loss: 0.2803\n",
      "Epoch [10/10], Batch [300/374], Loss: 0.3994\n",
      "Epoch [10/10], Batch [310/374], Loss: 0.4001\n",
      "Epoch [10/10], Batch [320/374], Loss: 0.2328\n",
      "Epoch [10/10], Batch [330/374], Loss: 0.5603\n",
      "Epoch [10/10], Batch [340/374], Loss: 0.2851\n",
      "Epoch [10/10], Batch [350/374], Loss: 0.2912\n",
      "Epoch [10/10], Batch [360/374], Loss: 0.2942\n",
      "Epoch [10/10], Batch [370/374], Loss: 0.3784\n",
      "Epoch [10/10] Train Loss: 0.3490, Train Accuracy: 84.21%\n",
      "Epoch [10/10] Val Loss: 0.2880, Val Accuracy: 87.33%\n",
      "New best model at epoch 10 with val accuracy: 87.33%\n",
      "Test Loss: 0.2969, Test Accuracy: 88.38%\n",
      "\n",
      "===== Final Performance Results =====\n",
      "{\n",
      "    \"world_size\": 2,\n",
      "    \"train_time\": 6836.623931646347,\n",
      "    \"avg_epoch_time\": 683.6623931646348,\n",
      "    \"val_accuracy\": 87.33472512178149,\n",
      "    \"test_accuracy\": 88.3785664578984,\n",
      "    \"test_loss\": 0.2969118374955264,\n",
      "    \"total_time\": 6836.623931646347,\n",
      "    \"train_losses\": [\n",
      "        0.6417003340357587,\n",
      "        0.6192422042392629,\n",
      "        0.5815311400039753,\n",
      "        0.5375208516721097,\n",
      "        0.46898650345752224,\n",
      "        0.4280572091224194,\n",
      "        0.3952170594166149,\n",
      "        0.3719532561432877,\n",
      "        0.3568889751191099,\n",
      "        0.3489892672697803\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        61.235249811699724,\n",
      "        65.00125533517449,\n",
      "        67.67930370742322,\n",
      "        72.0646079169805,\n",
      "        77.37049125449829,\n",
      "        79.37902753368483,\n",
      "        81.62189304544313,\n",
      "        82.94417942924095,\n",
      "        83.27056657460875,\n",
      "        84.2078835048958\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.6419250374464169,\n",
      "        0.597217460348943,\n",
      "        0.509419582499011,\n",
      "        0.5032340075377383,\n",
      "        0.4146047094752578,\n",
      "        0.36084455691366124,\n",
      "        0.3347709339239071,\n",
      "        0.3106860708634556,\n",
      "        0.2956236686321622,\n",
      "        0.2879638620441625\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        67.3625608907446,\n",
      "        73.86917188587334,\n",
      "        76.06123869171886,\n",
      "        75.81767571329158,\n",
      "        81.4892136395268,\n",
      "        84.65553235908142,\n",
      "        85.35142658315937,\n",
      "        86.43006263048017,\n",
      "        87.160751565762,\n",
      "        87.33472512178149\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "Model saved to models/training_using_cpus_2_best_model.pt\n",
      "Loss plot saved as plots/training_using_cpus_2_loss.png\n",
      "Accuracy plot saved as plots/training_using_cpus_2_accuracy.png\n",
      "Runtime parameters saved as metrics/training_using_cpus_2_params.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py --world_size 4 --num_threads 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b652019-0f26-4b52-ad13-ab1a76d6181a",
   "metadata": {},
   "source": [
    "### Workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a50e2-ca83-4214-9ab0-ae89833e78e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-03 20:51:38,503 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-03 20:51:38,503 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-03 20:51:38,503 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-03 20:51:38,503 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-03 20:54:48,798 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-03 20:54:48,798 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-03 20:54:48,798 - INFO - [Rank 3] Data loaded.\n",
      "2025-04-03 20:54:48,800 - INFO - [Rank 2] Data loaded.\n",
      "2025-04-03 20:54:49,808 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-03 20:54:49,808 - INFO - Total layers: 129\n",
      "2025-04-03 20:54:49,808 - INFO - Total parameters: 22,494,274\n",
      "2025-04-03 20:54:55,162 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6736\n",
      "2025-04-03 20:55:28,026 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.7043\n",
      "2025-04-03 20:55:56,812 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.6356\n",
      "2025-04-03 20:56:25,496 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.6111\n",
      "2025-04-03 20:56:54,061 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.5844\n",
      "2025-04-03 20:57:22,175 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6359\n",
      "2025-04-03 20:57:50,265 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.5580\n",
      "2025-04-03 20:58:18,914 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.6686\n",
      "2025-04-03 20:58:47,648 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.5879\n",
      "2025-04-03 20:59:16,182 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.6818\n",
      "2025-04-03 20:59:44,886 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.7504\n",
      "2025-04-03 21:00:13,238 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.5970\n",
      "2025-04-03 21:00:41,788 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6440\n",
      "2025-04-03 21:01:10,512 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.6292\n",
      "2025-04-03 21:01:39,573 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5481\n",
      "2025-04-03 21:02:08,888 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.5519\n",
      "2025-04-03 21:02:37,361 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.6144\n",
      "2025-04-03 21:03:06,288 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5410\n",
      "2025-04-03 21:03:35,000 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.6312\n",
      "2025-04-03 21:03:50,805 - INFO - Epoch [1/10] Train Loss: 0.6375, Train Accuracy: 61.94%\n",
      "2025-04-03 21:04:30,074 - INFO - Epoch [1/10] Val Loss: 0.6582, Val Accuracy: 67.15%\n",
      "2025-04-03 21:04:30,077 - INFO - New best model at epoch 1 with val accuracy: 67.15%\n",
      "2025-04-03 21:04:35,424 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.5884\n",
      "2025-04-03 21:05:05,775 - INFO - Epoch [2/10], Batch [10/187], Loss: 0.5790\n",
      "2025-04-03 21:05:34,638 - INFO - Epoch [2/10], Batch [20/187], Loss: 0.5585\n",
      "2025-04-03 21:06:04,313 - INFO - Epoch [2/10], Batch [30/187], Loss: 1.0120\n",
      "2025-04-03 21:06:33,260 - INFO - Epoch [2/10], Batch [40/187], Loss: 0.5679\n",
      "2025-04-03 21:07:02,348 - INFO - Epoch [2/10], Batch [50/187], Loss: 0.5413\n",
      "2025-04-03 21:07:31,455 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.5294\n",
      "2025-04-03 21:08:01,312 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.6159\n",
      "2025-04-03 21:08:30,433 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.6088\n",
      "2025-04-03 21:08:59,943 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.7200\n",
      "2025-04-03 21:09:29,488 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.6765\n",
      "2025-04-03 21:09:58,871 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.6110\n",
      "2025-04-03 21:10:28,657 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.6353\n",
      "2025-04-03 21:10:58,461 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.7017\n",
      "2025-04-03 21:11:27,754 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.5876\n",
      "2025-04-03 21:11:56,627 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.5929\n",
      "2025-04-03 21:12:25,956 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.5768\n",
      "2025-04-03 21:12:55,109 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.5223\n",
      "2025-04-03 21:13:24,676 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.6066\n",
      "2025-04-03 21:13:40,755 - INFO - Epoch [2/10] Train Loss: 0.6046, Train Accuracy: 66.13%\n",
      "2025-04-03 21:14:16,937 - INFO - Epoch [2/10] Val Loss: 0.5419, Val Accuracy: 75.16%\n",
      "2025-04-03 21:14:16,939 - INFO - New best model at epoch 2 with val accuracy: 75.16%\n",
      "2025-04-03 21:14:19,794 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.6249\n",
      "2025-04-03 21:14:49,350 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.5250\n",
      "2025-04-03 21:15:18,064 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.4195\n",
      "2025-04-03 21:15:47,010 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.5790\n",
      "2025-04-03 21:16:15,340 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.6630\n",
      "2025-04-03 21:16:43,764 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.6221\n",
      "2025-04-03 21:17:11,924 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.6240\n",
      "2025-04-03 21:17:39,710 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.5050\n",
      "2025-04-03 21:18:07,717 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.5824\n",
      "2025-04-03 21:18:35,580 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.4607\n",
      "2025-04-03 21:19:03,717 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.5076\n",
      "2025-04-03 21:19:31,573 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.5965\n",
      "2025-04-03 21:20:00,133 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.4066\n",
      "2025-04-03 21:20:27,991 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5281\n",
      "2025-04-03 21:20:56,372 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.4873\n",
      "2025-04-03 21:21:24,715 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.4651\n",
      "2025-04-03 21:21:53,240 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.4090\n",
      "2025-04-03 21:22:21,373 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.5577\n",
      "2025-04-03 21:22:49,151 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.4983\n",
      "2025-04-03 21:23:04,532 - INFO - Epoch [3/10] Train Loss: 0.5309, Train Accuracy: 71.62%\n",
      "2025-04-03 21:23:39,438 - INFO - Epoch [3/10] Val Loss: 0.5029, Val Accuracy: 77.87%\n",
      "2025-04-03 21:23:39,441 - INFO - New best model at epoch 3 with val accuracy: 77.87%\n",
      "2025-04-03 21:23:42,813 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.7157\n",
      "2025-04-03 21:24:10,719 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.3522\n",
      "2025-04-03 21:24:38,087 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.6002\n",
      "2025-04-03 21:25:05,426 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.4592\n",
      "2025-04-03 21:25:32,304 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.5016\n",
      "2025-04-03 21:25:58,861 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.5391\n",
      "2025-04-03 21:26:26,058 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.5754\n",
      "2025-04-03 21:26:53,202 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.3688\n",
      "2025-04-03 21:27:20,175 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.3780\n",
      "2025-04-03 21:27:47,505 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.5677\n",
      "2025-04-03 21:28:14,756 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4514\n",
      "2025-04-03 21:28:42,865 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.4888\n",
      "2025-04-03 21:29:11,006 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.5367\n",
      "2025-04-03 21:29:38,408 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.4589\n",
      "2025-04-03 21:30:05,672 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.3794\n",
      "2025-04-03 21:30:33,189 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.5092\n",
      "2025-04-03 21:31:00,805 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.3992\n",
      "2025-04-03 21:31:28,212 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.4825\n",
      "2025-04-03 21:31:56,243 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.4203\n",
      "2025-04-03 21:32:11,724 - INFO - Epoch [4/10] Train Loss: 0.4705, Train Accuracy: 76.99%\n",
      "2025-04-03 21:32:45,908 - INFO - Epoch [4/10] Val Loss: 0.3955, Val Accuracy: 82.19%\n",
      "2025-04-03 21:32:45,911 - INFO - New best model at epoch 4 with val accuracy: 82.19%\n",
      "2025-04-03 21:32:49,424 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.4702\n",
      "2025-04-03 21:33:17,436 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.4286\n",
      "2025-04-03 21:33:44,747 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.4381\n",
      "2025-04-03 21:34:12,174 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.6043\n",
      "2025-04-03 21:34:39,203 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.4275\n",
      "2025-04-03 21:35:06,820 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.3619\n",
      "2025-04-03 21:35:34,536 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.4451\n",
      "2025-04-03 21:36:02,018 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.2825\n",
      "2025-04-03 21:36:29,662 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.3775\n",
      "2025-04-03 21:36:57,531 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.5256\n",
      "2025-04-03 21:37:25,051 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.4655\n",
      "2025-04-03 21:37:52,341 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3058\n",
      "2025-04-03 21:38:20,973 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.3703\n",
      "2025-04-03 21:38:49,099 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.3400\n",
      "2025-04-03 21:39:17,012 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.3849\n",
      "2025-04-03 21:39:44,689 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.5309\n",
      "2025-04-03 21:40:12,273 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.3185\n",
      "2025-04-03 21:40:39,285 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.3253\n",
      "2025-04-03 21:41:06,385 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.6018\n",
      "2025-04-03 21:41:21,515 - INFO - Epoch [5/10] Train Loss: 0.4211, Train Accuracy: 80.35%\n",
      "2025-04-03 21:41:55,825 - INFO - Epoch [5/10] Val Loss: 0.3742, Val Accuracy: 83.92%\n",
      "2025-04-03 21:41:55,828 - INFO - New best model at epoch 5 with val accuracy: 83.92%\n",
      "2025-04-03 21:41:59,943 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.4734\n",
      "2025-04-03 21:42:28,322 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.4979\n",
      "2025-04-03 21:42:55,513 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.5358\n",
      "2025-04-03 21:43:22,590 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.3599\n",
      "2025-04-03 21:43:49,455 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2671\n",
      "2025-04-03 21:44:16,607 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.5522\n",
      "2025-04-03 21:44:43,629 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.4413\n",
      "2025-04-03 21:45:10,836 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.3783\n",
      "2025-04-03 21:45:38,262 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.3161\n",
      "2025-04-03 21:46:05,526 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.3743\n",
      "2025-04-03 21:46:32,723 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.4543\n",
      "2025-04-03 21:47:00,312 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2773\n",
      "2025-04-03 21:47:27,596 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.8882\n",
      "2025-04-03 21:47:54,656 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.3225\n",
      "2025-04-03 21:48:21,804 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.2760\n",
      "2025-04-03 21:48:49,898 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.4101\n",
      "2025-04-03 21:49:17,023 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.3635\n",
      "2025-04-03 21:49:43,850 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.5201\n",
      "2025-04-03 21:50:10,886 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.2414\n",
      "2025-04-03 21:50:25,963 - INFO - Epoch [6/10] Train Loss: 0.3968, Train Accuracy: 81.61%\n",
      "2025-04-03 21:51:00,680 - INFO - Epoch [6/10] Val Loss: 0.3371, Val Accuracy: 85.87%\n",
      "2025-04-03 21:51:00,683 - INFO - New best model at epoch 6 with val accuracy: 85.87%\n",
      "2025-04-03 21:51:03,455 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.2787\n",
      "2025-04-03 21:51:30,758 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.4604\n",
      "2025-04-03 21:51:57,897 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.4574\n",
      "2025-04-03 21:52:25,008 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.2833\n",
      "2025-04-03 21:52:52,264 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.4020\n",
      "2025-04-03 21:53:19,378 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.2876\n",
      "2025-04-03 21:53:46,466 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.4797\n",
      "2025-04-03 21:54:13,479 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.4188\n",
      "2025-04-03 21:54:40,625 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.3436\n",
      "2025-04-03 21:55:07,963 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.2492\n",
      "2025-04-03 21:55:35,078 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.3933\n",
      "2025-04-03 21:56:02,328 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.4020\n",
      "2025-04-03 21:56:29,816 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.4695\n",
      "2025-04-03 21:56:57,113 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.2318\n",
      "2025-04-03 21:57:23,920 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.5094\n",
      "2025-04-03 21:57:51,739 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.4365\n",
      "2025-04-03 21:58:18,647 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.3344\n",
      "2025-04-03 21:58:45,833 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.5748\n",
      "2025-04-03 21:59:13,014 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2538\n",
      "2025-04-03 21:59:28,106 - INFO - Epoch [7/10] Train Loss: 0.3782, Train Accuracy: 82.04%\n",
      "2025-04-03 22:00:03,896 - INFO - Epoch [7/10] Val Loss: 0.3140, Val Accuracy: 87.68%\n",
      "2025-04-03 22:00:03,899 - INFO - New best model at epoch 7 with val accuracy: 87.68%\n",
      "2025-04-03 22:00:06,717 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.3972\n",
      "2025-04-03 22:00:34,025 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.3850\n",
      "2025-04-03 22:01:00,910 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.3607\n",
      "2025-04-03 22:01:28,197 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.3907\n",
      "2025-04-03 22:01:55,559 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.2229\n",
      "2025-04-03 22:02:22,831 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2787\n",
      "2025-04-03 22:02:49,802 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.1630\n",
      "2025-04-03 22:03:17,058 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.4254\n",
      "2025-04-03 22:03:44,291 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.4966\n",
      "2025-04-03 22:04:11,683 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.4103\n",
      "2025-04-03 22:04:38,867 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.2912\n",
      "2025-04-03 22:05:06,142 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.2786\n",
      "2025-04-03 22:05:34,210 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.2393\n",
      "2025-04-03 22:06:01,914 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.2486\n",
      "2025-04-03 22:06:29,345 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.3124\n",
      "2025-04-03 22:06:58,118 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.3382\n",
      "2025-04-03 22:07:25,364 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.3153\n",
      "2025-04-03 22:07:52,631 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3359\n",
      "2025-04-03 22:08:19,902 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.3392\n",
      "2025-04-03 22:08:34,993 - INFO - Epoch [8/10] Train Loss: 0.3491, Train Accuracy: 84.38%\n",
      "2025-04-03 22:09:09,877 - INFO - Epoch [8/10] Val Loss: 0.2954, Val Accuracy: 87.96%\n",
      "2025-04-03 22:09:09,879 - INFO - New best model at epoch 8 with val accuracy: 87.96%\n",
      "2025-04-03 22:09:12,671 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.3249\n",
      "2025-04-03 22:09:39,546 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.4811\n",
      "2025-04-03 22:10:06,752 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.2319\n",
      "2025-04-03 22:10:33,801 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.4042\n",
      "2025-04-03 22:11:01,130 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.3615\n",
      "2025-04-03 22:11:29,509 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2481\n",
      "2025-04-03 22:11:56,846 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.1785\n",
      "2025-04-03 22:12:23,832 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2811\n",
      "2025-04-03 22:12:51,215 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.4531\n",
      "2025-04-03 22:13:18,887 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.3530\n",
      "2025-04-03 22:13:46,224 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4841\n",
      "2025-04-03 22:14:13,558 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.1894\n",
      "2025-04-03 22:14:41,079 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.2575\n",
      "2025-04-03 22:15:08,271 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2365\n",
      "2025-04-03 22:15:35,582 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.4627\n",
      "2025-04-03 22:16:02,589 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.3287\n",
      "2025-04-03 22:16:30,231 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.3204\n",
      "2025-04-03 22:16:57,692 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.3286\n",
      "2025-04-03 22:17:24,904 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.3250\n",
      "2025-04-03 22:17:39,982 - INFO - Epoch [9/10] Train Loss: 0.3443, Train Accuracy: 84.20%\n",
      "2025-04-03 22:18:14,286 - INFO - Epoch [9/10] Val Loss: 0.2724, Val Accuracy: 88.80%\n",
      "2025-04-03 22:18:14,288 - INFO - New best model at epoch 9 with val accuracy: 88.80%\n",
      "2025-04-03 22:18:17,533 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.4207\n",
      "2025-04-03 22:18:44,474 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.5387\n",
      "2025-04-03 22:19:11,863 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.3422\n",
      "2025-04-03 22:19:38,834 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.3005\n",
      "2025-04-03 22:20:06,142 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.5167\n",
      "2025-04-03 22:20:33,504 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2405\n",
      "2025-04-03 22:21:02,132 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2308\n",
      "2025-04-03 22:21:29,349 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.3871\n",
      "2025-04-03 22:21:56,781 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.4495\n",
      "2025-04-03 22:22:23,863 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.4913\n",
      "2025-04-03 22:22:51,089 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.1786\n",
      "2025-04-03 22:23:18,345 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.2871\n",
      "2025-04-03 22:23:47,328 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.2286\n",
      "2025-04-03 22:24:14,555 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.2877\n",
      "2025-04-03 22:24:43,142 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.5631\n",
      "2025-04-03 22:25:10,956 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.4254\n",
      "2025-04-03 22:25:38,294 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.2093\n",
      "2025-04-03 22:26:06,310 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.3817\n",
      "2025-04-03 22:26:33,602 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.2492\n",
      "2025-04-03 22:26:48,504 - INFO - Epoch [10/10] Train Loss: 0.3276, Train Accuracy: 84.23%\n",
      "2025-04-03 22:27:22,591 - INFO - Epoch [10/10] Val Loss: 0.2649, Val Accuracy: 89.42%\n",
      "2025-04-03 22:27:22,594 - INFO - New best model at epoch 10 with val accuracy: 89.42%\n",
      "2025-04-03 22:27:41,335 - INFO - Test Loss: 0.2857, Test Accuracy: 87.20%\n",
      "2025-04-03 22:27:41,335 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-03 22:27:41,339 - INFO - {\n",
      "    \"world_size\": 4,\n",
      "    \"train_time\": 5552.7846212387085,\n",
      "    \"avg_epoch_time\": 555.2784621238709,\n",
      "    \"val_accuracy\": 89.42240779401531,\n",
      "    \"test_accuracy\": 87.20445062586926,\n",
      "    \"test_loss\": 0.2857187781612465,\n",
      "    \"total_time\": 5552.7846212387085,\n",
      "    \"train_losses\": [\n",
      "        0.6374727773267355,\n",
      "        0.6045920153143017,\n",
      "        0.5309176274672712,\n",
      "        0.4704754906219419,\n",
      "        0.4211230176961572,\n",
      "        0.3967871574128522,\n",
      "        0.3782093461687096,\n",
      "        0.34914774755803113,\n",
      "        0.3443033064957942,\n",
      "        0.3276315889099153\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        61.94142259414226,\n",
      "        66.1255230125523,\n",
      "        71.61506276150628,\n",
      "        76.98744769874477,\n",
      "        80.35146443514644,\n",
      "        81.60669456066945,\n",
      "        82.0418410041841,\n",
      "        84.38493723849372,\n",
      "        84.20083682008368,\n",
      "        84.23430962343096\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.6581590212959337,\n",
      "        0.5418545465628638,\n",
      "        0.5028727820289575,\n",
      "        0.3955008366411564,\n",
      "        0.3741703352417146,\n",
      "        0.3371237175449698,\n",
      "        0.31399957090097746,\n",
      "        0.29544170834409256,\n",
      "        0.27241759040576347,\n",
      "        0.26485607382648924\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        67.15379262352123,\n",
      "        75.15657620041753,\n",
      "        77.8705636743215,\n",
      "        82.18510786360473,\n",
      "        83.92484342379959,\n",
      "        85.87334725121781,\n",
      "        87.68267223382045,\n",
      "        87.96102992345163,\n",
      "        88.79610299234517,\n",
      "        89.42240779401531\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-03 22:27:41,628 - INFO - Model saved to models/training_using_cpus_4_best_model.pt\n",
      "2025-04-03 22:27:44,904 - INFO - Loss plot saved as plots/training_using_cpus_4_loss.png\n",
      "2025-04-03 22:27:45,017 - INFO - Accuracy plot saved as plots/training_using_cpus_4_accuracy.png\n",
      "2025-04-03 22:27:45,035 - INFO - Runtime parameters saved as metrics/training_using_cpus_4_params.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py --world_size 4 --num_threads 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd786d1a-afbe-49c0-bb4c-8e9604feb9e5",
   "metadata": {},
   "source": [
    "### Workers = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ba4fcb-ea58-4ae8-ab0e-dfedd5ca84f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 11:37:44,192 - INFO - [Rank 1] Passed initial barrier\n",
      "2025-04-05 11:37:44,192 - INFO - [Rank 0] Passed initial barrier\n",
      "2025-04-05 11:37:44,192 - INFO - [Rank 2] Passed initial barrier\n",
      "2025-04-05 11:37:44,192 - INFO - [Rank 3] Passed initial barrier\n",
      "2025-04-05 11:37:44,222 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 11:37:44,222 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 11:37:44,223 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 11:37:44,223 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 11:40:36,967 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-05 11:40:36,968 - INFO - [Rank 3] Data loaded.\n",
      "2025-04-05 11:40:36,968 - INFO - [Rank 2] Data loaded.\n",
      "2025-04-05 11:40:36,967 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-05 11:40:37,729 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-05 11:40:37,730 - INFO - Total layers: 129\n",
      "2025-04-05 11:40:37,731 - INFO - Total parameters: 22,494,274\n",
      "2025-04-05 11:40:41,347 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6736\n",
      "2025-04-05 11:41:01,319 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.7005\n",
      "2025-04-05 11:41:21,203 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.6079\n",
      "2025-04-05 11:41:40,467 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.6025\n",
      "2025-04-05 11:41:59,796 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.6276\n",
      "2025-04-05 11:42:19,420 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6353\n",
      "2025-04-05 11:42:38,829 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.5886\n",
      "2025-04-05 11:42:58,033 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.6878\n",
      "2025-04-05 11:43:17,407 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.5948\n",
      "2025-04-05 11:43:36,995 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.7354\n",
      "2025-04-05 11:43:56,525 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.6904\n",
      "2025-04-05 11:44:15,916 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.6188\n",
      "2025-04-05 11:44:35,113 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6505\n",
      "2025-04-05 11:44:54,243 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.5898\n",
      "2025-04-05 11:45:13,695 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5169\n",
      "2025-04-05 11:45:33,101 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.6117\n",
      "2025-04-05 11:45:52,351 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.6701\n",
      "2025-04-05 11:46:11,515 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5238\n",
      "2025-04-05 11:46:30,827 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.6703\n",
      "2025-04-05 11:46:41,689 - INFO - Epoch [1/10] Train Loss: 0.6479, Train Accuracy: 60.32%\n",
      "2025-04-05 11:47:09,163 - INFO - Epoch [1/10] Val Loss: 0.6949, Val Accuracy: 58.94%\n",
      "2025-04-05 11:47:09,170 - INFO - New best model at epoch 1 with val accuracy: 58.94%\n",
      "2025-04-05 11:47:13,331 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.6649\n",
      "2025-04-05 11:47:38,305 - INFO - Epoch [2/10], Batch [10/187], Loss: 0.5264\n",
      "2025-04-05 11:48:03,564 - INFO - Epoch [2/10], Batch [20/187], Loss: 0.6094\n",
      "2025-04-05 11:48:27,944 - INFO - Epoch [2/10], Batch [30/187], Loss: 0.8688\n",
      "2025-04-05 11:48:52,518 - INFO - Epoch [2/10], Batch [40/187], Loss: 0.6223\n",
      "2025-04-05 11:49:16,531 - INFO - Epoch [2/10], Batch [50/187], Loss: 0.6252\n",
      "2025-04-05 11:49:41,185 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.6135\n",
      "2025-04-05 11:50:05,647 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.5057\n",
      "2025-04-05 11:50:29,834 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.6750\n",
      "2025-04-05 11:50:53,988 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.7278\n",
      "2025-04-05 11:51:18,452 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.5996\n",
      "2025-04-05 11:51:43,225 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.6514\n",
      "2025-04-05 11:52:07,547 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.6946\n",
      "2025-04-05 11:52:32,594 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.7043\n",
      "2025-04-05 11:52:57,115 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.5748\n",
      "2025-04-05 11:53:25,003 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.4560\n",
      "2025-04-05 11:53:51,838 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.6610\n",
      "2025-04-05 11:54:17,890 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.4651\n",
      "2025-04-05 11:54:42,743 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.5111\n",
      "2025-04-05 11:54:57,043 - INFO - Epoch [2/10] Train Loss: 0.6091, Train Accuracy: 66.61%\n",
      "2025-04-05 11:55:25,634 - INFO - Epoch [2/10] Val Loss: 0.5167, Val Accuracy: 77.04%\n",
      "2025-04-05 11:55:25,638 - INFO - New best model at epoch 2 with val accuracy: 77.04%\n",
      "2025-04-05 11:55:28,731 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.6582\n",
      "2025-04-05 11:55:54,311 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.5283\n",
      "2025-04-05 11:56:20,528 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.4841\n",
      "2025-04-05 11:56:45,078 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.5470\n",
      "2025-04-05 11:57:10,423 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.4893\n",
      "2025-04-05 11:57:37,840 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.6112\n",
      "2025-04-05 11:58:05,681 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.5283\n",
      "2025-04-05 11:58:31,107 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.4525\n",
      "2025-04-05 11:58:57,728 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.4934\n",
      "2025-04-05 11:59:23,675 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.4958\n",
      "2025-04-05 11:59:49,442 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.5620\n",
      "2025-04-05 12:00:14,751 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.7322\n",
      "2025-04-05 12:00:39,982 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.5042\n",
      "2025-04-05 12:01:05,425 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5418\n",
      "2025-04-05 12:01:31,650 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.4245\n",
      "2025-04-05 12:01:57,063 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.5552\n",
      "2025-04-05 12:02:22,277 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.3747\n",
      "2025-04-05 12:02:47,697 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.5642\n",
      "2025-04-05 12:03:13,307 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.5757\n",
      "2025-04-05 12:03:27,659 - INFO - Epoch [3/10] Train Loss: 0.5198, Train Accuracy: 73.19%\n",
      "2025-04-05 12:03:54,538 - INFO - Epoch [3/10] Val Loss: 0.5230, Val Accuracy: 75.78%\n",
      "2025-04-05 12:03:58,920 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.6405\n",
      "2025-04-05 12:04:24,193 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.3528\n",
      "2025-04-05 12:04:49,158 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.6574\n",
      "2025-04-05 12:05:14,719 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.4206\n",
      "2025-04-05 12:05:39,682 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.5592\n",
      "2025-04-05 12:06:05,540 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.4445\n",
      "2025-04-05 12:06:30,851 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.5800\n",
      "2025-04-05 12:06:55,997 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.4582\n",
      "2025-04-05 12:07:21,753 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.4484\n",
      "2025-04-05 12:07:46,862 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.5299\n",
      "2025-04-05 12:08:13,483 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4410\n",
      "2025-04-05 12:08:43,224 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.6430\n",
      "2025-04-05 12:09:13,559 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.5241\n",
      "2025-04-05 12:09:38,675 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.4842\n",
      "2025-04-05 12:10:04,473 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.4289\n",
      "2025-04-05 12:10:31,617 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.4157\n",
      "2025-04-05 12:10:59,441 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.3876\n",
      "2025-04-05 12:11:26,683 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.4521\n",
      "2025-04-05 12:11:54,876 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.3476\n",
      "2025-04-05 12:12:09,832 - INFO - Epoch [4/10] Train Loss: 0.4650, Train Accuracy: 77.86%\n",
      "2025-04-05 12:12:38,507 - INFO - Epoch [4/10] Val Loss: 0.4010, Val Accuracy: 82.60%\n",
      "2025-04-05 12:12:38,510 - INFO - New best model at epoch 4 with val accuracy: 82.60%\n",
      "2025-04-05 12:12:41,308 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.5139\n",
      "2025-04-05 12:13:07,038 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.4302\n",
      "2025-04-05 12:13:32,831 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.4434\n",
      "2025-04-05 12:13:58,870 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.5112\n",
      "2025-04-05 12:14:23,819 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.5218\n",
      "2025-04-05 12:14:48,330 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.2414\n",
      "2025-04-05 12:15:12,834 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.4072\n",
      "2025-04-05 12:15:37,563 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.2919\n",
      "2025-04-05 12:16:01,920 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.3813\n",
      "2025-04-05 12:16:27,418 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.4402\n",
      "2025-04-05 12:16:51,855 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.5505\n",
      "2025-04-05 12:17:17,609 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3010\n",
      "2025-04-05 12:17:43,249 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.4233\n",
      "2025-04-05 12:18:08,297 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.2769\n",
      "2025-04-05 12:18:34,971 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.3890\n",
      "2025-04-05 12:18:59,053 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.4906\n",
      "2025-04-05 12:19:24,986 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.3503\n",
      "2025-04-05 12:19:51,882 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.4495\n",
      "2025-04-05 12:20:15,453 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.5246\n",
      "2025-04-05 12:20:29,401 - INFO - Epoch [5/10] Train Loss: 0.4265, Train Accuracy: 79.88%\n",
      "2025-04-05 12:20:56,037 - INFO - Epoch [5/10] Val Loss: 0.8338, Val Accuracy: 65.48%\n",
      "2025-04-05 12:20:59,480 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.5858\n",
      "2025-04-05 12:21:23,700 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.4259\n",
      "2025-04-05 12:21:47,760 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.6521\n",
      "2025-04-05 12:22:13,011 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.3115\n",
      "2025-04-05 12:22:37,427 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2503\n",
      "2025-04-05 12:23:02,460 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.4491\n",
      "2025-04-05 12:23:27,306 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.5482\n",
      "2025-04-05 12:23:51,763 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.4674\n",
      "2025-04-05 12:24:16,157 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.3676\n",
      "2025-04-05 12:24:40,452 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.3742\n",
      "2025-04-05 12:25:05,231 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.3903\n",
      "2025-04-05 12:25:29,895 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2666\n",
      "2025-04-05 12:25:54,446 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.6941\n",
      "2025-04-05 12:26:19,038 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.3205\n",
      "2025-04-05 12:26:43,970 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.2870\n",
      "2025-04-05 12:27:08,554 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.3668\n",
      "2025-04-05 12:27:35,001 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.4614\n",
      "2025-04-05 12:28:01,275 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.5925\n",
      "2025-04-05 12:28:25,898 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.3344\n",
      "2025-04-05 12:28:40,114 - INFO - Epoch [6/10] Train Loss: 0.4029, Train Accuracy: 81.24%\n",
      "2025-04-05 12:29:07,734 - INFO - Epoch [6/10] Val Loss: 0.4079, Val Accuracy: 81.42%\n",
      "2025-04-05 12:29:11,410 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.2680\n",
      "2025-04-05 12:29:36,531 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.4085\n",
      "2025-04-05 12:30:00,823 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.4295\n",
      "2025-04-05 12:30:25,033 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.3492\n",
      "2025-04-05 12:30:48,961 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.3989\n",
      "2025-04-05 12:31:13,146 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.4087\n",
      "2025-04-05 12:31:38,036 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.4264\n",
      "2025-04-05 12:32:03,771 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.4873\n",
      "2025-04-05 12:32:31,023 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.3125\n",
      "2025-04-05 12:32:56,818 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.2927\n",
      "2025-04-05 12:33:23,250 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.3269\n",
      "2025-04-05 12:33:47,999 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.4528\n",
      "2025-04-05 12:34:13,314 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.6174\n",
      "2025-04-05 12:34:36,988 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.3346\n",
      "2025-04-05 12:35:01,123 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.5474\n",
      "2025-04-05 12:35:24,718 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.4309\n",
      "2025-04-05 12:35:49,104 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.4347\n",
      "2025-04-05 12:36:13,717 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.4654\n",
      "2025-04-05 12:36:38,617 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2974\n",
      "2025-04-05 12:36:53,400 - INFO - Epoch [7/10] Train Loss: 0.3871, Train Accuracy: 81.41%\n",
      "2025-04-05 12:37:21,115 - INFO - Epoch [7/10] Val Loss: 0.3147, Val Accuracy: 87.33%\n",
      "2025-04-05 12:37:21,118 - INFO - New best model at epoch 7 with val accuracy: 87.33%\n",
      "2025-04-05 12:37:23,453 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.6269\n",
      "2025-04-05 12:37:47,390 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.3558\n",
      "2025-04-05 12:38:11,315 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.3737\n",
      "2025-04-05 12:38:35,142 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.3001\n",
      "2025-04-05 12:38:59,322 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.2516\n",
      "2025-04-05 12:39:23,749 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2341\n",
      "2025-04-05 12:39:49,486 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.1978\n",
      "2025-04-05 12:40:14,833 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.4264\n",
      "2025-04-05 12:40:38,627 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.4492\n",
      "2025-04-05 12:41:02,423 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.4429\n",
      "2025-04-05 12:41:26,374 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.4406\n",
      "2025-04-05 12:41:50,547 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.2864\n",
      "2025-04-05 12:42:16,987 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.2182\n",
      "2025-04-05 12:42:44,721 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.2640\n",
      "2025-04-05 12:43:11,999 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.3368\n",
      "2025-04-05 12:43:39,729 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.3402\n",
      "2025-04-05 12:44:06,811 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.2902\n",
      "2025-04-05 12:44:30,942 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3680\n",
      "2025-04-05 12:44:55,057 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.3327\n",
      "2025-04-05 12:45:09,245 - INFO - Epoch [8/10] Train Loss: 0.3582, Train Accuracy: 83.92%\n",
      "2025-04-05 12:45:35,297 - INFO - Epoch [8/10] Val Loss: 0.3659, Val Accuracy: 84.55%\n",
      "2025-04-05 12:45:38,070 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.2857\n",
      "2025-04-05 12:46:02,323 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.3832\n",
      "2025-04-05 12:46:26,544 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.2816\n",
      "2025-04-05 12:46:51,918 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.3363\n",
      "2025-04-05 12:47:17,003 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.4264\n",
      "2025-04-05 12:47:41,664 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2409\n",
      "2025-04-05 12:48:06,016 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.1863\n",
      "2025-04-05 12:48:33,649 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2408\n",
      "2025-04-05 12:49:00,411 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.2905\n",
      "2025-04-05 12:49:27,453 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.3540\n",
      "2025-04-05 12:49:54,644 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4244\n",
      "2025-04-05 12:50:20,208 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.2284\n",
      "2025-04-05 12:50:45,218 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.2936\n",
      "2025-04-05 12:51:09,963 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2270\n",
      "2025-04-05 12:51:34,429 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.3952\n",
      "2025-04-05 12:51:58,010 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.2590\n",
      "2025-04-05 12:52:21,896 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.3299\n",
      "2025-04-05 12:52:45,190 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.3673\n",
      "2025-04-05 12:53:09,389 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.3383\n",
      "2025-04-05 12:53:23,001 - INFO - Epoch [9/10] Train Loss: 0.3457, Train Accuracy: 83.95%\n",
      "2025-04-05 12:53:49,178 - INFO - Epoch [9/10] Val Loss: 0.2780, Val Accuracy: 89.00%\n",
      "2025-04-05 12:53:49,182 - INFO - New best model at epoch 9 with val accuracy: 89.00%\n",
      "2025-04-05 12:53:53,376 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.4095\n",
      "2025-04-05 12:54:16,707 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.5701\n",
      "2025-04-05 12:54:39,760 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.3154\n",
      "2025-04-05 12:55:03,089 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.3200\n",
      "2025-04-05 12:55:27,203 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.4569\n",
      "2025-04-05 12:55:51,249 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2669\n",
      "2025-04-05 12:56:16,026 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2417\n",
      "2025-04-05 12:56:40,099 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.3896\n",
      "2025-04-05 12:57:06,035 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.3773\n",
      "2025-04-05 12:57:31,126 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.3967\n",
      "2025-04-05 12:57:58,602 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.1822\n",
      "2025-04-05 12:58:22,239 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.2930\n",
      "2025-04-05 12:58:45,842 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.3195\n",
      "2025-04-05 12:59:09,656 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.2853\n",
      "2025-04-05 12:59:33,793 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.5026\n",
      "2025-04-05 12:59:57,483 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.3581\n",
      "2025-04-05 13:00:21,468 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.2132\n",
      "2025-04-05 13:00:46,177 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.3637\n",
      "2025-04-05 13:01:09,864 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.2814\n",
      "2025-04-05 13:01:23,457 - INFO - Epoch [10/10] Train Loss: 0.3299, Train Accuracy: 84.52%\n",
      "2025-04-05 13:01:49,705 - INFO - Epoch [10/10] Val Loss: 0.2687, Val Accuracy: 89.42%\n",
      "2025-04-05 13:01:49,709 - INFO - New best model at epoch 10 with val accuracy: 89.42%\n",
      "2025-04-05 13:02:03,711 - INFO - Test Loss: 0.3067, Test Accuracy: 87.20%\n",
      "2025-04-05 13:02:03,712 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-05 13:02:03,716 - INFO - {\n",
      "    \"world_size\": 4,\n",
      "    \"train_time\": 4871.955017089844,\n",
      "    \"avg_epoch_time\": 487.19550170898435,\n",
      "    \"val_accuracy\": 89.42240779401531,\n",
      "    \"test_accuracy\": 87.20445062586926,\n",
      "    \"test_loss\": 0.30665782628702687,\n",
      "    \"total_time\": 4871.955017089844,\n",
      "    \"train_losses\": [\n",
      "        0.6479086761694074,\n",
      "        0.6090708466154761,\n",
      "        0.5197986322815947,\n",
      "        0.46499148074553104,\n",
      "        0.42646901890822536,\n",
      "        0.4028993652605113,\n",
      "        0.38711823966712633,\n",
      "        0.3581581878213204,\n",
      "        0.3457125015687743,\n",
      "        0.32986888491957755\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        60.31799163179916,\n",
      "        66.61087866108787,\n",
      "        73.18828451882845,\n",
      "        77.85774058577405,\n",
      "        79.88284518828452,\n",
      "        81.23849372384937,\n",
      "        81.40585774058577,\n",
      "        83.9163179916318,\n",
      "        83.94979079497908,\n",
      "        84.51882845188284\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.694937352389201,\n",
      "        0.5166993375113215,\n",
      "        0.52303525504754,\n",
      "        0.400955745554999,\n",
      "        0.8337661782610143,\n",
      "        0.40791592257007925,\n",
      "        0.31472845382796616,\n",
      "        0.3658994632872261,\n",
      "        0.2779759848457952,\n",
      "        0.2687094644614202\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        58.94224077940153,\n",
      "        77.03549060542798,\n",
      "        75.78288100208768,\n",
      "        82.6026443980515,\n",
      "        65.48364648573417,\n",
      "        81.419624217119,\n",
      "        87.33472512178149,\n",
      "        84.55114822546973,\n",
      "        89.00487125956855,\n",
      "        89.42240779401531\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-05 13:02:03,913 - INFO - Model saved to models/training_using_cpus_4_best_model.pt\n",
      "2025-04-05 13:02:07,566 - INFO - Loss plot saved as plots/training_using_cpus_4_loss.png\n",
      "2025-04-05 13:02:07,683 - INFO - Accuracy plot saved as plots/training_using_cpus_4_accuracy.png\n",
      "2025-04-05 13:02:07,693 - INFO - Runtime parameters saved as metrics/training_using_cpus_4_params.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py --world_size 4 --num_threads 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f594864-7207-45ac-84b6-a7c61daca6bf",
   "metadata": {},
   "source": [
    "### Workers = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1310069-85ed-4901-8610-b44b1bc6cbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 13:04:37,670 - INFO - [Rank 1] Passed initial barrier\n",
      "2025-04-05 13:04:37,670 - INFO - [Rank 2] Passed initial barrier\n",
      "2025-04-05 13:04:37,670 - INFO - [Rank 3] Passed initial barrier\n",
      "2025-04-05 13:04:37,671 - INFO - [Rank 0] Passed initial barrier\n",
      "2025-04-05 13:04:37,701 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 13:04:37,701 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 13:04:37,701 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 13:04:37,701 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...\n",
      "2025-04-05 13:07:15,409 - INFO - [Rank 0] Data loaded.\n",
      "2025-04-05 13:07:15,410 - INFO - [Rank 1] Data loaded.\n",
      "2025-04-05 13:07:15,410 - INFO - [Rank 2] Data loaded.\n",
      "2025-04-05 13:07:15,409 - INFO - [Rank 3] Data loaded.\n",
      "2025-04-05 13:07:15,937 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-05 13:07:15,937 - INFO - Total layers: 129\n",
      "2025-04-05 13:07:15,937 - INFO - Total parameters: 22,494,274\n",
      "2025-04-05 13:07:18,583 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6736\n",
      "2025-04-05 13:07:37,408 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.6999\n",
      "2025-04-05 13:07:56,596 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.6082\n",
      "2025-04-05 13:08:15,084 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.6426\n",
      "2025-04-05 13:08:33,504 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.6250\n",
      "2025-04-05 13:08:52,332 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6926\n",
      "2025-04-05 13:09:10,638 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.5806\n",
      "2025-04-05 13:09:29,407 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.6439\n",
      "2025-04-05 13:09:48,026 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.6302\n",
      "2025-04-05 13:10:06,637 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.7879\n",
      "2025-04-05 13:10:25,579 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.6981\n",
      "2025-04-05 13:10:44,539 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.5933\n",
      "2025-04-05 13:11:03,216 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6580\n",
      "2025-04-05 13:11:22,284 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.5929\n",
      "2025-04-05 13:11:41,028 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5200\n",
      "2025-04-05 13:12:00,474 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.6126\n",
      "2025-04-05 13:12:19,052 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.6818\n",
      "2025-04-05 13:12:37,325 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5544\n",
      "2025-04-05 13:12:56,097 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.6673\n",
      "2025-04-05 13:13:06,667 - INFO - Epoch [1/10] Train Loss: 0.6480, Train Accuracy: 60.72%\n",
      "2025-04-05 13:13:30,470 - INFO - Epoch [1/10] Val Loss: 0.6516, Val Accuracy: 71.12%\n",
      "2025-04-05 13:13:30,472 - INFO - New best model at epoch 1 with val accuracy: 71.12%\n",
      "2025-04-05 13:13:34,823 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.6042\n",
      "2025-04-05 13:15:32,962 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.5231\n",
      "2025-04-05 13:15:52,474 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.6181\n",
      "2025-04-05 13:16:12,211 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.7018\n",
      "2025-04-05 13:16:31,573 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.6552\n",
      "2025-04-05 13:16:51,153 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.6870\n",
      "2025-04-05 13:17:10,680 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.6485\n",
      "2025-04-05 13:17:30,062 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.7429\n",
      "2025-04-05 13:17:49,154 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.6781\n",
      "2025-04-05 13:18:08,719 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.6570\n",
      "2025-04-05 13:18:27,871 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.6513\n",
      "2025-04-05 13:18:47,376 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.5759\n",
      "2025-04-05 13:19:06,821 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.5777\n",
      "2025-04-05 13:19:26,901 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.5911\n",
      "2025-04-05 13:19:38,210 - INFO - Epoch [2/10] Train Loss: 0.6168, Train Accuracy: 65.66%\n",
      "2025-04-05 13:19:59,742 - INFO - Epoch [2/10] Val Loss: 0.5514, Val Accuracy: 73.56%\n",
      "2025-04-05 13:19:59,748 - INFO - New best model at epoch 2 with val accuracy: 73.56%\n",
      "2025-04-05 13:20:03,524 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.6305\n",
      "2025-04-05 13:20:23,185 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.4891\n",
      "2025-04-05 13:20:42,569 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.4230\n",
      "2025-04-05 13:21:02,200 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.6113\n",
      "2025-04-05 13:21:21,410 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.6040\n",
      "2025-04-05 13:21:40,664 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.7193\n",
      "2025-04-05 13:21:59,767 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.5959\n",
      "2025-04-05 13:22:18,744 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.5115\n",
      "2025-04-05 13:22:37,776 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.5750\n",
      "2025-04-05 13:22:57,005 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.5159\n",
      "2025-04-05 13:23:16,237 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.5478\n",
      "2025-04-05 13:23:35,635 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.6841\n",
      "2025-04-05 13:23:54,798 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.5758\n",
      "2025-04-05 13:24:13,945 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5359\n",
      "2025-04-05 13:24:33,070 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.4984\n",
      "2025-04-05 13:24:51,905 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.5148\n",
      "2025-04-05 13:25:10,928 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.3611\n",
      "2025-04-05 13:25:29,850 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.6698\n",
      "2025-04-05 13:25:49,574 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.5433\n",
      "2025-04-05 13:26:00,232 - INFO - Epoch [3/10] Train Loss: 0.5688, Train Accuracy: 68.52%\n",
      "2025-04-05 13:26:22,477 - INFO - Epoch [3/10] Val Loss: 0.5485, Val Accuracy: 72.93%\n",
      "2025-04-05 13:26:24,716 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.7428\n",
      "2025-04-05 13:26:44,062 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.4231\n",
      "2025-04-05 13:27:02,814 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.7030\n",
      "2025-04-05 13:27:22,126 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.4814\n",
      "2025-04-05 13:27:41,509 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.6609\n",
      "2025-04-05 13:28:00,910 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.5483\n",
      "2025-04-05 13:28:20,061 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.5223\n",
      "2025-04-05 13:28:38,894 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.4221\n",
      "2025-04-05 13:28:58,165 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.4792\n",
      "2025-04-05 13:29:17,384 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.5922\n",
      "2025-04-05 13:29:36,442 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4814\n",
      "2025-04-05 13:29:55,216 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.5485\n",
      "2025-04-05 13:30:14,149 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.6486\n",
      "2025-04-05 13:30:32,774 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.5250\n",
      "2025-04-05 13:30:51,176 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.4625\n",
      "2025-04-05 13:31:09,666 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.3973\n",
      "2025-04-05 13:31:28,504 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.5001\n",
      "2025-04-05 13:31:47,632 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.5781\n",
      "2025-04-05 13:32:06,041 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.4010\n",
      "2025-04-05 13:32:16,448 - INFO - Epoch [4/10] Train Loss: 0.5166, Train Accuracy: 74.06%\n",
      "2025-04-05 13:32:38,951 - INFO - Epoch [4/10] Val Loss: 0.4717, Val Accuracy: 77.59%\n",
      "2025-04-05 13:32:38,953 - INFO - New best model at epoch 4 with val accuracy: 77.59%\n",
      "2025-04-05 13:32:40,924 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.4531\n",
      "2025-04-05 13:32:59,487 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.4856\n",
      "2025-04-05 13:33:18,121 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.5225\n",
      "2025-04-05 13:33:37,572 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.6192\n",
      "2025-04-05 13:33:56,416 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.5688\n",
      "2025-04-05 13:34:15,411 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.3799\n",
      "2025-04-05 13:34:34,372 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.3915\n",
      "2025-04-05 13:34:53,051 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.3408\n",
      "2025-04-05 13:35:12,249 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.3736\n",
      "2025-04-05 13:35:31,228 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.4591\n",
      "2025-04-05 13:35:50,508 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.5327\n",
      "2025-04-05 13:36:09,203 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3012\n",
      "2025-04-05 13:36:27,993 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.6030\n",
      "2025-04-05 13:36:46,493 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.2930\n",
      "2025-04-05 13:37:05,702 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.4561\n",
      "2025-04-05 13:37:24,585 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.4433\n",
      "2025-04-05 13:37:43,519 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.3150\n",
      "2025-04-05 13:38:02,595 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.4944\n",
      "2025-04-05 13:38:21,200 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.4882\n",
      "2025-04-05 13:38:31,791 - INFO - Epoch [5/10] Train Loss: 0.4533, Train Accuracy: 78.23%\n",
      "2025-04-05 13:38:52,699 - INFO - Epoch [5/10] Val Loss: 0.3907, Val Accuracy: 83.16%\n",
      "2025-04-05 13:38:52,702 - INFO - New best model at epoch 5 with val accuracy: 83.16%\n",
      "2025-04-05 13:38:56,661 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.5351\n",
      "2025-04-05 13:39:15,934 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.5164\n",
      "2025-04-05 13:39:34,599 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.5512\n",
      "2025-04-05 13:39:53,817 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.3445\n",
      "2025-04-05 13:40:12,405 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2843\n",
      "2025-04-05 13:40:31,283 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.5171\n",
      "2025-04-05 13:40:50,491 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.4341\n",
      "2025-04-05 13:41:09,210 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.5765\n",
      "2025-04-05 13:41:28,130 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.3788\n",
      "2025-04-05 13:41:46,693 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.3437\n",
      "2025-04-05 13:42:05,643 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.4119\n",
      "2025-04-05 13:42:24,510 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2592\n",
      "2025-04-05 13:42:43,475 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.7175\n",
      "2025-04-05 13:43:03,016 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.3043\n",
      "2025-04-05 13:43:21,708 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.3276\n",
      "2025-04-05 13:43:40,437 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.3404\n",
      "2025-04-05 13:43:59,359 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.3855\n",
      "2025-04-05 13:44:18,562 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.5263\n",
      "2025-04-05 13:44:37,601 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.3292\n",
      "2025-04-05 13:44:48,176 - INFO - Epoch [6/10] Train Loss: 0.4174, Train Accuracy: 80.55%\n",
      "2025-04-05 13:45:10,239 - INFO - Epoch [6/10] Val Loss: 0.3526, Val Accuracy: 85.66%\n",
      "2025-04-05 13:45:10,241 - INFO - New best model at epoch 6 with val accuracy: 85.66%\n",
      "2025-04-05 13:45:12,894 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.2910\n",
      "2025-04-05 13:45:32,186 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.4296\n",
      "2025-04-05 13:45:50,969 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.4726\n",
      "2025-04-05 13:46:09,897 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.3646\n",
      "2025-04-05 13:46:29,393 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.3139\n",
      "2025-04-05 13:46:48,733 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.3137\n",
      "2025-04-05 13:47:07,372 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.4608\n",
      "2025-04-05 13:47:26,247 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.4610\n",
      "2025-04-05 13:47:45,316 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.3269\n",
      "2025-04-05 13:48:04,486 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.2732\n",
      "2025-04-05 13:48:23,560 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.4034\n",
      "2025-04-05 13:48:42,798 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.3849\n",
      "2025-04-05 13:49:01,563 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.6072\n",
      "2025-04-05 13:49:20,161 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.4122\n",
      "2025-04-05 13:49:39,783 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.7370\n",
      "2025-04-05 13:49:59,096 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.5006\n",
      "2025-04-05 13:50:18,206 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.3465\n",
      "2025-04-05 13:50:37,410 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.5155\n",
      "2025-04-05 13:50:56,533 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2652\n",
      "2025-04-05 13:51:07,175 - INFO - Epoch [7/10] Train Loss: 0.4021, Train Accuracy: 80.75%\n",
      "2025-04-05 13:51:28,527 - INFO - Epoch [7/10] Val Loss: 0.3130, Val Accuracy: 87.47%\n",
      "2025-04-05 13:51:28,529 - INFO - New best model at epoch 7 with val accuracy: 87.47%\n",
      "2025-04-05 13:51:31,552 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.3947\n",
      "2025-04-05 13:51:51,086 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.4126\n",
      "2025-04-05 13:52:10,304 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.4223\n",
      "2025-04-05 13:52:29,047 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.2661\n",
      "2025-04-05 13:52:47,832 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.2401\n",
      "2025-04-05 13:53:07,056 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2453\n",
      "2025-04-05 13:53:26,045 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.2190\n",
      "2025-04-05 13:53:44,642 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.3870\n",
      "2025-04-05 13:54:03,877 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.5868\n",
      "2025-04-05 13:54:23,351 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.3896\n",
      "2025-04-05 13:54:42,182 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.4902\n",
      "2025-04-05 13:55:01,247 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.3100\n",
      "2025-04-05 13:55:19,997 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.3089\n",
      "2025-04-05 13:55:38,946 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.2916\n",
      "2025-04-05 13:55:58,171 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.3045\n",
      "2025-04-05 13:56:16,832 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.4702\n",
      "2025-04-05 13:56:35,806 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.3035\n",
      "2025-04-05 13:56:54,424 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3893\n",
      "2025-04-05 13:57:12,879 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.4710\n",
      "2025-04-05 13:57:23,743 - INFO - Epoch [8/10] Train Loss: 0.3664, Train Accuracy: 82.59%\n",
      "2025-04-05 13:57:45,298 - INFO - Epoch [8/10] Val Loss: 0.3201, Val Accuracy: 86.43%\n",
      "2025-04-05 13:57:48,907 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.3455\n",
      "2025-04-05 13:58:07,918 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.3530\n",
      "2025-04-05 13:58:27,391 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.2239\n",
      "2025-04-05 13:58:46,330 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.2867\n",
      "2025-04-05 13:59:05,627 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.3717\n",
      "2025-04-05 13:59:24,370 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2702\n",
      "2025-04-05 13:59:43,434 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.1971\n",
      "2025-04-05 14:00:02,112 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2347\n",
      "2025-04-05 14:00:20,811 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.2225\n",
      "2025-04-05 14:00:40,092 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.4022\n",
      "2025-04-05 14:00:59,183 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4400\n",
      "2025-04-05 14:01:17,899 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.2079\n",
      "2025-04-05 14:01:36,898 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.2961\n",
      "2025-04-05 14:01:55,945 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2368\n",
      "2025-04-05 14:02:14,902 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.4061\n",
      "2025-04-05 14:02:33,612 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.3761\n",
      "2025-04-05 14:02:52,488 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.3470\n",
      "2025-04-05 14:03:11,290 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.3082\n",
      "2025-04-05 14:03:30,189 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.2836\n",
      "2025-04-05 14:03:41,016 - INFO - Epoch [9/10] Train Loss: 0.3561, Train Accuracy: 83.97%\n",
      "2025-04-05 14:04:03,733 - INFO - Epoch [9/10] Val Loss: 0.2906, Val Accuracy: 88.31%\n",
      "2025-04-05 14:04:03,736 - INFO - New best model at epoch 9 with val accuracy: 88.31%\n",
      "2025-04-05 14:04:05,707 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.4293\n",
      "2025-04-05 14:04:24,529 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.4122\n",
      "2025-04-05 14:04:43,205 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.3343\n",
      "2025-04-05 14:05:02,275 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.2833\n",
      "2025-04-05 14:05:21,059 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.3907\n",
      "2025-04-05 14:05:39,873 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2313\n",
      "2025-04-05 14:05:58,260 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2583\n",
      "2025-04-05 14:06:17,123 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.3629\n",
      "2025-04-05 14:06:36,568 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.4458\n",
      "2025-04-05 14:06:55,298 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.4151\n",
      "2025-04-05 14:07:14,127 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.1837\n",
      "2025-04-05 14:07:33,433 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.3502\n",
      "2025-04-05 14:07:52,756 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.3546\n",
      "2025-04-05 14:08:12,200 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.2384\n",
      "2025-04-05 14:08:31,053 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.5664\n",
      "2025-04-05 14:08:50,251 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.3768\n",
      "2025-04-05 14:09:09,844 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.2813\n",
      "2025-04-05 14:09:29,492 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.3180\n",
      "2025-04-05 14:09:48,703 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.2639\n",
      "2025-04-05 14:09:59,382 - INFO - Epoch [10/10] Train Loss: 0.3414, Train Accuracy: 83.75%\n",
      "2025-04-05 14:10:21,665 - INFO - Epoch [10/10] Val Loss: 0.2775, Val Accuracy: 88.31%\n",
      "2025-04-05 14:10:34,703 - INFO - Test Loss: 0.3109, Test Accuracy: 86.65%\n",
      "2025-04-05 14:10:34,703 - INFO - \n",
      "===== Final Performance Results =====\n",
      "2025-04-05 14:10:34,704 - INFO - {\n",
      "    \"world_size\": 4,\n",
      "    \"train_time\": 3785.717926502228,\n",
      "    \"avg_epoch_time\": 378.5717926502228,\n",
      "    \"val_accuracy\": 88.3089770354906,\n",
      "    \"test_accuracy\": 86.6481223922114,\n",
      "    \"test_loss\": 0.31092317412725246,\n",
      "    \"total_time\": 3785.717926502228,\n",
      "    \"train_losses\": [\n",
      "        0.6480397883518969,\n",
      "        0.6167628019923446,\n",
      "        0.568768892348062,\n",
      "        0.5166494257679545,\n",
      "        0.4532592488931313,\n",
      "        0.4173902630756091,\n",
      "        0.40206088277086555,\n",
      "        0.36639074012076006,\n",
      "        0.3560855256363937,\n",
      "        0.3413888303654962\n",
      "    ],\n",
      "    \"train_accuracies\": [\n",
      "        60.71966527196653,\n",
      "        65.65690376569037,\n",
      "        68.51882845188284,\n",
      "        74.05857740585775,\n",
      "        78.22594142259415,\n",
      "        80.55230125523012,\n",
      "        80.75313807531381,\n",
      "        82.59414225941423,\n",
      "        83.96652719665272,\n",
      "        83.7489539748954\n",
      "    ],\n",
      "    \"val_losses\": [\n",
      "        0.6515855416292603,\n",
      "        0.551400463450013,\n",
      "        0.548451015911089,\n",
      "        0.4716679027035739,\n",
      "        0.3907001546960617,\n",
      "        0.3525747205950605,\n",
      "        0.31295053048024346,\n",
      "        0.3200962401630983,\n",
      "        0.2905889655370719,\n",
      "        0.27752987315609956\n",
      "    ],\n",
      "    \"val_accuracies\": [\n",
      "        71.12038970076549,\n",
      "        73.55601948503828,\n",
      "        72.92971468336813,\n",
      "        77.59220598469032,\n",
      "        83.15935977731385,\n",
      "        85.66457898399443,\n",
      "        87.47390396659708,\n",
      "        86.43006263048017,\n",
      "        88.3089770354906,\n",
      "        88.3089770354906\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"num_epochs\": 10,\n",
      "        \"initial_lr\": 0.001,\n",
      "        \"weight_decay\": 0.0001,\n",
      "        \"num_classes\": 2,\n",
      "        \"save_model\": true\n",
      "    }\n",
      "}\n",
      "2025-04-05 14:10:34,897 - INFO - Model saved to models/training_using_cpus_4_best_model.pt\n",
      "2025-04-05 14:10:37,876 - INFO - Loss plot saved as plots/training_using_cpus_4_loss.png\n",
      "2025-04-05 14:10:37,964 - INFO - Accuracy plot saved as plots/training_using_cpus_4_accuracy.png\n",
      "2025-04-05 14:10:37,971 - INFO - Runtime parameters saved as metrics/training_using_cpus_4_params.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py --world_size 4 --num_threads 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec841e7c-3096-411f-b74e-3f24e2458bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
