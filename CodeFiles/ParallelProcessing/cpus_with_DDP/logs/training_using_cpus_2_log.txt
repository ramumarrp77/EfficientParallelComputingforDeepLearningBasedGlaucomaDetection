2025-04-04 16:07:54,631 - INFO - [Rank 1] Passed initial barrier
2025-04-04 16:07:54,631 - INFO - [Rank 0] Passed initial barrier
2025-04-04 16:07:54,656 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...
2025-04-04 16:07:54,656 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...
2025-04-04 16:10:43,923 - INFO - [Rank 1] Data loaded.
2025-04-04 16:10:43,923 - INFO - [Rank 0] Data loaded.
2025-04-04 16:10:44,455 - INFO - Model architecture: MedicalCNN
2025-04-04 16:10:44,455 - INFO - Total layers: 129
2025-04-04 16:10:44,455 - INFO - Total parameters: 22,494,274
2025-04-04 16:10:53,758 - INFO - Epoch [1/10], Batch [0/374], Loss: 0.6760
2025-04-04 16:11:59,289 - INFO - Epoch [1/10], Batch [10/374], Loss: 0.6599
2025-04-04 16:12:59,064 - INFO - Epoch [1/10], Batch [20/374], Loss: 0.7180
2025-04-04 16:13:59,563 - INFO - Epoch [1/10], Batch [30/374], Loss: 0.6635
2025-04-04 16:14:58,885 - INFO - Epoch [1/10], Batch [40/374], Loss: 0.6516
2025-04-04 16:15:58,533 - INFO - Epoch [1/10], Batch [50/374], Loss: 0.6806
2025-04-04 16:16:57,961 - INFO - Epoch [1/10], Batch [60/374], Loss: 0.6450
2025-04-04 16:17:57,397 - INFO - Epoch [1/10], Batch [70/374], Loss: 0.6863
2025-04-04 16:18:58,419 - INFO - Epoch [1/10], Batch [80/374], Loss: 0.6290
2025-04-04 16:19:58,717 - INFO - Epoch [1/10], Batch [90/374], Loss: 0.7215
2025-04-04 16:20:59,014 - INFO - Epoch [1/10], Batch [100/374], Loss: 0.6507
2025-04-04 16:21:58,961 - INFO - Epoch [1/10], Batch [110/374], Loss: 0.6000
2025-04-04 16:22:59,601 - INFO - Epoch [1/10], Batch [120/374], Loss: 0.6026
2025-04-04 16:23:59,404 - INFO - Epoch [1/10], Batch [130/374], Loss: 0.6735
2025-04-04 16:24:58,366 - INFO - Epoch [1/10], Batch [140/374], Loss: 0.6667
2025-04-04 16:25:58,617 - INFO - Epoch [1/10], Batch [150/374], Loss: 0.7315
2025-04-04 16:26:58,826 - INFO - Epoch [1/10], Batch [160/374], Loss: 0.6107
2025-04-04 16:27:59,808 - INFO - Epoch [1/10], Batch [170/374], Loss: 0.6883
2025-04-04 16:29:00,425 - INFO - Epoch [1/10], Batch [180/374], Loss: 0.6086
2025-04-04 16:30:01,043 - INFO - Epoch [1/10], Batch [190/374], Loss: 0.6761
2025-04-04 16:31:01,025 - INFO - Epoch [1/10], Batch [200/374], Loss: 0.6779
2025-04-04 16:32:00,487 - INFO - Epoch [1/10], Batch [210/374], Loss: 0.7650
2025-04-04 16:33:00,988 - INFO - Epoch [1/10], Batch [220/374], Loss: 0.6516
2025-04-04 16:34:02,092 - INFO - Epoch [1/10], Batch [230/374], Loss: 0.6346
2025-04-04 16:35:03,121 - INFO - Epoch [1/10], Batch [240/374], Loss: 0.7204
2025-04-04 16:36:03,076 - INFO - Epoch [1/10], Batch [250/374], Loss: 0.5511
2025-04-04 16:37:04,069 - INFO - Epoch [1/10], Batch [260/374], Loss: 0.5713
2025-04-04 16:38:04,065 - INFO - Epoch [1/10], Batch [270/374], Loss: 0.6098
2025-04-04 16:39:04,740 - INFO - Epoch [1/10], Batch [280/374], Loss: 0.5854
2025-04-04 16:40:05,455 - INFO - Epoch [1/10], Batch [290/374], Loss: 0.6265
2025-04-04 16:41:05,110 - INFO - Epoch [1/10], Batch [300/374], Loss: 0.5970
2025-04-04 16:42:04,552 - INFO - Epoch [1/10], Batch [310/374], Loss: 0.5327
2025-04-04 16:43:04,651 - INFO - Epoch [1/10], Batch [320/374], Loss: 0.6891
2025-04-04 16:44:04,899 - INFO - Epoch [1/10], Batch [330/374], Loss: 0.6378
2025-04-04 16:45:05,346 - INFO - Epoch [1/10], Batch [340/374], Loss: 0.5755
2025-04-04 16:46:05,454 - INFO - Epoch [1/10], Batch [350/374], Loss: 0.6582
2025-04-04 16:47:05,586 - INFO - Epoch [1/10], Batch [360/374], Loss: 0.6477
2025-04-04 16:48:05,277 - INFO - Epoch [1/10], Batch [370/374], Loss: 0.5742
2025-04-04 16:48:19,965 - INFO - Epoch [1/10] Train Loss: 0.6494, Train Accuracy: 60.23%
2025-04-04 16:51:00,054 - INFO - Epoch [1/10] Val Loss: 0.6207, Val Accuracy: 69.90%
2025-04-04 16:51:00,057 - INFO - New best model at epoch 1 with val accuracy: 69.90%
2025-04-04 16:51:08,506 - INFO - Epoch [2/10], Batch [0/374], Loss: 0.6476
2025-04-04 16:52:08,594 - INFO - Epoch [2/10], Batch [10/374], Loss: 0.6168
2025-04-04 16:53:08,252 - INFO - Epoch [2/10], Batch [20/374], Loss: 0.5836
2025-04-04 16:54:08,123 - INFO - Epoch [2/10], Batch [30/374], Loss: 0.6016
2025-04-04 16:55:09,938 - INFO - Epoch [2/10], Batch [40/374], Loss: 0.7590
2025-04-04 16:56:10,582 - INFO - Epoch [2/10], Batch [50/374], Loss: 0.5421
2025-04-04 16:57:10,537 - INFO - Epoch [2/10], Batch [60/374], Loss: 0.7318
2025-04-04 16:58:10,263 - INFO - Epoch [2/10], Batch [70/374], Loss: 0.6597
2025-04-04 16:59:10,651 - INFO - Epoch [2/10], Batch [80/374], Loss: 0.5618
2025-04-04 17:00:10,833 - INFO - Epoch [2/10], Batch [90/374], Loss: 0.7018
2025-04-04 17:01:10,961 - INFO - Epoch [2/10], Batch [100/374], Loss: 0.5608
2025-04-04 17:02:11,676 - INFO - Epoch [2/10], Batch [110/374], Loss: 0.6045
2025-04-04 17:03:11,861 - INFO - Epoch [2/10], Batch [120/374], Loss: 0.5458
2025-04-04 17:04:11,531 - INFO - Epoch [2/10], Batch [130/374], Loss: 0.5742
2025-04-04 17:05:11,193 - INFO - Epoch [2/10], Batch [140/374], Loss: 0.6314
2025-04-04 17:06:11,404 - INFO - Epoch [2/10], Batch [150/374], Loss: 0.5536
2025-04-04 17:07:11,644 - INFO - Epoch [2/10], Batch [160/374], Loss: 0.5731
2025-04-04 17:08:11,258 - INFO - Epoch [2/10], Batch [170/374], Loss: 0.6980
2025-04-04 17:09:10,678 - INFO - Epoch [2/10], Batch [180/374], Loss: 0.5081
2025-04-04 17:10:10,111 - INFO - Epoch [2/10], Batch [190/374], Loss: 0.4722
2025-04-04 17:11:09,971 - INFO - Epoch [2/10], Batch [200/374], Loss: 0.7237
2025-04-04 17:12:10,926 - INFO - Epoch [2/10], Batch [210/374], Loss: 0.6307
2025-04-04 17:13:11,757 - INFO - Epoch [2/10], Batch [220/374], Loss: 0.5803
2025-04-04 17:14:12,404 - INFO - Epoch [2/10], Batch [230/374], Loss: 0.6314
2025-04-04 17:15:11,544 - INFO - Epoch [2/10], Batch [240/374], Loss: 0.6833
2025-04-04 17:16:11,156 - INFO - Epoch [2/10], Batch [250/374], Loss: 0.8291
2025-04-04 17:17:11,150 - INFO - Epoch [2/10], Batch [260/374], Loss: 0.7387
2025-04-04 17:18:10,365 - INFO - Epoch [2/10], Batch [270/374], Loss: 0.5340
2025-04-04 17:19:10,464 - INFO - Epoch [2/10], Batch [280/374], Loss: 0.6656
2025-04-04 17:20:10,928 - INFO - Epoch [2/10], Batch [290/374], Loss: 0.5710
2025-04-04 17:21:10,906 - INFO - Epoch [2/10], Batch [300/374], Loss: 0.6142
2025-04-04 17:22:11,571 - INFO - Epoch [2/10], Batch [310/374], Loss: 0.5850
2025-04-04 17:23:12,434 - INFO - Epoch [2/10], Batch [320/374], Loss: 0.6446
2025-04-04 17:24:13,199 - INFO - Epoch [2/10], Batch [330/374], Loss: 0.5870
2025-04-04 17:25:12,758 - INFO - Epoch [2/10], Batch [340/374], Loss: 0.5840
2025-04-04 17:26:12,631 - INFO - Epoch [2/10], Batch [350/374], Loss: 0.6232
2025-04-04 17:27:12,751 - INFO - Epoch [2/10], Batch [360/374], Loss: 0.6075
2025-04-04 17:28:13,084 - INFO - Epoch [2/10], Batch [370/374], Loss: 0.7074
2025-04-04 17:28:27,476 - INFO - Epoch [2/10] Train Loss: 0.6192, Train Accuracy: 65.22%
2025-04-04 17:31:07,980 - INFO - Epoch [2/10] Val Loss: 0.5542, Val Accuracy: 74.29%
2025-04-04 17:31:07,983 - INFO - New best model at epoch 2 with val accuracy: 74.29%
2025-04-04 17:31:14,346 - INFO - Epoch [3/10], Batch [0/374], Loss: 0.8245
2025-04-04 17:32:15,869 - INFO - Epoch [3/10], Batch [10/374], Loss: 0.6766
2025-04-04 17:33:16,468 - INFO - Epoch [3/10], Batch [20/374], Loss: 0.6503
2025-04-04 17:34:17,451 - INFO - Epoch [3/10], Batch [30/374], Loss: 0.5435
2025-04-04 17:35:17,404 - INFO - Epoch [3/10], Batch [40/374], Loss: 0.5588
2025-04-04 17:36:18,115 - INFO - Epoch [3/10], Batch [50/374], Loss: 0.5990
2025-04-04 17:37:18,759 - INFO - Epoch [3/10], Batch [60/374], Loss: 0.7991
2025-04-04 17:38:18,757 - INFO - Epoch [3/10], Batch [70/374], Loss: 0.5246
2025-04-04 17:39:19,079 - INFO - Epoch [3/10], Batch [80/374], Loss: 0.6961
2025-04-04 17:40:19,400 - INFO - Epoch [3/10], Batch [90/374], Loss: 0.5544
2025-04-04 17:41:20,508 - INFO - Epoch [3/10], Batch [100/374], Loss: 0.6027
2025-04-04 17:42:21,368 - INFO - Epoch [3/10], Batch [110/374], Loss: 0.5998
2025-04-04 17:43:20,577 - INFO - Epoch [3/10], Batch [120/374], Loss: 0.5403
2025-04-04 17:44:20,693 - INFO - Epoch [3/10], Batch [130/374], Loss: 0.6350
2025-04-04 17:45:21,372 - INFO - Epoch [3/10], Batch [140/374], Loss: 0.5453
2025-04-04 17:46:20,995 - INFO - Epoch [3/10], Batch [150/374], Loss: 0.5837
2025-04-04 17:47:21,014 - INFO - Epoch [3/10], Batch [160/374], Loss: 0.6439
2025-04-04 17:48:20,560 - INFO - Epoch [3/10], Batch [170/374], Loss: 0.6924
2025-04-04 17:49:19,805 - INFO - Epoch [3/10], Batch [180/374], Loss: 0.6450
2025-04-04 17:50:20,565 - INFO - Epoch [3/10], Batch [190/374], Loss: 0.6515
2025-04-04 17:51:21,480 - INFO - Epoch [3/10], Batch [200/374], Loss: 0.5991
2025-04-04 17:52:21,292 - INFO - Epoch [3/10], Batch [210/374], Loss: 0.7209
2025-04-04 17:53:22,131 - INFO - Epoch [3/10], Batch [220/374], Loss: 0.5985
2025-04-04 17:54:22,385 - INFO - Epoch [3/10], Batch [230/374], Loss: 0.6760
2025-04-04 17:55:22,194 - INFO - Epoch [3/10], Batch [240/374], Loss: 0.6081
2025-04-04 17:56:21,831 - INFO - Epoch [3/10], Batch [250/374], Loss: 0.5724
2025-04-04 17:57:22,252 - INFO - Epoch [3/10], Batch [260/374], Loss: 0.5628
2025-04-04 17:58:22,092 - INFO - Epoch [3/10], Batch [270/374], Loss: 0.4904
2025-04-04 17:59:23,278 - INFO - Epoch [3/10], Batch [280/374], Loss: 0.6655
2025-04-04 18:00:23,424 - INFO - Epoch [3/10], Batch [290/374], Loss: 0.6303
2025-04-04 18:01:23,451 - INFO - Epoch [3/10], Batch [300/374], Loss: 0.4772
2025-04-04 18:02:23,881 - INFO - Epoch [3/10], Batch [310/374], Loss: 0.6011
2025-04-04 18:03:24,000 - INFO - Epoch [3/10], Batch [320/374], Loss: 0.5495
2025-04-04 18:04:24,326 - INFO - Epoch [3/10], Batch [330/374], Loss: 0.5638
2025-04-04 18:05:24,940 - INFO - Epoch [3/10], Batch [340/374], Loss: 0.4782
2025-04-04 18:06:25,247 - INFO - Epoch [3/10], Batch [350/374], Loss: 0.6773
2025-04-04 18:07:25,660 - INFO - Epoch [3/10], Batch [360/374], Loss: 0.5653
2025-04-04 18:08:26,344 - INFO - Epoch [3/10], Batch [370/374], Loss: 0.4299
2025-04-04 18:08:40,919 - INFO - Epoch [3/10] Train Loss: 0.5893, Train Accuracy: 67.39%
2025-04-04 18:11:21,634 - INFO - Epoch [3/10] Val Loss: 0.5207, Val Accuracy: 75.82%
2025-04-04 18:11:21,637 - INFO - New best model at epoch 3 with val accuracy: 75.82%
2025-04-04 18:11:30,530 - INFO - Epoch [4/10], Batch [0/374], Loss: 0.6069
2025-04-04 18:12:30,156 - INFO - Epoch [4/10], Batch [10/374], Loss: 0.5116
2025-04-04 18:13:29,511 - INFO - Epoch [4/10], Batch [20/374], Loss: 0.4631
2025-04-04 18:14:29,799 - INFO - Epoch [4/10], Batch [30/374], Loss: 0.5712
2025-04-04 18:15:30,791 - INFO - Epoch [4/10], Batch [40/374], Loss: 0.5133
2025-04-04 18:16:31,753 - INFO - Epoch [4/10], Batch [50/374], Loss: 0.5267
2025-04-04 18:17:32,234 - INFO - Epoch [4/10], Batch [60/374], Loss: 0.5852
2025-04-04 18:18:32,380 - INFO - Epoch [4/10], Batch [70/374], Loss: 0.5542
2025-04-04 18:19:32,711 - INFO - Epoch [4/10], Batch [80/374], Loss: 0.5982
2025-04-04 18:20:32,285 - INFO - Epoch [4/10], Batch [90/374], Loss: 0.5513
2025-04-04 18:21:31,871 - INFO - Epoch [4/10], Batch [100/374], Loss: 0.5296
2025-04-04 18:22:31,260 - INFO - Epoch [4/10], Batch [110/374], Loss: 0.7234
2025-04-04 18:23:30,563 - INFO - Epoch [4/10], Batch [120/374], Loss: 0.4928
2025-04-04 18:24:29,838 - INFO - Epoch [4/10], Batch [130/374], Loss: 0.5751
2025-04-04 18:25:29,204 - INFO - Epoch [4/10], Batch [140/374], Loss: 0.4277
2025-04-04 18:26:28,871 - INFO - Epoch [4/10], Batch [150/374], Loss: 0.5925
2025-04-04 18:27:28,097 - INFO - Epoch [4/10], Batch [160/374], Loss: 0.7010
2025-04-04 18:28:27,531 - INFO - Epoch [4/10], Batch [170/374], Loss: 0.5070
2025-04-04 18:29:27,824 - INFO - Epoch [4/10], Batch [180/374], Loss: 0.6276
2025-04-04 18:30:28,232 - INFO - Epoch [4/10], Batch [190/374], Loss: 0.4356
2025-04-04 18:31:28,030 - INFO - Epoch [4/10], Batch [200/374], Loss: 0.5933
2025-04-04 18:32:27,881 - INFO - Epoch [4/10], Batch [210/374], Loss: 0.4545
2025-04-04 18:33:27,464 - INFO - Epoch [4/10], Batch [220/374], Loss: 0.4613
2025-04-04 18:34:27,707 - INFO - Epoch [4/10], Batch [230/374], Loss: 0.4648
2025-04-04 18:35:28,390 - INFO - Epoch [4/10], Batch [240/374], Loss: 0.6062
2025-04-04 18:36:29,475 - INFO - Epoch [4/10], Batch [250/374], Loss: 0.6437
2025-04-04 18:37:29,467 - INFO - Epoch [4/10], Batch [260/374], Loss: 0.5406
2025-04-04 18:38:29,713 - INFO - Epoch [4/10], Batch [270/374], Loss: 0.4473
2025-04-04 18:39:29,495 - INFO - Epoch [4/10], Batch [280/374], Loss: 0.4914
2025-04-04 18:40:29,075 - INFO - Epoch [4/10], Batch [290/374], Loss: 0.4150
2025-04-04 18:41:29,050 - INFO - Epoch [4/10], Batch [300/374], Loss: 0.4905
2025-04-04 18:42:29,084 - INFO - Epoch [4/10], Batch [310/374], Loss: 0.3306
2025-04-04 18:43:27,791 - INFO - Epoch [4/10], Batch [320/374], Loss: 0.6063
2025-04-04 18:44:26,795 - INFO - Epoch [4/10], Batch [330/374], Loss: 0.4496
2025-04-04 18:45:26,036 - INFO - Epoch [4/10], Batch [340/374], Loss: 0.5047
2025-04-04 18:46:25,776 - INFO - Epoch [4/10], Batch [350/374], Loss: 0.4073
2025-04-04 18:47:26,089 - INFO - Epoch [4/10], Batch [360/374], Loss: 0.3404
2025-04-04 18:48:26,012 - INFO - Epoch [4/10], Batch [370/374], Loss: 0.5263
2025-04-04 18:48:40,275 - INFO - Epoch [4/10] Train Loss: 0.5392, Train Accuracy: 72.52%
2025-04-04 18:51:20,443 - INFO - Epoch [4/10] Val Loss: 0.5256, Val Accuracy: 76.83%
2025-04-04 18:51:20,446 - INFO - New best model at epoch 4 with val accuracy: 76.83%
2025-04-04 18:51:28,783 - INFO - Epoch [5/10], Batch [0/374], Loss: 0.5372
2025-04-04 18:52:28,434 - INFO - Epoch [5/10], Batch [10/374], Loss: 0.3794
2025-04-04 18:53:27,922 - INFO - Epoch [5/10], Batch [20/374], Loss: 0.4274
2025-04-04 18:54:27,764 - INFO - Epoch [5/10], Batch [30/374], Loss: 0.3677
2025-04-04 18:55:27,359 - INFO - Epoch [5/10], Batch [40/374], Loss: 0.3575
2025-04-04 18:56:26,721 - INFO - Epoch [5/10], Batch [50/374], Loss: 0.3638
2025-04-04 18:57:26,001 - INFO - Epoch [5/10], Batch [60/374], Loss: 0.5695
2025-04-04 18:58:25,178 - INFO - Epoch [5/10], Batch [70/374], Loss: 0.4102
2025-04-04 18:59:24,538 - INFO - Epoch [5/10], Batch [80/374], Loss: 0.3381
2025-04-04 19:00:24,602 - INFO - Epoch [5/10], Batch [90/374], Loss: 0.4315
2025-04-04 19:01:24,339 - INFO - Epoch [5/10], Batch [100/374], Loss: 0.4152
2025-04-04 19:02:23,978 - INFO - Epoch [5/10], Batch [110/374], Loss: 0.5663
2025-04-04 19:03:24,279 - INFO - Epoch [5/10], Batch [120/374], Loss: 0.5008
2025-04-04 19:04:23,772 - INFO - Epoch [5/10], Batch [130/374], Loss: 0.6164
2025-04-04 19:05:23,764 - INFO - Epoch [5/10], Batch [140/374], Loss: 0.4626
2025-04-04 19:06:24,131 - INFO - Epoch [5/10], Batch [150/374], Loss: 0.5808
2025-04-04 19:07:23,873 - INFO - Epoch [5/10], Batch [160/374], Loss: 0.4423
2025-04-04 19:08:23,800 - INFO - Epoch [5/10], Batch [170/374], Loss: 0.4545
2025-04-04 19:09:23,925 - INFO - Epoch [5/10], Batch [180/374], Loss: 0.3960
2025-04-04 19:10:23,699 - INFO - Epoch [5/10], Batch [190/374], Loss: 0.5714
2025-04-04 19:11:23,422 - INFO - Epoch [5/10], Batch [200/374], Loss: 0.5631
2025-04-04 19:12:22,935 - INFO - Epoch [5/10], Batch [210/374], Loss: 0.3296
2025-04-04 19:13:22,219 - INFO - Epoch [5/10], Batch [220/374], Loss: 0.4197
2025-04-04 19:14:21,977 - INFO - Epoch [5/10], Batch [230/374], Loss: 0.4810
2025-04-04 19:15:21,601 - INFO - Epoch [5/10], Batch [240/374], Loss: 0.4151
2025-04-04 19:16:21,906 - INFO - Epoch [5/10], Batch [250/374], Loss: 0.6918
2025-04-04 19:17:22,299 - INFO - Epoch [5/10], Batch [260/374], Loss: 0.3201
2025-04-04 19:18:22,626 - INFO - Epoch [5/10], Batch [270/374], Loss: 0.5241
2025-04-04 19:19:23,038 - INFO - Epoch [5/10], Batch [280/374], Loss: 0.4849
2025-04-04 19:20:22,801 - INFO - Epoch [5/10], Batch [290/374], Loss: 0.6168
2025-04-04 19:21:22,493 - INFO - Epoch [5/10], Batch [300/374], Loss: 0.5494
2025-04-04 19:22:22,064 - INFO - Epoch [5/10], Batch [310/374], Loss: 0.2686
2025-04-04 19:23:21,240 - INFO - Epoch [5/10], Batch [320/374], Loss: 0.3762
2025-04-04 19:24:20,875 - INFO - Epoch [5/10], Batch [330/374], Loss: 0.4640
2025-04-04 19:25:20,840 - INFO - Epoch [5/10], Batch [340/374], Loss: 0.3751
2025-04-04 19:26:20,830 - INFO - Epoch [5/10], Batch [350/374], Loss: 0.3892
2025-04-04 19:27:20,466 - INFO - Epoch [5/10], Batch [360/374], Loss: 0.5367
2025-04-04 19:28:20,241 - INFO - Epoch [5/10], Batch [370/374], Loss: 0.6245
2025-04-04 19:28:34,767 - INFO - Epoch [5/10] Train Loss: 0.4695, Train Accuracy: 77.32%
2025-04-04 19:31:15,613 - INFO - Epoch [5/10] Val Loss: 0.6135, Val Accuracy: 70.18%
2025-04-04 19:31:21,967 - INFO - Epoch [6/10], Batch [0/374], Loss: 0.4436
2025-04-04 19:32:21,193 - INFO - Epoch [6/10], Batch [10/374], Loss: 0.5167
2025-04-04 19:33:20,469 - INFO - Epoch [6/10], Batch [20/374], Loss: 0.6564
2025-04-04 19:34:19,990 - INFO - Epoch [6/10], Batch [30/374], Loss: 0.3838
2025-04-04 19:35:18,659 - INFO - Epoch [6/10], Batch [40/374], Loss: 0.4968
2025-04-04 19:36:18,257 - INFO - Epoch [6/10], Batch [50/374], Loss: 0.3633
2025-04-04 19:37:18,115 - INFO - Epoch [6/10], Batch [60/374], Loss: 0.4165
2025-04-04 19:38:16,917 - INFO - Epoch [6/10], Batch [70/374], Loss: 0.4464
2025-04-04 19:39:17,074 - INFO - Epoch [6/10], Batch [80/374], Loss: 0.4608
2025-04-04 19:40:16,874 - INFO - Epoch [6/10], Batch [90/374], Loss: 0.3905
2025-04-04 19:41:16,443 - INFO - Epoch [6/10], Batch [100/374], Loss: 0.4438
2025-04-04 19:42:16,273 - INFO - Epoch [6/10], Batch [110/374], Loss: 0.4134
2025-04-04 19:43:15,658 - INFO - Epoch [6/10], Batch [120/374], Loss: 0.4153
2025-04-04 19:44:15,716 - INFO - Epoch [6/10], Batch [130/374], Loss: 0.3590
2025-04-04 19:45:14,893 - INFO - Epoch [6/10], Batch [140/374], Loss: 0.3603
2025-04-04 19:46:14,546 - INFO - Epoch [6/10], Batch [150/374], Loss: 0.2973
2025-04-04 19:47:14,158 - INFO - Epoch [6/10], Batch [160/374], Loss: 0.3900
2025-04-04 19:48:14,182 - INFO - Epoch [6/10], Batch [170/374], Loss: 0.2661
2025-04-04 19:49:13,639 - INFO - Epoch [6/10], Batch [180/374], Loss: 0.4251
2025-04-04 19:50:13,122 - INFO - Epoch [6/10], Batch [190/374], Loss: 0.3490
2025-04-04 19:51:13,022 - INFO - Epoch [6/10], Batch [200/374], Loss: 0.2633
2025-04-04 19:52:12,214 - INFO - Epoch [6/10], Batch [210/374], Loss: 0.3631
2025-04-04 19:53:11,392 - INFO - Epoch [6/10], Batch [220/374], Loss: 0.3735
2025-04-04 19:54:10,489 - INFO - Epoch [6/10], Batch [230/374], Loss: 0.3458
2025-04-04 19:55:09,495 - INFO - Epoch [6/10], Batch [240/374], Loss: 0.6580
2025-04-04 19:56:08,556 - INFO - Epoch [6/10], Batch [250/374], Loss: 0.4076
2025-04-04 19:57:08,097 - INFO - Epoch [6/10], Batch [260/374], Loss: 0.3739
2025-04-04 19:58:07,718 - INFO - Epoch [6/10], Batch [270/374], Loss: 0.3590
2025-04-04 19:59:06,756 - INFO - Epoch [6/10], Batch [280/374], Loss: 0.3503
2025-04-04 20:00:05,822 - INFO - Epoch [6/10], Batch [290/374], Loss: 0.2801
2025-04-04 20:01:05,164 - INFO - Epoch [6/10], Batch [300/374], Loss: 0.4084
2025-04-04 20:02:04,775 - INFO - Epoch [6/10], Batch [310/374], Loss: 0.3660
2025-04-04 20:03:04,673 - INFO - Epoch [6/10], Batch [320/374], Loss: 0.4412
2025-04-04 20:04:04,673 - INFO - Epoch [6/10], Batch [330/374], Loss: 0.3437
2025-04-04 20:05:04,410 - INFO - Epoch [6/10], Batch [340/374], Loss: 0.3794
2025-04-04 20:06:04,501 - INFO - Epoch [6/10], Batch [350/374], Loss: 0.4519
2025-04-04 20:07:04,181 - INFO - Epoch [6/10], Batch [360/374], Loss: 0.6356
2025-04-04 20:08:03,978 - INFO - Epoch [6/10], Batch [370/374], Loss: 0.6562
2025-04-04 20:08:18,209 - INFO - Epoch [6/10] Train Loss: 0.4313, Train Accuracy: 79.60%
2025-04-04 20:10:56,570 - INFO - Epoch [6/10] Val Loss: 0.3677, Val Accuracy: 83.30%
2025-04-04 20:10:56,573 - INFO - New best model at epoch 6 with val accuracy: 83.30%
2025-04-04 20:11:05,182 - INFO - Epoch [7/10], Batch [0/374], Loss: 0.3157
2025-04-04 20:12:05,842 - INFO - Epoch [7/10], Batch [10/374], Loss: 0.4199
2025-04-04 20:13:05,776 - INFO - Epoch [7/10], Batch [20/374], Loss: 0.3712
2025-04-04 20:14:04,789 - INFO - Epoch [7/10], Batch [30/374], Loss: 0.3278
2025-04-04 20:15:04,651 - INFO - Epoch [7/10], Batch [40/374], Loss: 0.5406
2025-04-04 20:16:03,782 - INFO - Epoch [7/10], Batch [50/374], Loss: 0.3874
2025-04-04 20:17:03,467 - INFO - Epoch [7/10], Batch [60/374], Loss: 0.5013
2025-04-04 20:18:03,012 - INFO - Epoch [7/10], Batch [70/374], Loss: 0.3043
2025-04-04 20:19:03,707 - INFO - Epoch [7/10], Batch [80/374], Loss: 0.3626
2025-04-04 20:20:04,215 - INFO - Epoch [7/10], Batch [90/374], Loss: 0.2362
2025-04-04 20:21:03,892 - INFO - Epoch [7/10], Batch [100/374], Loss: 0.3892
2025-04-04 20:22:03,650 - INFO - Epoch [7/10], Batch [110/374], Loss: 0.2864
2025-04-04 20:23:03,106 - INFO - Epoch [7/10], Batch [120/374], Loss: 0.4717
2025-04-04 20:24:02,017 - INFO - Epoch [7/10], Batch [130/374], Loss: 0.3549
2025-04-04 20:25:01,298 - INFO - Epoch [7/10], Batch [140/374], Loss: 0.3138
2025-04-04 20:26:01,229 - INFO - Epoch [7/10], Batch [150/374], Loss: 0.1936
2025-04-04 20:27:01,120 - INFO - Epoch [7/10], Batch [160/374], Loss: 0.4222
2025-04-04 20:28:00,652 - INFO - Epoch [7/10], Batch [170/374], Loss: 0.2580
2025-04-04 20:28:59,653 - INFO - Epoch [7/10], Batch [180/374], Loss: 0.5260
2025-04-04 20:29:58,392 - INFO - Epoch [7/10], Batch [190/374], Loss: 0.4705
2025-04-04 20:30:58,957 - INFO - Epoch [7/10], Batch [200/374], Loss: 0.5239
2025-04-04 20:31:59,153 - INFO - Epoch [7/10], Batch [210/374], Loss: 0.2669
2025-04-04 20:32:58,923 - INFO - Epoch [7/10], Batch [220/374], Loss: 0.3522
2025-04-04 20:33:58,978 - INFO - Epoch [7/10], Batch [230/374], Loss: 0.3999
2025-04-04 20:34:58,769 - INFO - Epoch [7/10], Batch [240/374], Loss: 0.4328
2025-04-04 20:35:58,623 - INFO - Epoch [7/10], Batch [250/374], Loss: 0.3016
2025-04-04 20:36:58,487 - INFO - Epoch [7/10], Batch [260/374], Loss: 0.3115
2025-04-04 20:37:58,097 - INFO - Epoch [7/10], Batch [270/374], Loss: 0.5240
2025-04-04 20:38:57,509 - INFO - Epoch [7/10], Batch [280/374], Loss: 0.3429
2025-04-04 20:39:56,915 - INFO - Epoch [7/10], Batch [290/374], Loss: 0.2576
2025-04-04 20:40:56,822 - INFO - Epoch [7/10], Batch [300/374], Loss: 0.3403
2025-04-04 20:41:56,396 - INFO - Epoch [7/10], Batch [310/374], Loss: 0.4735
2025-04-04 20:42:56,938 - INFO - Epoch [7/10], Batch [320/374], Loss: 0.5547
2025-04-04 20:43:56,292 - INFO - Epoch [7/10], Batch [330/374], Loss: 0.3885
2025-04-04 20:44:55,242 - INFO - Epoch [7/10], Batch [340/374], Loss: 0.4274
2025-04-04 20:45:54,384 - INFO - Epoch [7/10], Batch [350/374], Loss: 0.4022
2025-04-04 20:46:53,759 - INFO - Epoch [7/10], Batch [360/374], Loss: 0.3632
2025-04-04 20:47:53,253 - INFO - Epoch [7/10], Batch [370/374], Loss: 0.4400
2025-04-04 20:48:07,650 - INFO - Epoch [7/10] Train Loss: 0.3998, Train Accuracy: 80.88%
2025-04-04 20:50:45,773 - INFO - Epoch [7/10] Val Loss: 0.3315, Val Accuracy: 86.26%
2025-04-04 20:50:45,775 - INFO - New best model at epoch 7 with val accuracy: 86.26%
2025-04-04 20:50:52,644 - INFO - Epoch [8/10], Batch [0/374], Loss: 0.5194
2025-04-04 20:51:52,630 - INFO - Epoch [8/10], Batch [10/374], Loss: 0.2643
2025-04-04 20:52:52,092 - INFO - Epoch [8/10], Batch [20/374], Loss: 0.4824
2025-04-04 20:53:51,227 - INFO - Epoch [8/10], Batch [30/374], Loss: 0.2722
2025-04-04 20:54:51,089 - INFO - Epoch [8/10], Batch [40/374], Loss: 0.5363
2025-04-04 20:55:51,020 - INFO - Epoch [8/10], Batch [50/374], Loss: 0.3480
2025-04-04 20:56:49,836 - INFO - Epoch [8/10], Batch [60/374], Loss: 0.2638
2025-04-04 20:57:48,916 - INFO - Epoch [8/10], Batch [70/374], Loss: 0.5155
2025-04-04 20:58:48,892 - INFO - Epoch [8/10], Batch [80/374], Loss: 0.2736
2025-04-04 20:59:48,767 - INFO - Epoch [8/10], Batch [90/374], Loss: 0.2929
2025-04-04 21:00:49,331 - INFO - Epoch [8/10], Batch [100/374], Loss: 0.4531
2025-04-04 21:01:49,936 - INFO - Epoch [8/10], Batch [110/374], Loss: 0.2055
2025-04-04 21:02:49,452 - INFO - Epoch [8/10], Batch [120/374], Loss: 0.2916
2025-04-04 21:03:49,126 - INFO - Epoch [8/10], Batch [130/374], Loss: 0.2457
2025-04-04 21:04:49,148 - INFO - Epoch [8/10], Batch [140/374], Loss: 0.4293
2025-04-04 21:05:48,970 - INFO - Epoch [8/10], Batch [150/374], Loss: 0.5548
2025-04-04 21:06:48,587 - INFO - Epoch [8/10], Batch [160/374], Loss: 0.8350
2025-04-04 21:07:47,770 - INFO - Epoch [8/10], Batch [170/374], Loss: 0.3342
2025-04-04 21:08:47,603 - INFO - Epoch [8/10], Batch [180/374], Loss: 0.5074
2025-04-04 21:09:48,278 - INFO - Epoch [8/10], Batch [190/374], Loss: 0.5849
2025-04-04 21:10:48,284 - INFO - Epoch [8/10], Batch [200/374], Loss: 0.3430
2025-04-04 21:11:48,952 - INFO - Epoch [8/10], Batch [210/374], Loss: 0.4343
2025-04-04 21:12:49,302 - INFO - Epoch [8/10], Batch [220/374], Loss: 0.2346
2025-04-04 21:13:48,652 - INFO - Epoch [8/10], Batch [230/374], Loss: 0.3009
2025-04-04 21:14:48,140 - INFO - Epoch [8/10], Batch [240/374], Loss: 0.2136
2025-04-04 21:15:47,471 - INFO - Epoch [8/10], Batch [250/374], Loss: 0.2556
2025-04-04 21:16:46,871 - INFO - Epoch [8/10], Batch [260/374], Loss: 0.3363
2025-04-04 21:17:47,197 - INFO - Epoch [8/10], Batch [270/374], Loss: 0.2714
2025-04-04 21:18:47,503 - INFO - Epoch [8/10], Batch [280/374], Loss: 0.3963
2025-04-04 21:19:48,677 - INFO - Epoch [8/10], Batch [290/374], Loss: 0.5050
2025-04-04 21:20:48,720 - INFO - Epoch [8/10], Batch [300/374], Loss: 0.4061
2025-04-04 21:21:48,576 - INFO - Epoch [8/10], Batch [310/374], Loss: 0.3124
2025-04-04 21:22:48,214 - INFO - Epoch [8/10], Batch [320/374], Loss: 0.4710
2025-04-04 21:23:48,124 - INFO - Epoch [8/10], Batch [330/374], Loss: 0.3110
2025-04-04 21:24:47,977 - INFO - Epoch [8/10], Batch [340/374], Loss: 0.3968
2025-04-04 21:25:48,045 - INFO - Epoch [8/10], Batch [350/374], Loss: 0.4467
2025-04-04 21:26:47,304 - INFO - Epoch [8/10], Batch [360/374], Loss: 0.3848
2025-04-04 21:27:46,875 - INFO - Epoch [8/10], Batch [370/374], Loss: 0.3024
2025-04-04 21:28:01,215 - INFO - Epoch [8/10] Train Loss: 0.3766, Train Accuracy: 82.41%
2025-04-04 21:30:40,315 - INFO - Epoch [8/10] Val Loss: 0.3051, Val Accuracy: 87.23%
2025-04-04 21:30:40,318 - INFO - New best model at epoch 8 with val accuracy: 87.23%
2025-04-04 21:30:48,515 - INFO - Epoch [9/10], Batch [0/374], Loss: 0.3785
2025-04-04 21:31:49,929 - INFO - Epoch [9/10], Batch [10/374], Loss: 0.5808
2025-04-04 21:32:50,261 - INFO - Epoch [9/10], Batch [20/374], Loss: 0.4333
2025-04-04 21:33:50,268 - INFO - Epoch [9/10], Batch [30/374], Loss: 0.4029
2025-04-04 21:34:51,327 - INFO - Epoch [9/10], Batch [40/374], Loss: 0.3554
2025-04-04 21:35:51,898 - INFO - Epoch [9/10], Batch [50/374], Loss: 0.5981
2025-04-04 22:23:06,100 - INFO - Epoch [10/10], Batch [120/374], Loss: 0.4328
2025-04-04 22:23:06,100 - INFO - Epoch [10/10], Batch [120/374], Loss: 0.4328
2025-04-04 22:24:05,165 - INFO - Epoch [10/10], Batch [130/374], Loss: 0.4192
2025-04-04 22:25:04,039 - INFO - Epoch [10/10], Batch [140/374], Loss: 0.3508
2025-04-04 22:26:03,328 - INFO - Epoch [10/10], Batch [150/374], Loss: 0.2627
2025-04-04 22:27:03,366 - INFO - Epoch [10/10], Batch [160/374], Loss: 0.2525
2025-04-04 22:28:02,862 - INFO - Epoch [10/10], Batch [170/374], Loss: 0.2998
2025-04-04 22:29:03,185 - INFO - Epoch [10/10], Batch [180/374], Loss: 0.5579
2025-04-04 22:30:03,703 - INFO - Epoch [10/10], Batch [190/374], Loss: 0.3731
2025-04-04 22:31:04,194 - INFO - Epoch [10/10], Batch [200/374], Loss: 0.2728
2025-04-04 22:32:04,223 - INFO - Epoch [10/10], Batch [210/374], Loss: 0.3292
2025-04-04 22:33:03,428 - INFO - Epoch [10/10], Batch [220/374], Loss: 0.3550
2025-04-04 22:34:03,355 - INFO - Epoch [10/10], Batch [230/374], Loss: 0.2211
2025-04-04 22:35:03,140 - INFO - Epoch [10/10], Batch [240/374], Loss: 0.3406
2025-04-04 22:36:03,250 - INFO - Epoch [10/10], Batch [250/374], Loss: 0.5024
2025-04-04 22:37:03,900 - INFO - Epoch [10/10], Batch [260/374], Loss: 0.2766
2025-04-04 22:38:04,529 - INFO - Epoch [10/10], Batch [270/374], Loss: 0.3089
2025-04-04 22:39:05,166 - INFO - Epoch [10/10], Batch [280/374], Loss: 0.3441
2025-04-04 22:40:04,171 - INFO - Epoch [10/10], Batch [290/374], Loss: 0.2872
2025-04-04 22:41:04,517 - INFO - Epoch [10/10], Batch [300/374], Loss: 0.3045
2025-04-04 22:42:05,629 - INFO - Epoch [10/10], Batch [310/374], Loss: 0.4487
2025-04-04 22:43:06,726 - INFO - Epoch [10/10], Batch [320/374], Loss: 0.3065
2025-04-04 22:44:07,051 - INFO - Epoch [10/10], Batch [330/374], Loss: 0.5802
2025-04-04 22:45:07,356 - INFO - Epoch [10/10], Batch [340/374], Loss: 0.3658
2025-04-04 22:46:07,010 - INFO - Epoch [10/10], Batch [350/374], Loss: 0.2300
2025-04-04 22:47:07,021 - INFO - Epoch [10/10], Batch [360/374], Loss: 0.5752
2025-04-04 22:48:07,077 - INFO - Epoch [10/10], Batch [370/374], Loss: 0.4117
2025-04-04 22:48:21,504 - INFO - Epoch [10/10] Train Loss: 0.3576, Train Accuracy: 83.33%
2025-04-04 22:51:02,640 - INFO - Epoch [10/10] Val Loss: 0.2857, Val Accuracy: 87.72%
2025-04-04 22:51:02,642 - INFO - New best model at epoch 10 with val accuracy: 87.72%
2025-04-04 22:52:23,734 - INFO - Test Loss: 0.3119, Test Accuracy: 87.40%
2025-04-04 22:52:23,734 - INFO - 
===== Final Performance Results =====
2025-04-04 22:52:23,735 - INFO - {
    "world_size": 2,
    "train_time": 24018.184190511703,
    "avg_epoch_time": 2401.81841905117,
    "val_accuracy": 87.71746694502436,
    "test_accuracy": 87.40431454418928,
    "test_loss": 0.3118752727262992,
    "total_time": 24018.184190511703,
    "train_losses": [
        0.6493814360328235,
        0.6192248269487838,
        0.5893083884083723,
        0.5391946390928389,
        0.46946226711482303,
        0.4312671113960992,
        0.39975917053498006,
        0.3765572106334911,
        0.3652168217787973,
        0.35761043967482575
    ],
    "train_accuracies": [
        60.23098167210645,
        65.2188467654197,
        67.39476106787178,
        72.51652857979747,
        77.32027784751862,
        79.6049878650933,
        80.87706084191146,
        82.4085697547912,
        82.58431667922002,
        83.3291488827517
    ],
    "val_losses": [
        0.6207433361598993,
        0.5541805636161056,
        0.520727162819068,
        0.5256088383926147,
        0.6134759488417693,
        0.36767511815351167,
        0.3314648286783621,
        0.3050777815080137,
        0.2894544955874452,
        0.28565800866071933
    ],
    "val_accuracies": [
        69.90257480862908,
        74.28670842032011,
        75.81767571329158,
        76.82672233820459,
        70.18093249826026,
        83.29853862212944,
        86.25608907446068,
        87.2303409881698,
        87.43910925539318,
        87.71746694502436
    ],
    "hyperparameters": {
        "batch_size": 32,
        "num_epochs": 10,
        "initial_lr": 0.001,
        "weight_decay": 0.0001,
        "num_classes": 2,
        "save_model": true
    }
}
2025-04-04 22:52:23,914 - INFO - Model saved to models/training_using_cpus_2_best_model.pt
2025-04-04 22:52:25,655 - INFO - Loss plot saved as plots/training_using_cpus_2_loss.png
2025-04-04 22:52:25,741 - INFO - Accuracy plot saved as plots/training_using_cpus_2_accuracy.png
2025-04-04 22:52:25,754 - INFO - Runtime parameters saved as metrics/training_using_cpus_2_params.json
2025-04-04 23:08:12,894 - INFO - [Rank 1] Passed initial barrier
2025-04-04 23:08:12,894 - INFO - [Rank 0] Passed initial barrier
2025-04-04 23:08:12,970 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...
2025-04-04 23:08:12,970 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...
2025-04-04 23:14:53,865 - INFO - [Rank 1] Data loaded.
2025-04-04 23:14:53,865 - INFO - [Rank 0] Data loaded.
2025-04-04 23:14:54,920 - INFO - Model architecture: MedicalCNN
2025-04-04 23:14:54,920 - INFO - Total layers: 129
2025-04-04 23:14:54,920 - INFO - Total parameters: 22,494,274
2025-04-04 23:15:10,249 - INFO - Epoch [1/10], Batch [0/374], Loss: 0.6760
2025-04-04 23:17:10,258 - INFO - Epoch [1/10], Batch [10/374], Loss: 0.6599
2025-04-04 23:19:12,153 - INFO - Epoch [1/10], Batch [20/374], Loss: 0.7180
2025-04-04 23:21:19,080 - INFO - Epoch [1/10], Batch [30/374], Loss: 0.6635
2025-04-04 23:23:25,664 - INFO - Epoch [1/10], Batch [40/374], Loss: 0.6516
2025-04-04 23:25:33,665 - INFO - Epoch [1/10], Batch [50/374], Loss: 0.6806
2025-04-04 23:27:42,484 - INFO - Epoch [1/10], Batch [60/374], Loss: 0.6450
2025-04-04 23:28:40,810 - INFO - [Rank 1] Passed initial barrier
2025-04-04 23:28:40,810 - INFO - [Rank 0] Passed initial barrier
2025-04-04 23:28:40,824 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...
2025-04-04 23:28:40,824 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...
2025-04-04 23:28:57,236 - INFO - [Rank 1] Data loaded.
2025-04-04 23:28:57,236 - INFO - [Rank 0] Data loaded.
2025-04-04 23:28:57,677 - INFO - Model architecture: MedicalCNN
2025-04-04 23:28:57,677 - INFO - Total layers: 129
2025-04-04 23:28:57,677 - INFO - Total parameters: 22,494,274
2025-04-04 23:29:05,733 - INFO - Epoch [1/10], Batch [0/374], Loss: 0.6760
2025-04-04 23:30:09,285 - INFO - Epoch [1/10], Batch [10/374], Loss: 0.6599
2025-04-04 23:31:08,744 - INFO - Epoch [1/10], Batch [20/374], Loss: 0.7180
2025-04-04 23:32:07,959 - INFO - Epoch [1/10], Batch [30/374], Loss: 0.6635
2025-04-04 23:33:08,674 - INFO - Epoch [1/10], Batch [40/374], Loss: 0.6516
2025-04-04 23:34:09,090 - INFO - Epoch [1/10], Batch [50/374], Loss: 0.6806
2025-04-04 23:35:10,126 - INFO - Epoch [1/10], Batch [60/374], Loss: 0.6450
2025-04-04 23:36:10,490 - INFO - Epoch [1/10], Batch [70/374], Loss: 0.6863
2025-04-04 23:37:11,061 - INFO - Epoch [1/10], Batch [80/374], Loss: 0.6290
2025-04-04 23:38:10,772 - INFO - Epoch [1/10], Batch [90/374], Loss: 0.7215
2025-04-04 23:39:10,864 - INFO - Epoch [1/10], Batch [100/374], Loss: 0.6507
2025-04-04 23:40:10,777 - INFO - Epoch [1/10], Batch [110/374], Loss: 0.6000
2025-04-04 23:41:10,265 - INFO - Epoch [1/10], Batch [120/374], Loss: 0.6026
2025-04-04 23:42:09,582 - INFO - Epoch [1/10], Batch [130/374], Loss: 0.6735
2025-04-04 23:43:09,540 - INFO - Epoch [1/10], Batch [140/374], Loss: 0.6667
2025-04-04 23:44:09,389 - INFO - Epoch [1/10], Batch [150/374], Loss: 0.7315
2025-04-04 23:45:09,142 - INFO - Epoch [1/10], Batch [160/374], Loss: 0.6107
2025-04-04 23:46:09,096 - INFO - Epoch [1/10], Batch [170/374], Loss: 0.6883
2025-04-04 23:47:08,256 - INFO - Epoch [1/10], Batch [180/374], Loss: 0.6086
2025-04-04 23:48:07,514 - INFO - Epoch [1/10], Batch [190/374], Loss: 0.6761
2025-04-04 23:49:08,590 - INFO - Epoch [1/10], Batch [200/374], Loss: 0.6779
2025-04-04 23:50:09,779 - INFO - Epoch [1/10], Batch [210/374], Loss: 0.7650
2025-04-04 23:51:09,638 - INFO - Epoch [1/10], Batch [220/374], Loss: 0.6516
2025-04-04 23:52:08,899 - INFO - Epoch [1/10], Batch [230/374], Loss: 0.6346
2025-04-04 23:53:07,751 - INFO - Epoch [1/10], Batch [240/374], Loss: 0.7204
2025-04-04 23:54:07,565 - INFO - Epoch [1/10], Batch [250/374], Loss: 0.5511
2025-04-04 23:55:06,743 - INFO - Epoch [1/10], Batch [260/374], Loss: 0.5713
2025-04-04 23:56:06,493 - INFO - Epoch [1/10], Batch [270/374], Loss: 0.6098
2025-04-04 23:57:06,377 - INFO - Epoch [1/10], Batch [280/374], Loss: 0.5854
2025-04-04 23:58:06,548 - INFO - Epoch [1/10], Batch [290/374], Loss: 0.6265
2025-04-04 23:59:06,915 - INFO - Epoch [1/10], Batch [300/374], Loss: 0.5970
2025-04-05 00:00:06,373 - INFO - Epoch [1/10], Batch [310/374], Loss: 0.5327
2025-04-05 00:01:06,569 - INFO - Epoch [1/10], Batch [320/374], Loss: 0.6891
2025-04-05 00:02:06,094 - INFO - Epoch [1/10], Batch [330/374], Loss: 0.6378
2025-04-05 00:03:05,671 - INFO - Epoch [1/10], Batch [340/374], Loss: 0.5755
2025-04-05 00:04:05,701 - INFO - Epoch [1/10], Batch [350/374], Loss: 0.6582
2025-04-05 00:05:05,203 - INFO - Epoch [1/10], Batch [360/374], Loss: 0.6477
2025-04-05 00:06:05,525 - INFO - Epoch [1/10], Batch [370/374], Loss: 0.5742
2025-04-05 00:06:20,137 - INFO - Epoch [1/10] Train Loss: 0.6494, Train Accuracy: 60.23%
2025-04-05 00:09:03,787 - INFO - Epoch [1/10] Val Loss: 0.6207, Val Accuracy: 69.90%
2025-04-05 00:09:03,791 - INFO - New best model at epoch 1 with val accuracy: 69.90%
2025-04-05 00:09:10,332 - INFO - Epoch [2/10], Batch [0/374], Loss: 0.6476
2025-04-05 00:10:10,588 - INFO - Epoch [2/10], Batch [10/374], Loss: 0.6168
2025-04-05 00:11:09,533 - INFO - Epoch [2/10], Batch [20/374], Loss: 0.5836
2025-04-05 00:12:09,180 - INFO - Epoch [2/10], Batch [30/374], Loss: 0.6016
2025-04-05 00:13:09,369 - INFO - Epoch [2/10], Batch [40/374], Loss: 0.7590
2025-04-05 00:14:09,960 - INFO - Epoch [2/10], Batch [50/374], Loss: 0.5421
2025-04-05 00:15:10,269 - INFO - Epoch [2/10], Batch [60/374], Loss: 0.7318
2025-04-05 00:16:10,328 - INFO - Epoch [2/10], Batch [70/374], Loss: 0.6597
2025-04-05 00:17:08,934 - INFO - Epoch [2/10], Batch [80/374], Loss: 0.5618
2025-04-05 00:18:07,843 - INFO - Epoch [2/10], Batch [90/374], Loss: 0.7018
2025-04-05 00:19:07,295 - INFO - Epoch [2/10], Batch [100/374], Loss: 0.5608
2025-04-05 00:20:06,263 - INFO - Epoch [2/10], Batch [110/374], Loss: 0.6045
2025-04-05 00:21:05,602 - INFO - Epoch [2/10], Batch [120/374], Loss: 0.5458
2025-04-05 00:22:04,497 - INFO - Epoch [2/10], Batch [130/374], Loss: 0.5742
2025-04-05 00:23:03,935 - INFO - Epoch [2/10], Batch [140/374], Loss: 0.6314
2025-04-05 00:24:03,347 - INFO - Epoch [2/10], Batch [150/374], Loss: 0.5536
2025-04-05 00:25:02,375 - INFO - Epoch [2/10], Batch [160/374], Loss: 0.5731
2025-04-05 00:26:01,511 - INFO - Epoch [2/10], Batch [170/374], Loss: 0.6980
2025-04-05 00:27:01,053 - INFO - Epoch [2/10], Batch [180/374], Loss: 0.5081
2025-04-05 00:28:00,696 - INFO - Epoch [2/10], Batch [190/374], Loss: 0.4722
2025-04-05 00:29:00,626 - INFO - Epoch [2/10], Batch [200/374], Loss: 0.7237
2025-04-05 00:30:00,817 - INFO - Epoch [2/10], Batch [210/374], Loss: 0.6307
2025-04-05 00:31:01,054 - INFO - Epoch [2/10], Batch [220/374], Loss: 0.5803
2025-04-05 00:32:01,901 - INFO - Epoch [2/10], Batch [230/374], Loss: 0.6314
2025-04-05 00:33:00,956 - INFO - Epoch [2/10], Batch [240/374], Loss: 0.6833
2025-04-05 00:34:00,420 - INFO - Epoch [2/10], Batch [250/374], Loss: 0.8291
2025-04-05 00:35:00,630 - INFO - Epoch [2/10], Batch [260/374], Loss: 0.7387
2025-04-05 00:36:00,941 - INFO - Epoch [2/10], Batch [270/374], Loss: 0.5340
2025-04-05 00:37:00,308 - INFO - Epoch [2/10], Batch [280/374], Loss: 0.6656
2025-04-05 00:37:59,740 - INFO - Epoch [2/10], Batch [290/374], Loss: 0.5710
2025-04-05 00:39:00,237 - INFO - Epoch [2/10], Batch [300/374], Loss: 0.6142
2025-04-05 00:40:00,193 - INFO - Epoch [2/10], Batch [310/374], Loss: 0.5850
2025-04-05 00:40:59,841 - INFO - Epoch [2/10], Batch [320/374], Loss: 0.6446
2025-04-05 00:42:00,069 - INFO - Epoch [2/10], Batch [330/374], Loss: 0.5870
2025-04-05 00:43:00,168 - INFO - Epoch [2/10], Batch [340/374], Loss: 0.5840
2025-04-05 00:44:00,226 - INFO - Epoch [2/10], Batch [350/374], Loss: 0.6232
2025-04-05 00:45:00,633 - INFO - Epoch [2/10], Batch [360/374], Loss: 0.6075
2025-04-05 00:46:00,606 - INFO - Epoch [2/10], Batch [370/374], Loss: 0.7074
2025-04-05 00:46:15,035 - INFO - Epoch [2/10] Train Loss: 0.6192, Train Accuracy: 65.22%
2025-04-05 00:48:52,531 - INFO - Epoch [2/10] Val Loss: 0.5542, Val Accuracy: 74.29%
2025-04-05 00:48:52,534 - INFO - New best model at epoch 2 with val accuracy: 74.29%
2025-04-05 00:48:58,764 - INFO - Epoch [3/10], Batch [0/374], Loss: 0.8245
2025-04-05 00:49:58,794 - INFO - Epoch [3/10], Batch [10/374], Loss: 0.6766
2025-04-05 00:50:58,955 - INFO - Epoch [3/10], Batch [20/374], Loss: 0.6503
2025-04-05 00:51:58,865 - INFO - Epoch [3/10], Batch [30/374], Loss: 0.5435
2025-04-05 00:52:58,686 - INFO - Epoch [3/10], Batch [40/374], Loss: 0.5588
2025-04-05 00:53:57,963 - INFO - Epoch [3/10], Batch [50/374], Loss: 0.5990
2025-04-05 00:54:56,865 - INFO - Epoch [3/10], Batch [60/374], Loss: 0.7991
2025-04-05 00:55:56,411 - INFO - Epoch [3/10], Batch [70/374], Loss: 0.5246
2025-04-05 00:56:54,956 - INFO - Epoch [3/10], Batch [80/374], Loss: 0.6961
2025-04-05 00:57:53,155 - INFO - Epoch [3/10], Batch [90/374], Loss: 0.5544
2025-04-05 00:58:51,513 - INFO - Epoch [3/10], Batch [100/374], Loss: 0.6027
2025-04-05 00:59:50,631 - INFO - Epoch [3/10], Batch [110/374], Loss: 0.5998
2025-04-05 01:00:50,114 - INFO - Epoch [3/10], Batch [120/374], Loss: 0.5403
2025-04-05 01:01:50,355 - INFO - Epoch [3/10], Batch [130/374], Loss: 0.6350
2025-04-05 01:02:50,486 - INFO - Epoch [3/10], Batch [140/374], Loss: 0.5453
2025-04-05 01:03:50,957 - INFO - Epoch [3/10], Batch [150/374], Loss: 0.5837
2025-04-05 01:04:51,636 - INFO - Epoch [3/10], Batch [160/374], Loss: 0.6439
2025-04-05 01:05:52,091 - INFO - Epoch [3/10], Batch [170/374], Loss: 0.6924
2025-04-05 01:06:51,501 - INFO - Epoch [3/10], Batch [180/374], Loss: 0.6450
2025-04-05 01:07:51,603 - INFO - Epoch [3/10], Batch [190/374], Loss: 0.6515
2025-04-05 01:08:51,655 - INFO - Epoch [3/10], Batch [200/374], Loss: 0.5991
2025-04-05 01:09:52,111 - INFO - Epoch [3/10], Batch [210/374], Loss: 0.7209
2025-04-05 01:10:52,206 - INFO - Epoch [3/10], Batch [220/374], Loss: 0.5985
2025-04-05 01:11:52,216 - INFO - Epoch [3/10], Batch [230/374], Loss: 0.6760
2025-04-05 01:12:51,998 - INFO - Epoch [3/10], Batch [240/374], Loss: 0.6081
2025-04-05 01:13:51,605 - INFO - Epoch [3/10], Batch [250/374], Loss: 0.5724
2025-04-05 01:14:51,843 - INFO - Epoch [3/10], Batch [260/374], Loss: 0.5628
2025-04-05 01:15:52,197 - INFO - Epoch [3/10], Batch [270/374], Loss: 0.4904
2025-04-05 01:16:52,791 - INFO - Epoch [3/10], Batch [280/374], Loss: 0.6655
2025-04-05 01:17:53,939 - INFO - Epoch [3/10], Batch [290/374], Loss: 0.6303
2025-04-05 01:18:55,338 - INFO - Epoch [3/10], Batch [300/374], Loss: 0.4772
2025-04-05 01:19:55,413 - INFO - Epoch [3/10], Batch [310/374], Loss: 0.6011
2025-04-05 01:20:55,743 - INFO - Epoch [3/10], Batch [320/374], Loss: 0.5495
2025-04-05 01:21:55,929 - INFO - Epoch [3/10], Batch [330/374], Loss: 0.5638
2025-04-05 01:22:55,835 - INFO - Epoch [3/10], Batch [340/374], Loss: 0.4782
2025-04-05 01:23:55,841 - INFO - Epoch [3/10], Batch [350/374], Loss: 0.6773
2025-04-05 01:24:55,870 - INFO - Epoch [3/10], Batch [360/374], Loss: 0.5653
2025-04-05 01:25:55,979 - INFO - Epoch [3/10], Batch [370/374], Loss: 0.4299
2025-04-05 01:26:10,464 - INFO - Epoch [3/10] Train Loss: 0.5893, Train Accuracy: 67.39%
2025-04-05 01:28:47,843 - INFO - Epoch [3/10] Val Loss: 0.5207, Val Accuracy: 75.82%
2025-04-05 01:28:47,846 - INFO - New best model at epoch 3 with val accuracy: 75.82%
2025-04-05 01:28:55,220 - INFO - Epoch [4/10], Batch [0/374], Loss: 0.6069
2025-04-05 01:29:53,456 - INFO - Epoch [4/10], Batch [10/374], Loss: 0.5116
2025-04-05 01:30:52,404 - INFO - Epoch [4/10], Batch [20/374], Loss: 0.4631
2025-04-05 01:31:52,113 - INFO - Epoch [4/10], Batch [30/374], Loss: 0.5712
2025-04-05 01:32:50,987 - INFO - Epoch [4/10], Batch [40/374], Loss: 0.5133
2025-04-05 01:33:50,493 - INFO - Epoch [4/10], Batch [50/374], Loss: 0.5267
2025-04-05 01:34:50,272 - INFO - Epoch [4/10], Batch [60/374], Loss: 0.5852
2025-04-05 01:35:49,946 - INFO - Epoch [4/10], Batch [70/374], Loss: 0.5542
2025-04-05 01:36:51,092 - INFO - Epoch [4/10], Batch [80/374], Loss: 0.5982
2025-04-05 01:37:51,818 - INFO - Epoch [4/10], Batch [90/374], Loss: 0.5513
2025-04-05 01:38:52,310 - INFO - Epoch [4/10], Batch [100/374], Loss: 0.5296
2025-04-05 01:39:52,295 - INFO - Epoch [4/10], Batch [110/374], Loss: 0.7234
2025-04-05 01:40:52,263 - INFO - Epoch [4/10], Batch [120/374], Loss: 0.4928
2025-04-05 01:41:52,498 - INFO - Epoch [4/10], Batch [130/374], Loss: 0.5751
2025-04-05 01:42:53,927 - INFO - Epoch [4/10], Batch [140/374], Loss: 0.4277
2025-04-05 01:43:54,830 - INFO - Epoch [4/10], Batch [150/374], Loss: 0.5925
2025-04-05 01:44:53,377 - INFO - Epoch [4/10], Batch [160/374], Loss: 0.7010
2025-04-05 01:45:51,742 - INFO - Epoch [4/10], Batch [170/374], Loss: 0.5070
2025-04-05 01:46:50,509 - INFO - Epoch [4/10], Batch [180/374], Loss: 0.6276
2025-04-05 01:47:49,319 - INFO - Epoch [4/10], Batch [190/374], Loss: 0.4356
2025-04-05 01:48:47,932 - INFO - Epoch [4/10], Batch [200/374], Loss: 0.5933
2025-04-05 01:49:47,610 - INFO - Epoch [4/10], Batch [210/374], Loss: 0.4545
2025-04-05 01:50:47,121 - INFO - Epoch [4/10], Batch [220/374], Loss: 0.4613
2025-04-05 01:51:45,605 - INFO - Epoch [4/10], Batch [230/374], Loss: 0.4648
2025-04-05 01:52:45,935 - INFO - Epoch [4/10], Batch [240/374], Loss: 0.6062
2025-04-05 01:53:47,198 - INFO - Epoch [4/10], Batch [250/374], Loss: 0.6437
2025-04-05 01:54:47,679 - INFO - Epoch [4/10], Batch [260/374], Loss: 0.5406
2025-04-05 01:55:47,408 - INFO - Epoch [4/10], Batch [270/374], Loss: 0.4473
2025-04-05 01:56:46,289 - INFO - Epoch [4/10], Batch [280/374], Loss: 0.4914
2025-04-05 01:57:45,083 - INFO - Epoch [4/10], Batch [290/374], Loss: 0.4150
2025-04-05 01:58:44,912 - INFO - Epoch [4/10], Batch [300/374], Loss: 0.4905
2025-04-05 01:59:44,247 - INFO - Epoch [4/10], Batch [310/374], Loss: 0.3306
2025-04-05 02:00:43,644 - INFO - Epoch [4/10], Batch [320/374], Loss: 0.6063
2025-04-05 02:01:44,354 - INFO - Epoch [4/10], Batch [330/374], Loss: 0.4496
2025-04-05 02:02:44,414 - INFO - Epoch [4/10], Batch [340/374], Loss: 0.5047
2025-04-05 02:03:44,335 - INFO - Epoch [4/10], Batch [350/374], Loss: 0.4073
2025-04-05 02:04:43,609 - INFO - Epoch [4/10], Batch [360/374], Loss: 0.3404
2025-04-05 02:05:42,761 - INFO - Epoch [4/10], Batch [370/374], Loss: 0.5263
2025-04-05 02:05:57,240 - INFO - Epoch [4/10] Train Loss: 0.5392, Train Accuracy: 72.52%
2025-04-05 02:08:42,141 - INFO - Epoch [4/10] Val Loss: 0.5256, Val Accuracy: 76.83%
2025-04-05 02:08:42,144 - INFO - New best model at epoch 4 with val accuracy: 76.83%
2025-04-05 02:08:48,285 - INFO - Epoch [5/10], Batch [0/374], Loss: 0.5372
2025-04-05 02:09:47,825 - INFO - Epoch [5/10], Batch [10/374], Loss: 0.3794
2025-04-05 02:10:47,519 - INFO - Epoch [5/10], Batch [20/374], Loss: 0.4274
2025-04-05 02:11:47,901 - INFO - Epoch [5/10], Batch [30/374], Loss: 0.3677
2025-04-05 02:12:48,274 - INFO - Epoch [5/10], Batch [40/374], Loss: 0.3575
2025-04-05 02:13:49,500 - INFO - Epoch [5/10], Batch [50/374], Loss: 0.3638
2025-04-05 02:14:50,394 - INFO - Epoch [5/10], Batch [60/374], Loss: 0.5695
2025-04-05 02:15:50,565 - INFO - Epoch [5/10], Batch [70/374], Loss: 0.4102
2025-04-05 02:16:50,652 - INFO - Epoch [5/10], Batch [80/374], Loss: 0.3381
2025-04-05 02:17:51,301 - INFO - Epoch [5/10], Batch [90/374], Loss: 0.4315
2025-04-05 02:18:50,190 - INFO - Epoch [5/10], Batch [100/374], Loss: 0.4152
2025-04-05 02:19:48,879 - INFO - Epoch [5/10], Batch [110/374], Loss: 0.5663
2025-04-05 02:20:48,737 - INFO - Epoch [5/10], Batch [120/374], Loss: 0.5008
2025-04-05 02:21:47,848 - INFO - Epoch [5/10], Batch [130/374], Loss: 0.6164
2025-04-05 02:22:47,354 - INFO - Epoch [5/10], Batch [140/374], Loss: 0.4626
2025-04-05 02:23:46,229 - INFO - Epoch [5/10], Batch [150/374], Loss: 0.5808
2025-04-05 02:24:45,260 - INFO - Epoch [5/10], Batch [160/374], Loss: 0.4423
2025-04-05 02:25:44,487 - INFO - Epoch [5/10], Batch [170/374], Loss: 0.4545
2025-04-05 02:26:44,560 - INFO - Epoch [5/10], Batch [180/374], Loss: 0.3960
2025-04-05 02:27:43,998 - INFO - Epoch [5/10], Batch [190/374], Loss: 0.5714
2025-04-05 02:28:43,745 - INFO - Epoch [5/10], Batch [200/374], Loss: 0.5631
2025-04-05 02:29:42,908 - INFO - Epoch [5/10], Batch [210/374], Loss: 0.3296
2025-04-05 02:30:41,955 - INFO - Epoch [5/10], Batch [220/374], Loss: 0.4197
2025-04-05 02:31:41,938 - INFO - Epoch [5/10], Batch [230/374], Loss: 0.4810
2025-04-05 02:32:41,810 - INFO - Epoch [5/10], Batch [240/374], Loss: 0.4151
2025-04-05 02:33:41,034 - INFO - Epoch [5/10], Batch [250/374], Loss: 0.6918
2025-04-05 02:34:39,811 - INFO - Epoch [5/10], Batch [260/374], Loss: 0.3201
2025-04-05 02:35:38,603 - INFO - Epoch [5/10], Batch [270/374], Loss: 0.5241
2025-04-05 02:36:38,144 - INFO - Epoch [5/10], Batch [280/374], Loss: 0.4849
2025-04-05 02:37:37,238 - INFO - Epoch [5/10], Batch [290/374], Loss: 0.6168
2025-04-05 02:38:36,587 - INFO - Epoch [5/10], Batch [300/374], Loss: 0.5494
2025-04-05 02:39:35,730 - INFO - Epoch [5/10], Batch [310/374], Loss: 0.2686
2025-04-05 02:40:35,404 - INFO - Epoch [5/10], Batch [320/374], Loss: 0.3762
2025-04-05 02:41:35,326 - INFO - Epoch [5/10], Batch [330/374], Loss: 0.4640
2025-04-05 02:42:34,779 - INFO - Epoch [5/10], Batch [340/374], Loss: 0.3751
2025-04-05 02:43:34,237 - INFO - Epoch [5/10], Batch [350/374], Loss: 0.3892
2025-04-05 02:44:32,999 - INFO - Epoch [5/10], Batch [360/374], Loss: 0.5367
2025-04-05 02:45:32,265 - INFO - Epoch [5/10], Batch [370/374], Loss: 0.6245
2025-04-05 02:45:46,657 - INFO - Epoch [5/10] Train Loss: 0.4695, Train Accuracy: 77.32%
2025-04-05 02:48:28,669 - INFO - Epoch [5/10] Val Loss: 0.6135, Val Accuracy: 70.18%
2025-04-05 02:48:34,805 - INFO - Epoch [6/10], Batch [0/374], Loss: 0.4436
2025-04-05 02:49:33,968 - INFO - Epoch [6/10], Batch [10/374], Loss: 0.5167
2025-04-05 02:50:33,405 - INFO - Epoch [6/10], Batch [20/374], Loss: 0.6564
2025-04-05 02:51:33,778 - INFO - Epoch [6/10], Batch [30/374], Loss: 0.3838
2025-04-05 02:52:33,763 - INFO - Epoch [6/10], Batch [40/374], Loss: 0.4968
2025-04-05 02:53:32,472 - INFO - Epoch [6/10], Batch [50/374], Loss: 0.3633
2025-04-05 02:54:31,604 - INFO - Epoch [6/10], Batch [60/374], Loss: 0.4165
2025-04-05 02:55:31,696 - INFO - Epoch [6/10], Batch [70/374], Loss: 0.4464
2025-04-05 02:56:31,867 - INFO - Epoch [6/10], Batch [80/374], Loss: 0.4608
2025-04-05 02:57:31,454 - INFO - Epoch [6/10], Batch [90/374], Loss: 0.3905
2025-04-05 02:58:31,067 - INFO - Epoch [6/10], Batch [100/374], Loss: 0.4438
2025-04-05 02:59:30,812 - INFO - Epoch [6/10], Batch [110/374], Loss: 0.4134
2025-04-05 03:00:31,042 - INFO - Epoch [6/10], Batch [120/374], Loss: 0.4153
2025-04-05 03:01:32,323 - INFO - Epoch [6/10], Batch [130/374], Loss: 0.3590
2025-04-05 03:02:32,533 - INFO - Epoch [6/10], Batch [140/374], Loss: 0.3603
2025-04-05 03:03:32,231 - INFO - Epoch [6/10], Batch [150/374], Loss: 0.2973
2025-04-05 03:04:31,583 - INFO - Epoch [6/10], Batch [160/374], Loss: 0.3900
2025-04-05 03:05:31,537 - INFO - Epoch [6/10], Batch [170/374], Loss: 0.2661
2025-04-05 03:06:30,399 - INFO - Epoch [6/10], Batch [180/374], Loss: 0.4251
2025-04-05 03:07:29,032 - INFO - Epoch [6/10], Batch [190/374], Loss: 0.3490
2025-04-05 03:08:28,588 - INFO - Epoch [6/10], Batch [200/374], Loss: 0.2633
2025-04-05 03:09:28,077 - INFO - Epoch [6/10], Batch [210/374], Loss: 0.3631
2025-04-05 03:10:28,377 - INFO - Epoch [6/10], Batch [220/374], Loss: 0.3735
2025-04-05 03:11:29,990 - INFO - Epoch [6/10], Batch [230/374], Loss: 0.3458
2025-04-05 03:12:30,204 - INFO - Epoch [6/10], Batch [240/374], Loss: 0.6580
2025-04-05 03:13:30,084 - INFO - Epoch [6/10], Batch [250/374], Loss: 0.4076
2025-04-05 03:14:29,596 - INFO - Epoch [6/10], Batch [260/374], Loss: 0.3739
2025-04-05 03:15:28,679 - INFO - Epoch [6/10], Batch [270/374], Loss: 0.3590
2025-04-05 03:16:28,114 - INFO - Epoch [6/10], Batch [280/374], Loss: 0.3503
2025-04-05 03:17:28,540 - INFO - Epoch [6/10], Batch [290/374], Loss: 0.2801
2025-04-05 03:18:28,661 - INFO - Epoch [6/10], Batch [300/374], Loss: 0.4084
2025-04-05 03:19:29,978 - INFO - Epoch [6/10], Batch [310/374], Loss: 0.3660
2025-04-05 03:20:30,768 - INFO - Epoch [6/10], Batch [320/374], Loss: 0.4412
2025-04-05 03:21:31,245 - INFO - Epoch [6/10], Batch [330/374], Loss: 0.3437
2025-04-05 03:22:30,854 - INFO - Epoch [6/10], Batch [340/374], Loss: 0.3794
2025-04-05 03:23:30,258 - INFO - Epoch [6/10], Batch [350/374], Loss: 0.4519
2025-04-05 03:24:29,827 - INFO - Epoch [6/10], Batch [360/374], Loss: 0.6356
2025-04-05 03:25:30,160 - INFO - Epoch [6/10], Batch [370/374], Loss: 0.6562
2025-04-05 03:25:44,414 - INFO - Epoch [6/10] Train Loss: 0.4313, Train Accuracy: 79.60%
2025-04-05 03:28:22,184 - INFO - Epoch [6/10] Val Loss: 0.3677, Val Accuracy: 83.30%
2025-04-05 03:28:22,187 - INFO - New best model at epoch 6 with val accuracy: 83.30%
2025-04-05 03:28:29,692 - INFO - Epoch [7/10], Batch [0/374], Loss: 0.3157
2025-04-05 03:29:28,374 - INFO - Epoch [7/10], Batch [10/374], Loss: 0.4199
2025-04-05 03:30:27,449 - INFO - Epoch [7/10], Batch [20/374], Loss: 0.3712
2025-04-05 03:31:27,889 - INFO - Epoch [7/10], Batch [30/374], Loss: 0.3278
2025-04-05 03:32:28,301 - INFO - Epoch [7/10], Batch [40/374], Loss: 0.5406
2025-04-05 03:33:28,326 - INFO - Epoch [7/10], Batch [50/374], Loss: 0.3874
2025-04-05 03:34:27,425 - INFO - Epoch [7/10], Batch [60/374], Loss: 0.5013
2025-04-05 03:35:26,843 - INFO - Epoch [7/10], Batch [70/374], Loss: 0.3043
2025-04-05 03:36:26,547 - INFO - Epoch [7/10], Batch [80/374], Loss: 0.3626
2025-04-05 03:37:26,284 - INFO - Epoch [7/10], Batch [90/374], Loss: 0.2362
2025-04-05 03:38:25,306 - INFO - Epoch [7/10], Batch [100/374], Loss: 0.3892
2025-04-05 03:39:23,649 - INFO - Epoch [7/10], Batch [110/374], Loss: 0.2864
2025-04-05 03:40:23,196 - INFO - Epoch [7/10], Batch [120/374], Loss: 0.4717
2025-04-05 03:41:24,211 - INFO - Epoch [7/10], Batch [130/374], Loss: 0.3549
2025-04-05 03:42:24,346 - INFO - Epoch [7/10], Batch [140/374], Loss: 0.3138
2025-04-05 03:43:23,873 - INFO - Epoch [7/10], Batch [150/374], Loss: 0.1936
2025-04-05 03:44:24,799 - INFO - Epoch [7/10], Batch [160/374], Loss: 0.4222
2025-04-05 03:45:25,123 - INFO - Epoch [7/10], Batch [170/374], Loss: 0.2580
2025-04-05 03:46:25,278 - INFO - Epoch [7/10], Batch [180/374], Loss: 0.5260
2025-04-05 03:47:24,996 - INFO - Epoch [7/10], Batch [190/374], Loss: 0.4705
2025-04-05 03:48:25,090 - INFO - Epoch [7/10], Batch [200/374], Loss: 0.5239
2025-04-05 03:49:24,206 - INFO - Epoch [7/10], Batch [210/374], Loss: 0.2669
2025-04-05 03:50:23,831 - INFO - Epoch [7/10], Batch [220/374], Loss: 0.3522
2025-04-05 03:51:23,985 - INFO - Epoch [7/10], Batch [230/374], Loss: 0.3999
2025-04-05 03:52:24,014 - INFO - Epoch [7/10], Batch [240/374], Loss: 0.4328
2025-04-05 03:53:24,383 - INFO - Epoch [7/10], Batch [250/374], Loss: 0.3016
2025-04-05 03:54:23,745 - INFO - Epoch [7/10], Batch [260/374], Loss: 0.3115
2025-04-05 03:55:23,280 - INFO - Epoch [7/10], Batch [270/374], Loss: 0.5240
2025-04-05 03:56:23,394 - INFO - Epoch [7/10], Batch [280/374], Loss: 0.3429
2025-04-05 03:57:23,961 - INFO - Epoch [7/10], Batch [290/374], Loss: 0.2576
2025-04-05 03:58:23,720 - INFO - Epoch [7/10], Batch [300/374], Loss: 0.3403
2025-04-05 03:59:24,071 - INFO - Epoch [7/10], Batch [310/374], Loss: 0.4735
2025-04-05 04:00:23,715 - INFO - Epoch [7/10], Batch [320/374], Loss: 0.5547
2025-04-05 04:01:23,508 - INFO - Epoch [7/10], Batch [330/374], Loss: 0.3885
2025-04-05 04:02:23,470 - INFO - Epoch [7/10], Batch [340/374], Loss: 0.4274
2025-04-05 04:03:23,748 - INFO - Epoch [7/10], Batch [350/374], Loss: 0.4022
2025-04-05 04:04:24,329 - INFO - Epoch [7/10], Batch [360/374], Loss: 0.3632
2025-04-05 04:05:24,551 - INFO - Epoch [7/10], Batch [370/374], Loss: 0.4400
2025-04-05 04:05:39,197 - INFO - Epoch [7/10] Train Loss: 0.3998, Train Accuracy: 80.88%
2025-04-05 04:08:16,119 - INFO - Epoch [7/10] Val Loss: 0.3315, Val Accuracy: 86.26%
2025-04-05 04:08:16,121 - INFO - New best model at epoch 7 with val accuracy: 86.26%
2025-04-05 04:08:22,410 - INFO - Epoch [8/10], Batch [0/374], Loss: 0.5194
2025-04-05 04:09:22,001 - INFO - Epoch [8/10], Batch [10/374], Loss: 0.2643
2025-04-05 04:10:22,231 - INFO - Epoch [8/10], Batch [20/374], Loss: 0.4824
2025-04-05 04:11:22,868 - INFO - Epoch [8/10], Batch [30/374], Loss: 0.2722
2025-04-05 04:12:24,256 - INFO - Epoch [8/10], Batch [40/374], Loss: 0.5363
2025-04-05 04:13:24,823 - INFO - Epoch [8/10], Batch [50/374], Loss: 0.3480
2025-04-05 04:14:25,115 - INFO - Epoch [8/10], Batch [60/374], Loss: 0.2638
2025-04-05 04:15:25,673 - INFO - Epoch [8/10], Batch [70/374], Loss: 0.5155
2025-04-05 04:16:26,390 - INFO - Epoch [8/10], Batch [80/374], Loss: 0.2736
2025-04-05 04:17:26,862 - INFO - Epoch [8/10], Batch [90/374], Loss: 0.2929
2025-04-05 04:18:27,073 - INFO - Epoch [8/10], Batch [100/374], Loss: 0.4531
2025-04-05 04:19:26,812 - INFO - Epoch [8/10], Batch [110/374], Loss: 0.2055
2025-04-05 04:20:26,831 - INFO - Epoch [8/10], Batch [120/374], Loss: 0.2916
2025-04-05 04:21:27,000 - INFO - Epoch [8/10], Batch [130/374], Loss: 0.2457
2025-04-05 04:22:26,674 - INFO - Epoch [8/10], Batch [140/374], Loss: 0.4293
2025-04-05 04:23:26,758 - INFO - Epoch [8/10], Batch [150/374], Loss: 0.5548
2025-04-05 04:24:26,970 - INFO - Epoch [8/10], Batch [160/374], Loss: 0.8350
2025-04-05 04:25:27,650 - INFO - Epoch [8/10], Batch [170/374], Loss: 0.3342
2025-04-05 04:26:27,596 - INFO - Epoch [8/10], Batch [180/374], Loss: 0.5074
2025-04-05 04:27:27,249 - INFO - Epoch [8/10], Batch [190/374], Loss: 0.5849
2025-04-05 04:28:27,825 - INFO - Epoch [8/10], Batch [200/374], Loss: 0.3430
2025-04-05 04:29:28,810 - INFO - Epoch [8/10], Batch [210/374], Loss: 0.4343
2025-04-05 04:30:29,491 - INFO - Epoch [8/10], Batch [220/374], Loss: 0.2346
2025-04-05 04:31:29,426 - INFO - Epoch [8/10], Batch [230/374], Loss: 0.3009
2025-04-05 04:32:29,361 - INFO - Epoch [8/10], Batch [240/374], Loss: 0.2136
2025-04-05 04:33:29,161 - INFO - Epoch [8/10], Batch [250/374], Loss: 0.2556
2025-04-05 04:34:29,340 - INFO - Epoch [8/10], Batch [260/374], Loss: 0.3363
2025-04-05 04:35:29,182 - INFO - Epoch [8/10], Batch [270/374], Loss: 0.2714
2025-04-05 04:36:28,580 - INFO - Epoch [8/10], Batch [280/374], Loss: 0.3963
2025-04-05 04:37:28,586 - INFO - Epoch [8/10], Batch [290/374], Loss: 0.5050
2025-04-05 04:38:28,938 - INFO - Epoch [8/10], Batch [300/374], Loss: 0.4061
2025-04-05 04:39:29,304 - INFO - Epoch [8/10], Batch [310/374], Loss: 0.3124
2025-04-05 04:40:29,767 - INFO - Epoch [8/10], Batch [320/374], Loss: 0.4710
2025-04-05 04:41:30,011 - INFO - Epoch [8/10], Batch [330/374], Loss: 0.3110
2025-04-05 04:42:29,388 - INFO - Epoch [8/10], Batch [340/374], Loss: 0.3968
2025-04-05 04:43:28,007 - INFO - Epoch [8/10], Batch [350/374], Loss: 0.4467
2025-04-05 04:44:26,826 - INFO - Epoch [8/10], Batch [360/374], Loss: 0.3848
2025-04-05 04:45:26,004 - INFO - Epoch [8/10], Batch [370/374], Loss: 0.3024
2025-04-05 04:45:40,552 - INFO - Epoch [8/10] Train Loss: 0.3766, Train Accuracy: 82.41%
2025-04-05 04:48:20,733 - INFO - Epoch [8/10] Val Loss: 0.3051, Val Accuracy: 87.23%
2025-04-05 04:48:20,736 - INFO - New best model at epoch 8 with val accuracy: 87.23%
2025-04-05 04:48:26,899 - INFO - Epoch [9/10], Batch [0/374], Loss: 0.3785
2025-04-05 04:49:25,747 - INFO - Epoch [9/10], Batch [10/374], Loss: 0.5808
2025-04-05 04:50:24,842 - INFO - Epoch [9/10], Batch [20/374], Loss: 0.4333
2025-04-05 04:51:25,042 - INFO - Epoch [9/10], Batch [30/374], Loss: 0.4029
2025-04-05 04:52:24,391 - INFO - Epoch [9/10], Batch [40/374], Loss: 0.3554
2025-04-05 04:53:24,731 - INFO - Epoch [9/10], Batch [50/374], Loss: 0.5981
2025-04-05 04:54:24,141 - INFO - Epoch [9/10], Batch [60/374], Loss: 0.3751
2025-04-05 04:55:22,602 - INFO - Epoch [9/10], Batch [70/374], Loss: 0.3677
2025-04-05 04:56:21,664 - INFO - Epoch [9/10], Batch [80/374], Loss: 0.3527
2025-04-05 04:57:21,381 - INFO - Epoch [9/10], Batch [90/374], Loss: 0.4643
2025-04-05 04:58:20,267 - INFO - Epoch [9/10], Batch [100/374], Loss: 0.3786
2025-04-05 04:59:20,272 - INFO - Epoch [9/10], Batch [110/374], Loss: 0.2996
2025-04-05 05:00:19,945 - INFO - Epoch [9/10], Batch [120/374], Loss: 0.2513
2025-04-05 05:01:18,669 - INFO - Epoch [9/10], Batch [130/374], Loss: 0.3468
2025-04-05 05:02:17,427 - INFO - Epoch [9/10], Batch [140/374], Loss: 0.5048
2025-04-05 05:03:16,940 - INFO - Epoch [9/10], Batch [150/374], Loss: 0.3676
2025-04-05 05:04:16,246 - INFO - Epoch [9/10], Batch [160/374], Loss: 0.2697
2025-04-05 05:05:16,215 - INFO - Epoch [9/10], Batch [170/374], Loss: 0.2907
2025-04-05 05:06:15,407 - INFO - Epoch [9/10], Batch [180/374], Loss: 0.4027
2025-04-05 05:07:13,930 - INFO - Epoch [9/10], Batch [190/374], Loss: 0.4426
2025-04-05 05:08:14,332 - INFO - Epoch [9/10], Batch [200/374], Loss: 0.3461
2025-04-05 05:09:13,022 - INFO - Epoch [9/10], Batch [210/374], Loss: 0.3001
2025-04-05 05:10:11,810 - INFO - Epoch [9/10], Batch [220/374], Loss: 0.2646
2025-04-05 05:11:10,449 - INFO - Epoch [9/10], Batch [230/374], Loss: 0.2549
2025-04-05 05:12:08,799 - INFO - Epoch [9/10], Batch [240/374], Loss: 0.3691
2025-04-05 05:13:07,805 - INFO - Epoch [9/10], Batch [250/374], Loss: 0.2395
2025-04-05 05:14:07,337 - INFO - Epoch [9/10], Batch [260/374], Loss: 0.1828
2025-04-05 05:15:06,253 - INFO - Epoch [9/10], Batch [270/374], Loss: 0.2739
2025-04-05 05:16:06,139 - INFO - Epoch [9/10], Batch [280/374], Loss: 0.2831
2025-04-05 05:17:05,968 - INFO - Epoch [9/10], Batch [290/374], Loss: 0.4521
2025-04-05 05:18:04,475 - INFO - Epoch [9/10], Batch [300/374], Loss: 0.2657
2025-04-05 05:19:02,770 - INFO - Epoch [9/10], Batch [310/374], Loss: 0.3316
2025-04-05 05:20:01,959 - INFO - Epoch [9/10], Batch [320/374], Loss: 0.4136
2025-04-05 05:21:00,671 - INFO - Epoch [9/10], Batch [330/374], Loss: 0.3758
2025-04-05 05:21:58,907 - INFO - Epoch [9/10], Batch [340/374], Loss: 0.2961
2025-04-05 05:22:58,705 - INFO - Epoch [9/10], Batch [350/374], Loss: 0.5014
2025-04-05 05:23:59,059 - INFO - Epoch [9/10], Batch [360/374], Loss: 0.2819
2025-04-05 05:24:59,369 - INFO - Epoch [9/10], Batch [370/374], Loss: 0.2746
2025-04-05 05:25:13,994 - INFO - Epoch [9/10] Train Loss: 0.3652, Train Accuracy: 82.58%
2025-04-05 05:27:54,361 - INFO - Epoch [9/10] Val Loss: 0.2895, Val Accuracy: 87.44%
2025-04-05 05:27:54,364 - INFO - New best model at epoch 9 with val accuracy: 87.44%
2025-04-05 05:28:00,538 - INFO - Epoch [10/10], Batch [0/374], Loss: 0.3159
2025-04-05 05:29:00,279 - INFO - Epoch [10/10], Batch [10/374], Loss: 0.3688
2025-04-05 05:29:59,199 - INFO - Epoch [10/10], Batch [20/374], Loss: 0.5108
2025-04-05 05:30:57,775 - INFO - Epoch [10/10], Batch [30/374], Loss: 0.5457
2025-04-05 05:31:57,336 - INFO - Epoch [10/10], Batch [40/374], Loss: 0.2402
2025-04-05 05:32:57,166 - INFO - Epoch [10/10], Batch [50/374], Loss: 0.4275
2025-04-05 05:33:56,012 - INFO - Epoch [10/10], Batch [60/374], Loss: 0.1841
2025-04-05 05:34:54,521 - INFO - Epoch [10/10], Batch [70/374], Loss: 0.3313
2025-04-05 05:35:53,417 - INFO - Epoch [10/10], Batch [80/374], Loss: 0.3656
2025-04-05 05:36:53,530 - INFO - Epoch [10/10], Batch [90/374], Loss: 0.3270
2025-04-05 05:37:54,573 - INFO - Epoch [10/10], Batch [100/374], Loss: 0.4360
2025-04-05 05:38:55,226 - INFO - Epoch [10/10], Batch [110/374], Loss: 0.2535
2025-04-05 05:39:55,202 - INFO - Epoch [10/10], Batch [120/374], Loss: 0.4328
2025-04-05 05:40:55,705 - INFO - Epoch [10/10], Batch [130/374], Loss: 0.4192
2025-04-05 05:41:55,040 - INFO - Epoch [10/10], Batch [140/374], Loss: 0.3508
2025-04-05 05:42:54,929 - INFO - Epoch [10/10], Batch [150/374], Loss: 0.2627
2025-04-05 05:43:55,335 - INFO - Epoch [10/10], Batch [160/374], Loss: 0.2525
2025-04-05 05:44:54,908 - INFO - Epoch [10/10], Batch [170/374], Loss: 0.2998
2025-04-05 05:45:54,073 - INFO - Epoch [10/10], Batch [180/374], Loss: 0.5579
2025-04-05 05:46:53,215 - INFO - Epoch [10/10], Batch [190/374], Loss: 0.3731
2025-04-05 05:47:53,301 - INFO - Epoch [10/10], Batch [200/374], Loss: 0.2728
2025-04-05 05:48:52,393 - INFO - Epoch [10/10], Batch [210/374], Loss: 0.3292
2025-04-05 05:49:52,194 - INFO - Epoch [10/10], Batch [220/374], Loss: 0.3550
2025-04-05 05:50:52,620 - INFO - Epoch [10/10], Batch [230/374], Loss: 0.2211
2025-04-05 05:51:52,105 - INFO - Epoch [10/10], Batch [240/374], Loss: 0.3406
2025-04-05 05:52:51,008 - INFO - Epoch [10/10], Batch [250/374], Loss: 0.5024
2025-04-05 05:53:49,682 - INFO - Epoch [10/10], Batch [260/374], Loss: 0.2766
2025-04-05 05:54:48,577 - INFO - Epoch [10/10], Batch [270/374], Loss: 0.3089
2025-04-05 05:55:47,613 - INFO - Epoch [10/10], Batch [280/374], Loss: 0.3441
2025-04-05 05:56:45,982 - INFO - Epoch [10/10], Batch [290/374], Loss: 0.2872
2025-04-05 05:57:44,532 - INFO - Epoch [10/10], Batch [300/374], Loss: 0.3045
2025-04-05 05:58:43,646 - INFO - Epoch [10/10], Batch [310/374], Loss: 0.4487
2025-04-05 05:59:42,656 - INFO - Epoch [10/10], Batch [320/374], Loss: 0.3065
2025-04-05 06:00:41,725 - INFO - Epoch [10/10], Batch [330/374], Loss: 0.5802
2025-04-05 06:01:41,025 - INFO - Epoch [10/10], Batch [340/374], Loss: 0.3658
2025-04-05 06:02:40,165 - INFO - Epoch [10/10], Batch [350/374], Loss: 0.2300
2025-04-05 06:03:40,529 - INFO - Epoch [10/10], Batch [360/374], Loss: 0.5752
2025-04-05 06:04:40,638 - INFO - Epoch [10/10], Batch [370/374], Loss: 0.4117
2025-04-05 06:04:55,019 - INFO - Epoch [10/10] Train Loss: 0.3576, Train Accuracy: 83.33%
2025-04-05 06:07:33,938 - INFO - Epoch [10/10] Val Loss: 0.2857, Val Accuracy: 87.72%
2025-04-05 06:07:33,941 - INFO - New best model at epoch 10 with val accuracy: 87.72%
2025-04-05 06:08:54,243 - INFO - Test Loss: 0.3119, Test Accuracy: 87.40%
2025-04-05 06:08:54,244 - INFO - 
===== Final Performance Results =====
2025-04-05 06:08:54,244 - INFO - {
    "world_size": 2,
    "train_time": 23916.26261806488,
    "avg_epoch_time": 2391.626261806488,
    "val_accuracy": 87.71746694502436,
    "test_accuracy": 87.40431454418928,
    "test_loss": 0.3118752727262992,
    "total_time": 23916.26261806488,
    "train_losses": [
        0.6493814360328235,
        0.6192248269487838,
        0.5893083884083723,
        0.5391946390928389,
        0.46946226711482303,
        0.4312671113960992,
        0.39975917053498006,
        0.3765572106334911,
        0.3652168217787973,
        0.35761043967482575
    ],
    "train_accuracies": [
        60.23098167210645,
        65.2188467654197,
        67.39476106787178,
        72.51652857979747,
        77.32027784751862,
        79.6049878650933,
        80.87706084191146,
        82.4085697547912,
        82.58431667922002,
        83.3291488827517
    ],
    "val_losses": [
        0.6207433361598993,
        0.5541805636161056,
        0.520727162819068,
        0.5256088383926147,
        0.6134759488417693,
        0.36767511815351167,
        0.3314648286783621,
        0.3050777815080137,
        0.2894544955874452,
        0.28565800866071933
    ],
    "val_accuracies": [
        69.90257480862908,
        74.28670842032011,
        75.81767571329158,
        76.82672233820459,
        70.18093249826026,
        83.29853862212944,
        86.25608907446068,
        87.2303409881698,
        87.43910925539318,
        87.71746694502436
    ],
    "hyperparameters": {
        "batch_size": 32,
        "num_epochs": 10,
        "initial_lr": 0.001,
        "weight_decay": 0.0001,
        "num_classes": 2,
        "save_model": true
    }
}
2025-04-05 06:08:54,425 - INFO - Model saved to models/training_using_cpus_2_best_model.pt
2025-04-05 06:08:55,287 - INFO - Loss plot saved as plots/training_using_cpus_2_loss.png
2025-04-05 06:08:55,368 - INFO - Accuracy plot saved as plots/training_using_cpus_2_accuracy.png
2025-04-05 06:08:55,373 - INFO - Runtime parameters saved as metrics/training_using_cpus_2_params.json
