2025-04-05 10:04:41,209 - INFO - [Rank 2] Passed initial barrier
2025-04-05 10:04:41,209 - INFO - [Rank 0] Passed initial barrier
2025-04-05 10:04:41,210 - INFO - [Rank 1] Passed initial barrier
2025-04-05 10:04:41,210 - INFO - [Rank 3] Passed initial barrier
2025-04-05 10:04:41,242 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 10:04:41,242 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 10:04:41,242 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 10:04:41,242 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 10:07:02,305 - INFO - [Rank 2] Data loaded.
2025-04-05 10:07:02,305 - INFO - [Rank 1] Data loaded.
2025-04-05 10:07:02,305 - INFO - [Rank 3] Data loaded.
2025-04-05 10:07:02,305 - INFO - [Rank 0] Data loaded.
2025-04-05 10:07:02,764 - INFO - Model architecture: MedicalCNN
2025-04-05 10:07:02,765 - INFO - Total layers: 129
2025-04-05 10:07:02,765 - INFO - Total parameters: 22,494,274
2025-04-05 10:07:05,832 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6736
2025-04-05 10:07:30,120 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.6960
2025-04-05 10:07:56,669 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.6995
2025-04-05 10:08:23,483 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.6632
2025-04-05 10:08:51,418 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.6176
2025-04-05 10:09:20,245 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6610
2025-04-05 10:09:45,590 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.5951
2025-04-05 10:10:12,233 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.7010
2025-04-05 10:10:37,551 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.5896
2025-04-05 10:11:02,956 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.7011
2025-04-05 10:11:28,274 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.7558
2025-04-05 10:11:55,628 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.6255
2025-04-05 10:12:23,316 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6252
2025-04-05 10:12:49,365 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.5757
2025-04-05 10:13:15,584 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5267
2025-04-05 10:13:42,887 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.5994
2025-04-05 10:14:11,408 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.6919
2025-04-05 10:14:39,386 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5411
2025-04-05 10:15:09,481 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.6586
2025-04-05 10:15:25,495 - INFO - Epoch [1/10] Train Loss: 0.6505, Train Accuracy: 60.50%
2025-04-05 10:15:59,797 - INFO - Epoch [1/10] Val Loss: 0.9453, Val Accuracy: 38.41%
2025-04-05 10:15:59,799 - INFO - New best model at epoch 1 with val accuracy: 38.41%
2025-04-05 10:16:02,366 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.6042
2025-04-05 10:16:32,088 - INFO - Epoch [2/10], Batch [10/187], Loss: 0.5819
2025-04-05 10:16:58,445 - INFO - Epoch [2/10], Batch [20/187], Loss: 0.5819
2025-04-05 10:17:25,848 - INFO - Epoch [2/10], Batch [30/187], Loss: 0.9564
2025-04-05 10:17:55,319 - INFO - Epoch [2/10], Batch [40/187], Loss: 0.5632
2025-04-05 10:18:24,953 - INFO - Epoch [2/10], Batch [50/187], Loss: 0.5499
2025-04-05 10:18:53,888 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.5459
2025-04-05 10:19:24,221 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.5732
2025-04-05 10:19:54,200 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.7055
2025-04-05 10:20:23,628 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.7548
2025-04-05 10:20:51,302 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.6173
2025-04-05 10:21:18,935 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.7175
2025-04-05 10:21:46,081 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.7353
2025-04-05 10:22:13,634 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.7678
2025-04-05 10:22:40,803 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.5898
2025-04-05 10:23:07,894 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.4875
2025-04-05 10:23:36,016 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.5698
2025-04-05 10:24:03,030 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.4381
2025-04-05 10:24:29,996 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.5565
2025-04-05 10:24:45,424 - INFO - Epoch [2/10] Train Loss: 0.5837, Train Accuracy: 69.32%
2025-04-05 10:25:16,834 - INFO - Epoch [2/10] Val Loss: 0.6865, Val Accuracy: 66.53%
2025-04-05 10:25:16,838 - INFO - New best model at epoch 2 with val accuracy: 66.53%
2025-04-05 10:25:19,207 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.5235
2025-04-05 10:25:46,842 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.5485
2025-04-05 10:26:14,958 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.4612
2025-04-05 10:26:42,370 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.4823
2025-04-05 10:27:09,578 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.4707
2025-04-05 10:27:36,501 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.4830
2025-04-05 10:28:03,949 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.5433
2025-04-05 10:28:31,121 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.4392
2025-04-05 10:28:57,904 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.4458
2025-04-05 10:29:27,638 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.4265
2025-04-05 10:29:55,187 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.6218
2025-04-05 10:30:24,426 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.6135
2025-04-05 10:30:53,929 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.3957
2025-04-05 10:31:22,658 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5792
2025-04-05 10:31:49,643 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.4300
2025-04-05 10:32:16,987 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.4790
2025-04-05 10:32:44,307 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.3319
2025-04-05 10:33:13,729 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.6419
2025-04-05 10:33:41,945 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.4950
2025-04-05 10:33:58,658 - INFO - Epoch [3/10] Train Loss: 0.5023, Train Accuracy: 74.33%
2025-04-05 10:34:31,049 - INFO - Epoch [3/10] Val Loss: 0.4638, Val Accuracy: 78.91%
2025-04-05 10:34:31,051 - INFO - New best model at epoch 3 with val accuracy: 78.91%
2025-04-05 10:34:33,170 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.5873
2025-04-05 10:35:01,777 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.3686
2025-04-05 10:35:28,667 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.6564
2025-04-05 10:35:56,665 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.5048
2025-04-05 10:36:24,586 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.5246
2025-04-05 10:36:50,671 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.4574
2025-04-05 10:37:16,587 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.5610
2025-04-05 10:37:42,088 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.4950
2025-04-05 10:38:07,761 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.4286
2025-04-05 10:38:33,646 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.5470
2025-04-05 10:38:59,304 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4211
2025-04-05 10:39:25,768 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.5620
2025-04-05 10:39:51,723 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.4180
2025-04-05 10:40:18,694 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.5286
2025-04-05 10:40:45,399 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.4591
2025-04-05 10:41:13,104 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.4203
2025-04-05 10:41:39,509 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.3575
2025-04-05 10:42:06,899 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.4826
2025-04-05 10:42:33,249 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.3718
2025-04-05 10:42:47,914 - INFO - Epoch [4/10] Train Loss: 0.4648, Train Accuracy: 76.95%
2025-04-05 10:43:19,704 - INFO - Epoch [4/10] Val Loss: 0.3935, Val Accuracy: 82.12%
2025-04-05 10:43:19,706 - INFO - New best model at epoch 4 with val accuracy: 82.12%
2025-04-05 10:43:22,068 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.4579
2025-04-05 10:43:51,028 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.4118
2025-04-05 10:44:16,138 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.4410
2025-04-05 10:44:41,785 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.5562
2025-04-05 10:45:07,118 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.4904
2025-04-05 10:45:32,396 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.3245
2025-04-05 10:45:57,667 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.4104
2025-04-05 10:46:23,123 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.2995
2025-04-05 10:46:48,280 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.3619
2025-04-05 10:47:14,186 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.5034
2025-04-05 10:47:39,538 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.5851
2025-04-05 10:48:04,817 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3743
2025-04-05 10:48:30,509 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.5379
2025-04-05 10:48:59,031 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.2565
2025-04-05 10:49:26,747 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.3809
2025-04-05 10:49:53,657 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.4814
2025-04-05 10:50:20,127 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.3214
2025-04-05 10:50:45,230 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.4620
2025-04-05 10:51:10,361 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.5065
2025-04-05 10:51:25,889 - INFO - Epoch [5/10] Train Loss: 0.4171, Train Accuracy: 80.52%
2025-04-05 10:51:56,521 - INFO - Epoch [5/10] Val Loss: 0.6658, Val Accuracy: 68.13%
2025-04-05 10:51:58,993 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.5331
2025-04-05 10:52:25,437 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.3997
2025-04-05 10:52:50,426 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.5771
2025-04-05 10:53:15,436 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.2825
2025-04-05 10:53:41,471 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2655
2025-04-05 10:54:08,075 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.4329
2025-04-05 10:54:34,308 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.4047
2025-04-05 10:55:00,882 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.4292
2025-04-05 10:55:26,071 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.3322
2025-04-05 10:55:51,221 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.3349
2025-04-05 10:56:17,395 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.4187
2025-04-05 10:56:42,124 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2591
2025-04-05 10:57:08,504 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.6772
2025-04-05 10:57:36,194 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.3323
2025-04-05 10:58:02,222 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.2905
2025-04-05 10:58:27,310 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.3476
2025-04-05 10:58:52,078 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.3968
2025-04-05 10:59:17,302 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.5130
2025-04-05 10:59:42,126 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.2581
2025-04-05 10:59:56,354 - INFO - Epoch [6/10] Train Loss: 0.3957, Train Accuracy: 81.54%
2025-04-05 11:00:25,946 - INFO - Epoch [6/10] Val Loss: 0.3423, Val Accuracy: 85.39%
2025-04-05 11:00:25,949 - INFO - New best model at epoch 6 with val accuracy: 85.39%
2025-04-05 11:00:28,100 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.2690
2025-04-05 11:00:54,186 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.4444
2025-04-05 11:01:20,499 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.3700
2025-04-05 11:01:45,838 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.3225
2025-04-05 11:02:10,726 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.3774
2025-04-05 11:02:35,421 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.3669
2025-04-05 11:03:00,298 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.5375
2025-04-05 11:03:25,130 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.4448
2025-04-05 11:03:49,852 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.2759
2025-04-05 11:04:14,832 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.2154
2025-04-05 11:04:40,482 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.2619
2025-04-05 11:05:07,283 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.3878
2025-04-05 11:05:34,334 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.4388
2025-04-05 11:05:58,665 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.3880
2025-04-05 11:06:24,296 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.7290
2025-04-05 11:06:48,824 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.4339
2025-04-05 11:07:16,251 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.3565
2025-04-05 11:07:42,164 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.5827
2025-04-05 11:08:07,191 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2893
2025-04-05 11:08:21,601 - INFO - Epoch [7/10] Train Loss: 0.3786, Train Accuracy: 82.36%
2025-04-05 11:08:52,119 - INFO - Epoch [7/10] Val Loss: 0.3187, Val Accuracy: 86.43%
2025-04-05 11:08:52,122 - INFO - New best model at epoch 7 with val accuracy: 86.43%
2025-04-05 11:08:54,446 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.5327
2025-04-05 11:09:19,302 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.3363
2025-04-05 11:09:46,465 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.4068
2025-04-05 11:10:12,541 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.2922
2025-04-05 11:10:37,715 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.2255
2025-04-05 11:11:02,618 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2074
2025-04-05 11:11:28,956 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.1788
2025-04-05 11:11:53,764 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.3627
2025-04-05 11:12:18,214 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.4443
2025-04-05 11:12:43,631 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.3957
2025-04-05 11:13:09,483 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.4693
2025-04-05 11:13:35,802 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.2260
2025-04-05 11:14:00,316 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.2315
2025-04-05 11:14:27,314 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.2679
2025-04-05 11:14:54,692 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.3325
2025-04-05 11:15:21,076 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.4254
2025-04-05 11:15:46,616 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.2989
2025-04-05 11:16:12,128 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3014
2025-04-05 11:16:37,721 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.4179
2025-04-05 11:16:52,046 - INFO - Epoch [8/10] Train Loss: 0.3512, Train Accuracy: 84.90%
2025-04-05 11:17:22,849 - INFO - Epoch [8/10] Val Loss: 0.2997, Val Accuracy: 87.68%
2025-04-05 11:17:22,851 - INFO - New best model at epoch 8 with val accuracy: 87.68%
2025-04-05 11:17:25,232 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.3056
2025-04-05 11:17:50,895 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.5170
2025-04-05 11:18:16,481 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.2284
2025-04-05 11:18:41,775 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.2670
2025-04-05 11:19:08,230 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.3882
2025-04-05 11:19:33,307 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2220
2025-04-05 11:19:58,315 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.1876
2025-04-05 11:20:25,130 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2815
2025-04-05 11:20:51,028 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.3761
2025-04-05 11:21:17,246 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.4351
2025-04-05 11:21:43,186 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4229
2025-04-05 11:22:10,047 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.2109
2025-04-05 11:22:35,105 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.3073
2025-04-05 11:23:00,600 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2375
2025-04-05 11:23:25,773 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.4208
2025-04-05 11:23:50,420 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.3381
2025-04-05 11:24:15,831 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.2962
2025-04-05 11:24:40,532 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.3341
2025-04-05 11:25:05,932 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.2887
2025-04-05 11:25:20,328 - INFO - Epoch [9/10] Train Loss: 0.3462, Train Accuracy: 84.42%
2025-04-05 11:25:51,867 - INFO - Epoch [9/10] Val Loss: 0.2774, Val Accuracy: 88.52%
2025-04-05 11:25:51,870 - INFO - New best model at epoch 9 with val accuracy: 88.52%
2025-04-05 11:25:54,082 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.4716
2025-04-05 11:26:19,792 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.5447
2025-04-05 11:26:47,312 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.3415
2025-04-05 11:27:15,285 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.3510
2025-04-05 11:27:42,604 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.2911
2025-04-05 11:28:08,854 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2718
2025-04-05 11:28:33,708 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2020
2025-04-05 11:28:59,121 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.3346
2025-04-05 11:29:25,074 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.3839
2025-04-05 11:29:50,806 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.4021
2025-04-05 11:30:15,918 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.1686
2025-04-05 11:30:40,994 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.3425
2025-04-05 11:31:07,090 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.2757
2025-04-05 11:31:33,857 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.2780
2025-04-05 11:32:00,018 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.5005
2025-04-05 11:32:25,189 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.4681
2025-04-05 11:32:50,188 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.2348
2025-04-05 11:33:15,016 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.3822
2025-04-05 11:33:39,390 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.2440
2025-04-05 11:33:53,964 - INFO - Epoch [10/10] Train Loss: 0.3297, Train Accuracy: 84.72%
2025-04-05 11:34:24,130 - INFO - Epoch [10/10] Val Loss: 0.2705, Val Accuracy: 89.00%
2025-04-05 11:34:24,136 - INFO - New best model at epoch 10 with val accuracy: 89.00%
2025-04-05 11:34:39,299 - INFO - Test Loss: 0.3032, Test Accuracy: 86.23%
2025-04-05 11:34:39,300 - INFO - 
===== Final Performance Results =====
2025-04-05 11:34:39,304 - INFO - {
    "world_size": 4,
    "train_time": 5241.370862722397,
    "avg_epoch_time": 524.1370862722397,
    "val_accuracy": 89.00487125956855,
    "test_accuracy": 86.23087621696801,
    "test_loss": 0.30321821790079745,
    "total_time": 5241.370862722397,
    "train_losses": [
        0.6505199718974125,
        0.5836584987899749,
        0.5023062559051993,
        0.4647915743484657,
        0.4171338688479308,
        0.39572005758724454,
        0.37862422605937496,
        0.35116285854054297,
        0.3461937021610627,
        0.32969347835834056
    ],
    "train_accuracies": [
        60.50209205020921,
        69.32217573221757,
        74.32635983263599,
        76.9539748953975,
        80.51882845188284,
        81.5397489539749,
        82.35983263598327,
        84.90376569037657,
        84.418410041841,
        84.71966527196653
    ],
    "val_losses": [
        0.9453251381756618,
        0.6865380228229422,
        0.46379785108002175,
        0.39349500952118704,
        0.6658126527680733,
        0.3422745106613462,
        0.31869609155170436,
        0.2997387532318476,
        0.2774499800774316,
        0.2704863183267762
    ],
    "val_accuracies": [
        38.413361169102295,
        66.52748782185108,
        78.91440501043841,
        82.11551844119694,
        68.12804453723034,
        85.38622129436325,
        86.43006263048017,
        87.68267223382045,
        88.51774530271399,
        89.00487125956855
    ],
    "hyperparameters": {
        "batch_size": 32,
        "num_epochs": 10,
        "initial_lr": 0.001,
        "weight_decay": 0.0001,
        "num_classes": 2,
        "save_model": true
    }
}
2025-04-05 11:34:39,527 - INFO - Model saved to models/training_using_cpus_4_best_model.pt
2025-04-05 11:34:42,652 - INFO - Loss plot saved as plots/training_using_cpus_4_loss.png
2025-04-05 11:34:42,740 - INFO - Accuracy plot saved as plots/training_using_cpus_4_accuracy.png
2025-04-05 11:34:42,748 - INFO - Runtime parameters saved as metrics/training_using_cpus_4_params.json
