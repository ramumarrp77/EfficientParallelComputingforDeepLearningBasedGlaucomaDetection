2025-04-05 13:04:37,670 - INFO - [Rank 1] Passed initial barrier
2025-04-05 13:04:37,670 - INFO - [Rank 2] Passed initial barrier
2025-04-05 13:04:37,670 - INFO - [Rank 3] Passed initial barrier
2025-04-05 13:04:37,671 - INFO - [Rank 0] Passed initial barrier
2025-04-05 13:04:37,701 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 13:04:37,701 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 13:04:37,701 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 13:04:37,701 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 13:07:15,409 - INFO - [Rank 0] Data loaded.
2025-04-05 13:07:15,410 - INFO - [Rank 1] Data loaded.
2025-04-05 13:07:15,410 - INFO - [Rank 2] Data loaded.
2025-04-05 13:07:15,409 - INFO - [Rank 3] Data loaded.
2025-04-05 13:07:15,937 - INFO - Model architecture: MedicalCNN
2025-04-05 13:07:15,937 - INFO - Total layers: 129
2025-04-05 13:07:15,937 - INFO - Total parameters: 22,494,274
2025-04-05 13:07:18,583 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6736
2025-04-05 13:07:37,408 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.6999
2025-04-05 13:07:56,596 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.6082
2025-04-05 13:08:15,084 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.6426
2025-04-05 13:08:33,504 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.6250
2025-04-05 13:08:52,332 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6926
2025-04-05 13:09:10,638 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.5806
2025-04-05 13:09:29,407 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.6439
2025-04-05 13:09:48,026 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.6302
2025-04-05 13:10:06,637 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.7879
2025-04-05 13:10:25,579 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.6981
2025-04-05 13:10:44,539 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.5933
2025-04-05 13:11:03,216 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6580
2025-04-05 13:11:22,284 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.5929
2025-04-05 13:11:41,028 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5200
2025-04-05 13:12:00,474 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.6126
2025-04-05 13:12:19,052 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.6818
2025-04-05 13:12:37,325 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5544
2025-04-05 13:12:56,097 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.6673
2025-04-05 13:13:06,667 - INFO - Epoch [1/10] Train Loss: 0.6480, Train Accuracy: 60.72%
2025-04-05 13:13:30,470 - INFO - Epoch [1/10] Val Loss: 0.6516, Val Accuracy: 71.12%
2025-04-05 13:13:30,472 - INFO - New best model at epoch 1 with val accuracy: 71.12%
2025-04-05 13:13:34,823 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.6042
2025-04-05 13:13:55,152 - INFO - Epoch [2/10], Batch [10/187], Loss: 0.5961
2025-04-05 13:14:14,575 - INFO - Epoch [2/10], Batch [20/187], Loss: 0.6672
2025-04-05 13:14:34,134 - INFO - Epoch [2/10], Batch [30/187], Loss: 0.9194
2025-04-05 13:14:53,695 - INFO - Epoch [2/10], Batch [40/187], Loss: 0.5115
2025-04-05 13:15:13,345 - INFO - Epoch [2/10], Batch [50/187], Loss: 0.5597
2025-04-05 13:15:32,962 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.5231
2025-04-05 13:15:52,474 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.6181
2025-04-05 13:16:12,211 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.7018
2025-04-05 13:16:31,573 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.6552
2025-04-05 13:16:51,153 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.6870
2025-04-05 13:17:10,680 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.6485
2025-04-05 13:17:30,062 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.7429
2025-04-05 13:17:49,154 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.6781
2025-04-05 13:18:08,719 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.6570
2025-04-05 13:18:27,871 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.6513
2025-04-05 13:18:47,376 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.5759
2025-04-05 13:19:06,821 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.5777
2025-04-05 13:19:26,901 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.5911
2025-04-05 13:19:38,210 - INFO - Epoch [2/10] Train Loss: 0.6168, Train Accuracy: 65.66%
2025-04-05 13:19:59,742 - INFO - Epoch [2/10] Val Loss: 0.5514, Val Accuracy: 73.56%
2025-04-05 13:19:59,748 - INFO - New best model at epoch 2 with val accuracy: 73.56%
2025-04-05 13:20:03,524 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.6305
2025-04-05 13:20:23,185 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.4891
2025-04-05 13:20:42,569 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.4230
2025-04-05 13:21:02,200 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.6113
2025-04-05 13:21:21,410 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.6040
2025-04-05 13:21:40,664 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.7193
2025-04-05 13:21:59,767 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.5959
2025-04-05 13:22:18,744 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.5115
2025-04-05 13:22:37,776 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.5750
2025-04-05 13:22:57,005 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.5159
2025-04-05 13:23:16,237 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.5478
2025-04-05 13:23:35,635 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.6841
2025-04-05 13:23:54,798 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.5758
2025-04-05 13:24:13,945 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5359
2025-04-05 13:24:33,070 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.4984
2025-04-05 13:24:51,905 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.5148
2025-04-05 13:25:10,928 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.3611
2025-04-05 13:25:29,850 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.6698
2025-04-05 13:25:49,574 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.5433
2025-04-05 13:26:00,232 - INFO - Epoch [3/10] Train Loss: 0.5688, Train Accuracy: 68.52%
2025-04-05 13:26:22,477 - INFO - Epoch [3/10] Val Loss: 0.5485, Val Accuracy: 72.93%
2025-04-05 13:26:24,716 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.7428
2025-04-05 13:26:44,062 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.4231
2025-04-05 13:27:02,814 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.7030
2025-04-05 13:27:22,126 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.4814
2025-04-05 13:27:41,509 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.6609
2025-04-05 13:28:00,910 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.5483
2025-04-05 13:28:20,061 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.5223
2025-04-05 13:28:38,894 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.4221
2025-04-05 13:28:58,165 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.4792
2025-04-05 13:29:17,384 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.5922
2025-04-05 13:29:36,442 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4814
2025-04-05 13:29:55,216 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.5485
2025-04-05 13:30:14,149 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.6486
2025-04-05 13:30:32,774 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.5250
2025-04-05 13:30:51,176 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.4625
2025-04-05 13:31:09,666 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.3973
2025-04-05 13:31:28,504 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.5001
2025-04-05 13:31:47,632 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.5781
2025-04-05 13:32:06,041 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.4010
2025-04-05 13:32:16,448 - INFO - Epoch [4/10] Train Loss: 0.5166, Train Accuracy: 74.06%
2025-04-05 13:32:38,951 - INFO - Epoch [4/10] Val Loss: 0.4717, Val Accuracy: 77.59%
2025-04-05 13:32:38,953 - INFO - New best model at epoch 4 with val accuracy: 77.59%
2025-04-05 13:32:40,924 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.4531
2025-04-05 13:32:59,487 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.4856
2025-04-05 13:33:18,121 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.5225
2025-04-05 13:33:37,572 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.6192
2025-04-05 13:33:56,416 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.5688
2025-04-05 13:34:15,411 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.3799
2025-04-05 13:34:34,372 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.3915
2025-04-05 13:34:53,051 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.3408
2025-04-05 13:35:12,249 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.3736
2025-04-05 13:35:31,228 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.4591
2025-04-05 13:35:50,508 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.5327
2025-04-05 13:36:09,203 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3012
2025-04-05 13:36:27,993 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.6030
2025-04-05 13:36:46,493 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.2930
2025-04-05 13:37:05,702 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.4561
2025-04-05 13:37:24,585 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.4433
2025-04-05 13:37:43,519 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.3150
2025-04-05 13:38:02,595 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.4944
2025-04-05 13:38:21,200 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.4882
2025-04-05 13:38:31,791 - INFO - Epoch [5/10] Train Loss: 0.4533, Train Accuracy: 78.23%
2025-04-05 13:38:52,699 - INFO - Epoch [5/10] Val Loss: 0.3907, Val Accuracy: 83.16%
2025-04-05 13:38:52,702 - INFO - New best model at epoch 5 with val accuracy: 83.16%
2025-04-05 13:38:56,661 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.5351
2025-04-05 13:39:15,934 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.5164
2025-04-05 13:39:34,599 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.5512
2025-04-05 13:39:53,817 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.3445
2025-04-05 13:40:12,405 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2843
2025-04-05 13:40:31,283 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.5171
2025-04-05 13:40:50,491 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.4341
2025-04-05 13:41:09,210 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.5765
2025-04-05 13:41:28,130 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.3788
2025-04-05 13:41:46,693 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.3437
2025-04-05 13:42:05,643 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.4119
2025-04-05 13:42:24,510 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2592
2025-04-05 13:42:43,475 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.7175
2025-04-05 13:43:03,016 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.3043
2025-04-05 13:43:21,708 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.3276
2025-04-05 13:43:40,437 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.3404
2025-04-05 13:43:59,359 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.3855
2025-04-05 13:44:18,562 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.5263
2025-04-05 13:44:37,601 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.3292
2025-04-05 13:44:48,176 - INFO - Epoch [6/10] Train Loss: 0.4174, Train Accuracy: 80.55%
2025-04-05 13:45:10,239 - INFO - Epoch [6/10] Val Loss: 0.3526, Val Accuracy: 85.66%
2025-04-05 13:45:10,241 - INFO - New best model at epoch 6 with val accuracy: 85.66%
2025-04-05 13:45:12,894 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.2910
2025-04-05 13:45:32,186 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.4296
2025-04-05 13:45:50,969 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.4726
2025-04-05 13:46:09,897 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.3646
2025-04-05 13:46:29,393 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.3139
2025-04-05 13:46:48,733 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.3137
2025-04-05 13:47:07,372 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.4608
2025-04-05 13:47:26,247 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.4610
2025-04-05 13:47:45,316 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.3269
2025-04-05 13:48:04,486 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.2732
2025-04-05 13:48:23,560 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.4034
2025-04-05 13:48:42,798 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.3849
2025-04-05 13:49:01,563 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.6072
2025-04-05 13:49:20,161 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.4122
2025-04-05 13:49:39,783 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.7370
2025-04-05 13:49:59,096 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.5006
2025-04-05 13:50:18,206 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.3465
2025-04-05 13:50:37,410 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.5155
2025-04-05 13:50:56,533 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2652
2025-04-05 13:51:07,175 - INFO - Epoch [7/10] Train Loss: 0.4021, Train Accuracy: 80.75%
2025-04-05 13:51:28,527 - INFO - Epoch [7/10] Val Loss: 0.3130, Val Accuracy: 87.47%
2025-04-05 13:51:28,529 - INFO - New best model at epoch 7 with val accuracy: 87.47%
2025-04-05 13:51:31,552 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.3947
2025-04-05 13:51:51,086 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.4126
2025-04-05 13:52:10,304 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.4223
2025-04-05 13:52:29,047 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.2661
2025-04-05 13:52:47,832 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.2401
2025-04-05 13:53:07,056 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2453
2025-04-05 13:53:26,045 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.2190
2025-04-05 13:53:44,642 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.3870
2025-04-05 13:54:03,877 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.5868
2025-04-05 13:54:23,351 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.3896
2025-04-05 13:54:42,182 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.4902
2025-04-05 13:55:01,247 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.3100
2025-04-05 13:55:19,997 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.3089
2025-04-05 13:55:38,946 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.2916
2025-04-05 13:55:58,171 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.3045
2025-04-05 13:56:16,832 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.4702
2025-04-05 13:56:35,806 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.3035
2025-04-05 13:56:54,424 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3893
2025-04-05 13:57:12,879 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.4710
2025-04-05 13:57:23,743 - INFO - Epoch [8/10] Train Loss: 0.3664, Train Accuracy: 82.59%
2025-04-05 13:57:45,298 - INFO - Epoch [8/10] Val Loss: 0.3201, Val Accuracy: 86.43%
2025-04-05 13:57:48,907 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.3455
2025-04-05 13:58:07,918 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.3530
2025-04-05 13:58:27,391 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.2239
2025-04-05 13:58:46,330 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.2867
2025-04-05 13:59:05,627 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.3717
2025-04-05 13:59:24,370 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2702
2025-04-05 13:59:43,434 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.1971
2025-04-05 14:00:02,112 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2347
2025-04-05 14:00:20,811 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.2225
2025-04-05 14:00:40,092 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.4022
2025-04-05 14:00:59,183 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4400
2025-04-05 14:01:17,899 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.2079
2025-04-05 14:01:36,898 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.2961
2025-04-05 14:01:55,945 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2368
2025-04-05 14:02:14,902 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.4061
2025-04-05 14:02:33,612 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.3761
2025-04-05 14:02:52,488 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.3470
2025-04-05 14:03:11,290 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.3082
2025-04-05 14:03:30,189 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.2836
2025-04-05 14:03:41,016 - INFO - Epoch [9/10] Train Loss: 0.3561, Train Accuracy: 83.97%
2025-04-05 14:04:03,733 - INFO - Epoch [9/10] Val Loss: 0.2906, Val Accuracy: 88.31%
2025-04-05 14:04:03,736 - INFO - New best model at epoch 9 with val accuracy: 88.31%
2025-04-05 14:04:05,707 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.4293
2025-04-05 14:04:24,529 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.4122
2025-04-05 14:04:43,205 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.3343
2025-04-05 14:05:02,275 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.2833
2025-04-05 14:05:21,059 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.3907
2025-04-05 14:05:39,873 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2313
2025-04-05 14:05:58,260 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2583
2025-04-05 14:06:17,123 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.3629
2025-04-05 14:06:36,568 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.4458
2025-04-05 14:06:55,298 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.4151
2025-04-05 14:07:14,127 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.1837
2025-04-05 14:07:33,433 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.3502
2025-04-05 14:07:52,756 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.3546
2025-04-05 14:08:12,200 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.2384
2025-04-05 14:08:31,053 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.5664
2025-04-05 14:08:50,251 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.3768
2025-04-05 14:09:09,844 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.2813
2025-04-05 14:09:29,492 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.3180
2025-04-05 14:09:48,703 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.2639
2025-04-05 14:09:59,382 - INFO - Epoch [10/10] Train Loss: 0.3414, Train Accuracy: 83.75%
2025-04-05 14:10:21,665 - INFO - Epoch [10/10] Val Loss: 0.2775, Val Accuracy: 88.31%
2025-04-05 14:10:34,703 - INFO - Test Loss: 0.3109, Test Accuracy: 86.65%
2025-04-05 14:10:34,703 - INFO - 
===== Final Performance Results =====
2025-04-05 14:10:34,704 - INFO - {
    "world_size": 4,
    "train_time": 3785.717926502228,
    "avg_epoch_time": 378.5717926502228,
    "val_accuracy": 88.3089770354906,
    "test_accuracy": 86.6481223922114,
    "test_loss": 0.31092317412725246,
    "total_time": 3785.717926502228,
    "train_losses": [
        0.6480397883518969,
        0.6167628019923446,
        0.568768892348062,
        0.5166494257679545,
        0.4532592488931313,
        0.4173902630756091,
        0.40206088277086555,
        0.36639074012076006,
        0.3560855256363937,
        0.3413888303654962
    ],
    "train_accuracies": [
        60.71966527196653,
        65.65690376569037,
        68.51882845188284,
        74.05857740585775,
        78.22594142259415,
        80.55230125523012,
        80.75313807531381,
        82.59414225941423,
        83.96652719665272,
        83.7489539748954
    ],
    "val_losses": [
        0.6515855416292603,
        0.551400463450013,
        0.548451015911089,
        0.4716679027035739,
        0.3907001546960617,
        0.3525747205950605,
        0.31295053048024346,
        0.3200962401630983,
        0.2905889655370719,
        0.27752987315609956
    ],
    "val_accuracies": [
        71.12038970076549,
        73.55601948503828,
        72.92971468336813,
        77.59220598469032,
        83.15935977731385,
        85.66457898399443,
        87.47390396659708,
        86.43006263048017,
        88.3089770354906,
        88.3089770354906
    ],
    "hyperparameters": {
        "batch_size": 32,
        "num_epochs": 10,
        "initial_lr": 0.001,
        "weight_decay": 0.0001,
        "num_classes": 2,
        "save_model": true
    }
}
2025-04-05 14:10:34,897 - INFO - Model saved to models/training_using_cpus_4_best_model.pt
2025-04-05 14:10:37,876 - INFO - Loss plot saved as plots/training_using_cpus_4_loss.png
2025-04-05 14:10:37,964 - INFO - Accuracy plot saved as plots/training_using_cpus_4_accuracy.png
2025-04-05 14:10:37,971 - INFO - Runtime parameters saved as metrics/training_using_cpus_4_params.json
