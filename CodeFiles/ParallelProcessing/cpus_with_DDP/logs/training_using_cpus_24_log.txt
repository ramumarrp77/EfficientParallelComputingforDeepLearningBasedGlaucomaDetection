2025-04-05 11:37:44,192 - INFO - [Rank 1] Passed initial barrier
2025-04-05 11:37:44,192 - INFO - [Rank 0] Passed initial barrier
2025-04-05 11:37:44,192 - INFO - [Rank 2] Passed initial barrier
2025-04-05 11:37:44,192 - INFO - [Rank 3] Passed initial barrier
2025-04-05 11:37:44,222 - INFO - [Rank 0] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 11:37:44,222 - INFO - [Rank 1] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 11:37:44,223 - INFO - [Rank 3] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 11:37:44,223 - INFO - [Rank 2] Loading data from ../../preprocessed_glaucoma_data...
2025-04-05 11:40:36,967 - INFO - [Rank 0] Data loaded.
2025-04-05 11:40:36,968 - INFO - [Rank 2] Data loaded.
2025-04-05 11:40:36,968 - INFO - [Rank 3] Data loaded.
2025-04-05 11:40:36,967 - INFO - [Rank 1] Data loaded.
2025-04-05 11:40:37,729 - INFO - Model architecture: MedicalCNN
2025-04-05 11:40:37,730 - INFO - Total layers: 129
2025-04-05 11:40:37,731 - INFO - Total parameters: 22,494,274
2025-04-05 11:40:41,347 - INFO - Epoch [1/10], Batch [0/187], Loss: 0.6736
2025-04-05 11:41:01,319 - INFO - Epoch [1/10], Batch [10/187], Loss: 0.7005
2025-04-05 11:41:21,203 - INFO - Epoch [1/10], Batch [20/187], Loss: 0.6079
2025-04-05 11:41:40,467 - INFO - Epoch [1/10], Batch [30/187], Loss: 0.6025
2025-04-05 11:41:59,796 - INFO - Epoch [1/10], Batch [40/187], Loss: 0.6276
2025-04-05 11:42:19,420 - INFO - Epoch [1/10], Batch [50/187], Loss: 0.6353
2025-04-05 11:42:38,829 - INFO - Epoch [1/10], Batch [60/187], Loss: 0.5886
2025-04-05 11:42:58,033 - INFO - Epoch [1/10], Batch [70/187], Loss: 0.6878
2025-04-05 11:43:17,407 - INFO - Epoch [1/10], Batch [80/187], Loss: 0.5948
2025-04-05 11:43:36,995 - INFO - Epoch [1/10], Batch [90/187], Loss: 0.7354
2025-04-05 11:43:56,525 - INFO - Epoch [1/10], Batch [100/187], Loss: 0.6904
2025-04-05 11:44:15,916 - INFO - Epoch [1/10], Batch [110/187], Loss: 0.6188
2025-04-05 11:44:35,113 - INFO - Epoch [1/10], Batch [120/187], Loss: 0.6505
2025-04-05 11:44:54,243 - INFO - Epoch [1/10], Batch [130/187], Loss: 0.5898
2025-04-05 11:45:13,695 - INFO - Epoch [1/10], Batch [140/187], Loss: 0.5169
2025-04-05 11:45:33,101 - INFO - Epoch [1/10], Batch [150/187], Loss: 0.6117
2025-04-05 11:45:52,351 - INFO - Epoch [1/10], Batch [160/187], Loss: 0.6701
2025-04-05 11:46:11,515 - INFO - Epoch [1/10], Batch [170/187], Loss: 0.5238
2025-04-05 11:46:30,827 - INFO - Epoch [1/10], Batch [180/187], Loss: 0.6703
2025-04-05 11:46:41,689 - INFO - Epoch [1/10] Train Loss: 0.6479, Train Accuracy: 60.32%
2025-04-05 11:47:09,163 - INFO - Epoch [1/10] Val Loss: 0.6949, Val Accuracy: 58.94%
2025-04-05 11:47:09,170 - INFO - New best model at epoch 1 with val accuracy: 58.94%
2025-04-05 11:47:13,331 - INFO - Epoch [2/10], Batch [0/187], Loss: 0.6649
2025-04-05 11:47:38,305 - INFO - Epoch [2/10], Batch [10/187], Loss: 0.5264
2025-04-05 11:48:03,564 - INFO - Epoch [2/10], Batch [20/187], Loss: 0.6094
2025-04-05 11:48:27,944 - INFO - Epoch [2/10], Batch [30/187], Loss: 0.8688
2025-04-05 11:48:52,518 - INFO - Epoch [2/10], Batch [40/187], Loss: 0.6223
2025-04-05 11:49:16,531 - INFO - Epoch [2/10], Batch [50/187], Loss: 0.6252
2025-04-05 11:49:41,185 - INFO - Epoch [2/10], Batch [60/187], Loss: 0.6135
2025-04-05 11:50:05,647 - INFO - Epoch [2/10], Batch [70/187], Loss: 0.5057
2025-04-05 11:50:29,834 - INFO - Epoch [2/10], Batch [80/187], Loss: 0.6750
2025-04-05 11:50:53,988 - INFO - Epoch [2/10], Batch [90/187], Loss: 0.7278
2025-04-05 11:51:18,452 - INFO - Epoch [2/10], Batch [100/187], Loss: 0.5996
2025-04-05 11:51:43,225 - INFO - Epoch [2/10], Batch [110/187], Loss: 0.6514
2025-04-05 11:52:07,547 - INFO - Epoch [2/10], Batch [120/187], Loss: 0.6946
2025-04-05 11:52:32,594 - INFO - Epoch [2/10], Batch [130/187], Loss: 0.7043
2025-04-05 11:52:57,115 - INFO - Epoch [2/10], Batch [140/187], Loss: 0.5748
2025-04-05 11:53:25,003 - INFO - Epoch [2/10], Batch [150/187], Loss: 0.4560
2025-04-05 11:53:51,838 - INFO - Epoch [2/10], Batch [160/187], Loss: 0.6610
2025-04-05 11:54:17,890 - INFO - Epoch [2/10], Batch [170/187], Loss: 0.4651
2025-04-05 11:54:42,743 - INFO - Epoch [2/10], Batch [180/187], Loss: 0.5111
2025-04-05 11:54:57,043 - INFO - Epoch [2/10] Train Loss: 0.6091, Train Accuracy: 66.61%
2025-04-05 11:55:25,634 - INFO - Epoch [2/10] Val Loss: 0.5167, Val Accuracy: 77.04%
2025-04-05 11:55:25,638 - INFO - New best model at epoch 2 with val accuracy: 77.04%
2025-04-05 11:55:28,731 - INFO - Epoch [3/10], Batch [0/187], Loss: 0.6582
2025-04-05 11:55:54,311 - INFO - Epoch [3/10], Batch [10/187], Loss: 0.5283
2025-04-05 11:56:20,528 - INFO - Epoch [3/10], Batch [20/187], Loss: 0.4841
2025-04-05 11:56:45,078 - INFO - Epoch [3/10], Batch [30/187], Loss: 0.5470
2025-04-05 11:57:10,423 - INFO - Epoch [3/10], Batch [40/187], Loss: 0.4893
2025-04-05 11:57:37,840 - INFO - Epoch [3/10], Batch [50/187], Loss: 0.6112
2025-04-05 11:58:05,681 - INFO - Epoch [3/10], Batch [60/187], Loss: 0.5283
2025-04-05 11:58:31,107 - INFO - Epoch [3/10], Batch [70/187], Loss: 0.4525
2025-04-05 11:58:57,728 - INFO - Epoch [3/10], Batch [80/187], Loss: 0.4934
2025-04-05 11:59:23,675 - INFO - Epoch [3/10], Batch [90/187], Loss: 0.4958
2025-04-05 11:59:49,442 - INFO - Epoch [3/10], Batch [100/187], Loss: 0.5620
2025-04-05 12:00:14,751 - INFO - Epoch [3/10], Batch [110/187], Loss: 0.7322
2025-04-05 12:00:39,982 - INFO - Epoch [3/10], Batch [120/187], Loss: 0.5042
2025-04-05 12:01:05,425 - INFO - Epoch [3/10], Batch [130/187], Loss: 0.5418
2025-04-05 12:01:31,650 - INFO - Epoch [3/10], Batch [140/187], Loss: 0.4245
2025-04-05 12:01:57,063 - INFO - Epoch [3/10], Batch [150/187], Loss: 0.5552
2025-04-05 12:02:22,277 - INFO - Epoch [3/10], Batch [160/187], Loss: 0.3747
2025-04-05 12:02:47,697 - INFO - Epoch [3/10], Batch [170/187], Loss: 0.5642
2025-04-05 12:03:13,307 - INFO - Epoch [3/10], Batch [180/187], Loss: 0.5757
2025-04-05 12:03:27,659 - INFO - Epoch [3/10] Train Loss: 0.5198, Train Accuracy: 73.19%
2025-04-05 12:03:54,538 - INFO - Epoch [3/10] Val Loss: 0.5230, Val Accuracy: 75.78%
2025-04-05 12:03:58,920 - INFO - Epoch [4/10], Batch [0/187], Loss: 0.6405
2025-04-05 12:04:24,193 - INFO - Epoch [4/10], Batch [10/187], Loss: 0.3528
2025-04-05 12:04:49,158 - INFO - Epoch [4/10], Batch [20/187], Loss: 0.6574
2025-04-05 12:05:14,719 - INFO - Epoch [4/10], Batch [30/187], Loss: 0.4206
2025-04-05 12:05:39,682 - INFO - Epoch [4/10], Batch [40/187], Loss: 0.5592
2025-04-05 12:06:05,540 - INFO - Epoch [4/10], Batch [50/187], Loss: 0.4445
2025-04-05 12:06:30,851 - INFO - Epoch [4/10], Batch [60/187], Loss: 0.5800
2025-04-05 12:06:55,997 - INFO - Epoch [4/10], Batch [70/187], Loss: 0.4582
2025-04-05 12:07:21,753 - INFO - Epoch [4/10], Batch [80/187], Loss: 0.4484
2025-04-05 12:07:46,862 - INFO - Epoch [4/10], Batch [90/187], Loss: 0.5299
2025-04-05 12:08:13,483 - INFO - Epoch [4/10], Batch [100/187], Loss: 0.4410
2025-04-05 12:08:43,224 - INFO - Epoch [4/10], Batch [110/187], Loss: 0.6430
2025-04-05 12:09:13,559 - INFO - Epoch [4/10], Batch [120/187], Loss: 0.5241
2025-04-05 12:09:38,675 - INFO - Epoch [4/10], Batch [130/187], Loss: 0.4842
2025-04-05 12:10:04,473 - INFO - Epoch [4/10], Batch [140/187], Loss: 0.4289
2025-04-05 12:10:31,617 - INFO - Epoch [4/10], Batch [150/187], Loss: 0.4157
2025-04-05 12:10:59,441 - INFO - Epoch [4/10], Batch [160/187], Loss: 0.3876
2025-04-05 12:11:26,683 - INFO - Epoch [4/10], Batch [170/187], Loss: 0.4521
2025-04-05 12:11:54,876 - INFO - Epoch [4/10], Batch [180/187], Loss: 0.3476
2025-04-05 12:12:09,832 - INFO - Epoch [4/10] Train Loss: 0.4650, Train Accuracy: 77.86%
2025-04-05 12:12:38,507 - INFO - Epoch [4/10] Val Loss: 0.4010, Val Accuracy: 82.60%
2025-04-05 12:12:38,510 - INFO - New best model at epoch 4 with val accuracy: 82.60%
2025-04-05 12:12:41,308 - INFO - Epoch [5/10], Batch [0/187], Loss: 0.5139
2025-04-05 12:13:07,038 - INFO - Epoch [5/10], Batch [10/187], Loss: 0.4302
2025-04-05 12:13:32,831 - INFO - Epoch [5/10], Batch [20/187], Loss: 0.4434
2025-04-05 12:13:58,870 - INFO - Epoch [5/10], Batch [30/187], Loss: 0.5112
2025-04-05 12:14:23,819 - INFO - Epoch [5/10], Batch [40/187], Loss: 0.5218
2025-04-05 12:14:48,330 - INFO - Epoch [5/10], Batch [50/187], Loss: 0.2414
2025-04-05 12:15:12,834 - INFO - Epoch [5/10], Batch [60/187], Loss: 0.4072
2025-04-05 12:15:37,563 - INFO - Epoch [5/10], Batch [70/187], Loss: 0.2919
2025-04-05 12:16:01,920 - INFO - Epoch [5/10], Batch [80/187], Loss: 0.3813
2025-04-05 12:16:27,418 - INFO - Epoch [5/10], Batch [90/187], Loss: 0.4402
2025-04-05 12:16:51,855 - INFO - Epoch [5/10], Batch [100/187], Loss: 0.5505
2025-04-05 12:17:17,609 - INFO - Epoch [5/10], Batch [110/187], Loss: 0.3010
2025-04-05 12:17:43,249 - INFO - Epoch [5/10], Batch [120/187], Loss: 0.4233
2025-04-05 12:18:08,297 - INFO - Epoch [5/10], Batch [130/187], Loss: 0.2769
2025-04-05 12:18:34,971 - INFO - Epoch [5/10], Batch [140/187], Loss: 0.3890
2025-04-05 12:18:59,053 - INFO - Epoch [5/10], Batch [150/187], Loss: 0.4906
2025-04-05 12:19:24,986 - INFO - Epoch [5/10], Batch [160/187], Loss: 0.3503
2025-04-05 12:19:51,882 - INFO - Epoch [5/10], Batch [170/187], Loss: 0.4495
2025-04-05 12:20:15,453 - INFO - Epoch [5/10], Batch [180/187], Loss: 0.5246
2025-04-05 12:20:29,401 - INFO - Epoch [5/10] Train Loss: 0.4265, Train Accuracy: 79.88%
2025-04-05 12:20:56,037 - INFO - Epoch [5/10] Val Loss: 0.8338, Val Accuracy: 65.48%
2025-04-05 12:20:59,480 - INFO - Epoch [6/10], Batch [0/187], Loss: 0.5858
2025-04-05 12:21:23,700 - INFO - Epoch [6/10], Batch [10/187], Loss: 0.4259
2025-04-05 12:21:47,760 - INFO - Epoch [6/10], Batch [20/187], Loss: 0.6521
2025-04-05 12:22:13,011 - INFO - Epoch [6/10], Batch [30/187], Loss: 0.3115
2025-04-05 12:22:37,427 - INFO - Epoch [6/10], Batch [40/187], Loss: 0.2503
2025-04-05 12:23:02,460 - INFO - Epoch [6/10], Batch [50/187], Loss: 0.4491
2025-04-05 12:23:27,306 - INFO - Epoch [6/10], Batch [60/187], Loss: 0.5482
2025-04-05 12:23:51,763 - INFO - Epoch [6/10], Batch [70/187], Loss: 0.4674
2025-04-05 12:24:16,157 - INFO - Epoch [6/10], Batch [80/187], Loss: 0.3676
2025-04-05 12:24:40,452 - INFO - Epoch [6/10], Batch [90/187], Loss: 0.3742
2025-04-05 12:25:05,231 - INFO - Epoch [6/10], Batch [100/187], Loss: 0.3903
2025-04-05 12:25:29,895 - INFO - Epoch [6/10], Batch [110/187], Loss: 0.2666
2025-04-05 12:25:54,446 - INFO - Epoch [6/10], Batch [120/187], Loss: 0.6941
2025-04-05 12:26:19,038 - INFO - Epoch [6/10], Batch [130/187], Loss: 0.3205
2025-04-05 12:26:43,970 - INFO - Epoch [6/10], Batch [140/187], Loss: 0.2870
2025-04-05 12:27:08,554 - INFO - Epoch [6/10], Batch [150/187], Loss: 0.3668
2025-04-05 12:27:35,001 - INFO - Epoch [6/10], Batch [160/187], Loss: 0.4614
2025-04-05 12:28:01,275 - INFO - Epoch [6/10], Batch [170/187], Loss: 0.5925
2025-04-05 12:28:25,898 - INFO - Epoch [6/10], Batch [180/187], Loss: 0.3344
2025-04-05 12:28:40,114 - INFO - Epoch [6/10] Train Loss: 0.4029, Train Accuracy: 81.24%
2025-04-05 12:29:07,734 - INFO - Epoch [6/10] Val Loss: 0.4079, Val Accuracy: 81.42%
2025-04-05 12:29:11,410 - INFO - Epoch [7/10], Batch [0/187], Loss: 0.2680
2025-04-05 12:29:36,531 - INFO - Epoch [7/10], Batch [10/187], Loss: 0.4085
2025-04-05 12:30:00,823 - INFO - Epoch [7/10], Batch [20/187], Loss: 0.4295
2025-04-05 12:30:25,033 - INFO - Epoch [7/10], Batch [30/187], Loss: 0.3492
2025-04-05 12:30:48,961 - INFO - Epoch [7/10], Batch [40/187], Loss: 0.3989
2025-04-05 12:31:13,146 - INFO - Epoch [7/10], Batch [50/187], Loss: 0.4087
2025-04-05 12:31:38,036 - INFO - Epoch [7/10], Batch [60/187], Loss: 0.4264
2025-04-05 12:32:03,771 - INFO - Epoch [7/10], Batch [70/187], Loss: 0.4873
2025-04-05 12:32:31,023 - INFO - Epoch [7/10], Batch [80/187], Loss: 0.3125
2025-04-05 12:32:56,818 - INFO - Epoch [7/10], Batch [90/187], Loss: 0.2927
2025-04-05 12:33:23,250 - INFO - Epoch [7/10], Batch [100/187], Loss: 0.3269
2025-04-05 12:33:47,999 - INFO - Epoch [7/10], Batch [110/187], Loss: 0.4528
2025-04-05 12:34:13,314 - INFO - Epoch [7/10], Batch [120/187], Loss: 0.6174
2025-04-05 12:34:36,988 - INFO - Epoch [7/10], Batch [130/187], Loss: 0.3346
2025-04-05 12:35:01,123 - INFO - Epoch [7/10], Batch [140/187], Loss: 0.5474
2025-04-05 12:35:24,718 - INFO - Epoch [7/10], Batch [150/187], Loss: 0.4309
2025-04-05 12:35:49,104 - INFO - Epoch [7/10], Batch [160/187], Loss: 0.4347
2025-04-05 12:36:13,717 - INFO - Epoch [7/10], Batch [170/187], Loss: 0.4654
2025-04-05 12:36:38,617 - INFO - Epoch [7/10], Batch [180/187], Loss: 0.2974
2025-04-05 12:36:53,400 - INFO - Epoch [7/10] Train Loss: 0.3871, Train Accuracy: 81.41%
2025-04-05 12:37:21,115 - INFO - Epoch [7/10] Val Loss: 0.3147, Val Accuracy: 87.33%
2025-04-05 12:37:21,118 - INFO - New best model at epoch 7 with val accuracy: 87.33%
2025-04-05 12:37:23,453 - INFO - Epoch [8/10], Batch [0/187], Loss: 0.6269
2025-04-05 12:37:47,390 - INFO - Epoch [8/10], Batch [10/187], Loss: 0.3558
2025-04-05 12:38:11,315 - INFO - Epoch [8/10], Batch [20/187], Loss: 0.3737
2025-04-05 12:38:35,142 - INFO - Epoch [8/10], Batch [30/187], Loss: 0.3001
2025-04-05 12:38:59,322 - INFO - Epoch [8/10], Batch [40/187], Loss: 0.2516
2025-04-05 12:39:23,749 - INFO - Epoch [8/10], Batch [50/187], Loss: 0.2341
2025-04-05 12:39:49,486 - INFO - Epoch [8/10], Batch [60/187], Loss: 0.1978
2025-04-05 12:40:14,833 - INFO - Epoch [8/10], Batch [70/187], Loss: 0.4264
2025-04-05 12:40:38,627 - INFO - Epoch [8/10], Batch [80/187], Loss: 0.4492
2025-04-05 12:41:02,423 - INFO - Epoch [8/10], Batch [90/187], Loss: 0.4429
2025-04-05 12:41:26,374 - INFO - Epoch [8/10], Batch [100/187], Loss: 0.4406
2025-04-05 12:41:50,547 - INFO - Epoch [8/10], Batch [110/187], Loss: 0.2864
2025-04-05 12:42:16,987 - INFO - Epoch [8/10], Batch [120/187], Loss: 0.2182
2025-04-05 12:42:44,721 - INFO - Epoch [8/10], Batch [130/187], Loss: 0.2640
2025-04-05 12:43:11,999 - INFO - Epoch [8/10], Batch [140/187], Loss: 0.3368
2025-04-05 12:43:39,729 - INFO - Epoch [8/10], Batch [150/187], Loss: 0.3402
2025-04-05 12:44:06,811 - INFO - Epoch [8/10], Batch [160/187], Loss: 0.2902
2025-04-05 12:44:30,942 - INFO - Epoch [8/10], Batch [170/187], Loss: 0.3680
2025-04-05 12:44:55,057 - INFO - Epoch [8/10], Batch [180/187], Loss: 0.3327
2025-04-05 12:45:09,245 - INFO - Epoch [8/10] Train Loss: 0.3582, Train Accuracy: 83.92%
2025-04-05 12:45:35,297 - INFO - Epoch [8/10] Val Loss: 0.3659, Val Accuracy: 84.55%
2025-04-05 12:45:38,070 - INFO - Epoch [9/10], Batch [0/187], Loss: 0.2857
2025-04-05 12:46:02,323 - INFO - Epoch [9/10], Batch [10/187], Loss: 0.3832
2025-04-05 12:46:26,544 - INFO - Epoch [9/10], Batch [20/187], Loss: 0.2816
2025-04-05 12:46:51,918 - INFO - Epoch [9/10], Batch [30/187], Loss: 0.3363
2025-04-05 12:47:17,003 - INFO - Epoch [9/10], Batch [40/187], Loss: 0.4264
2025-04-05 12:47:41,664 - INFO - Epoch [9/10], Batch [50/187], Loss: 0.2409
2025-04-05 12:48:06,016 - INFO - Epoch [9/10], Batch [60/187], Loss: 0.1863
2025-04-05 12:48:33,649 - INFO - Epoch [9/10], Batch [70/187], Loss: 0.2408
2025-04-05 12:49:00,411 - INFO - Epoch [9/10], Batch [80/187], Loss: 0.2905
2025-04-05 12:49:27,453 - INFO - Epoch [9/10], Batch [90/187], Loss: 0.3540
2025-04-05 12:49:54,644 - INFO - Epoch [9/10], Batch [100/187], Loss: 0.4244
2025-04-05 12:50:20,208 - INFO - Epoch [9/10], Batch [110/187], Loss: 0.2284
2025-04-05 12:50:45,218 - INFO - Epoch [9/10], Batch [120/187], Loss: 0.2936
2025-04-05 12:51:09,963 - INFO - Epoch [9/10], Batch [130/187], Loss: 0.2270
2025-04-05 12:51:34,429 - INFO - Epoch [9/10], Batch [140/187], Loss: 0.3952
2025-04-05 12:51:58,010 - INFO - Epoch [9/10], Batch [150/187], Loss: 0.2590
2025-04-05 12:52:21,896 - INFO - Epoch [9/10], Batch [160/187], Loss: 0.3299
2025-04-05 12:52:45,190 - INFO - Epoch [9/10], Batch [170/187], Loss: 0.3673
2025-04-05 12:53:09,389 - INFO - Epoch [9/10], Batch [180/187], Loss: 0.3383
2025-04-05 12:53:23,001 - INFO - Epoch [9/10] Train Loss: 0.3457, Train Accuracy: 83.95%
2025-04-05 12:53:49,178 - INFO - Epoch [9/10] Val Loss: 0.2780, Val Accuracy: 89.00%
2025-04-05 12:53:49,182 - INFO - New best model at epoch 9 with val accuracy: 89.00%
2025-04-05 12:53:53,376 - INFO - Epoch [10/10], Batch [0/187], Loss: 0.4095
2025-04-05 12:54:16,707 - INFO - Epoch [10/10], Batch [10/187], Loss: 0.5701
2025-04-05 12:54:39,760 - INFO - Epoch [10/10], Batch [20/187], Loss: 0.3154
2025-04-05 12:55:03,089 - INFO - Epoch [10/10], Batch [30/187], Loss: 0.3200
2025-04-05 12:55:27,203 - INFO - Epoch [10/10], Batch [40/187], Loss: 0.4569
2025-04-05 12:55:51,249 - INFO - Epoch [10/10], Batch [50/187], Loss: 0.2669
2025-04-05 12:56:16,026 - INFO - Epoch [10/10], Batch [60/187], Loss: 0.2417
2025-04-05 12:56:40,099 - INFO - Epoch [10/10], Batch [70/187], Loss: 0.3896
2025-04-05 12:57:06,035 - INFO - Epoch [10/10], Batch [80/187], Loss: 0.3773
2025-04-05 12:57:31,126 - INFO - Epoch [10/10], Batch [90/187], Loss: 0.3967
2025-04-05 12:57:58,602 - INFO - Epoch [10/10], Batch [100/187], Loss: 0.1822
2025-04-05 12:58:22,239 - INFO - Epoch [10/10], Batch [110/187], Loss: 0.2930
2025-04-05 12:58:45,842 - INFO - Epoch [10/10], Batch [120/187], Loss: 0.3195
2025-04-05 12:59:09,656 - INFO - Epoch [10/10], Batch [130/187], Loss: 0.2853
2025-04-05 12:59:33,793 - INFO - Epoch [10/10], Batch [140/187], Loss: 0.5026
2025-04-05 12:59:57,483 - INFO - Epoch [10/10], Batch [150/187], Loss: 0.3581
2025-04-05 13:00:21,468 - INFO - Epoch [10/10], Batch [160/187], Loss: 0.2132
2025-04-05 13:00:46,177 - INFO - Epoch [10/10], Batch [170/187], Loss: 0.3637
2025-04-05 13:01:09,864 - INFO - Epoch [10/10], Batch [180/187], Loss: 0.2814
2025-04-05 13:01:23,457 - INFO - Epoch [10/10] Train Loss: 0.3299, Train Accuracy: 84.52%
2025-04-05 13:01:49,705 - INFO - Epoch [10/10] Val Loss: 0.2687, Val Accuracy: 89.42%
2025-04-05 13:01:49,709 - INFO - New best model at epoch 10 with val accuracy: 89.42%
2025-04-05 13:02:03,711 - INFO - Test Loss: 0.3067, Test Accuracy: 87.20%
2025-04-05 13:02:03,712 - INFO - 
===== Final Performance Results =====
2025-04-05 13:02:03,716 - INFO - {
    "world_size": 4,
    "train_time": 4871.955017089844,
    "avg_epoch_time": 487.19550170898435,
    "val_accuracy": 89.42240779401531,
    "test_accuracy": 87.20445062586926,
    "test_loss": 0.30665782628702687,
    "total_time": 4871.955017089844,
    "train_losses": [
        0.6479086761694074,
        0.6090708466154761,
        0.5197986322815947,
        0.46499148074553104,
        0.42646901890822536,
        0.4028993652605113,
        0.38711823966712633,
        0.3581581878213204,
        0.3457125015687743,
        0.32986888491957755
    ],
    "train_accuracies": [
        60.31799163179916,
        66.61087866108787,
        73.18828451882845,
        77.85774058577405,
        79.88284518828452,
        81.23849372384937,
        81.40585774058577,
        83.9163179916318,
        83.94979079497908,
        84.51882845188284
    ],
    "val_losses": [
        0.694937352389201,
        0.5166993375113215,
        0.52303525504754,
        0.400955745554999,
        0.8337661782610143,
        0.40791592257007925,
        0.31472845382796616,
        0.3658994632872261,
        0.2779759848457952,
        0.2687094644614202
    ],
    "val_accuracies": [
        58.94224077940153,
        77.03549060542798,
        75.78288100208768,
        82.6026443980515,
        65.48364648573417,
        81.419624217119,
        87.33472512178149,
        84.55114822546973,
        89.00487125956855,
        89.42240779401531
    ],
    "hyperparameters": {
        "batch_size": 32,
        "num_epochs": 10,
        "initial_lr": 0.001,
        "weight_decay": 0.0001,
        "num_classes": 2,
        "save_model": true
    }
}
2025-04-05 13:02:03,913 - INFO - Model saved to models/training_using_cpus_4_best_model.pt
2025-04-05 13:02:07,566 - INFO - Loss plot saved as plots/training_using_cpus_4_loss.png
2025-04-05 13:02:07,683 - INFO - Accuracy plot saved as plots/training_using_cpus_4_accuracy.png
2025-04-05 13:02:07,693 - INFO - Runtime parameters saved as metrics/training_using_cpus_4_params.json
