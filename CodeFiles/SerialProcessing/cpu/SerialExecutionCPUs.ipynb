{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca5f84e-5793-4d7d-bfbc-2c0b80c63ed5",
   "metadata": {},
   "source": [
    "# CSYE 7105 - High Perfoamnce Machine Learning & AI - Project - Team 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba95a4-ac4b-48d7-af4f-40e89b7809c1",
   "metadata": {},
   "source": [
    "## Analysis of Efficient Parallel Computing for Deep Learning-Based Glaucoma Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e087e64-2c4c-4179-a3ba-30960ab55ce9",
   "metadata": {},
   "source": [
    "### Serial Execution using CPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b1afc-4abe-4194-974c-798a92cad4da",
   "metadata": {},
   "source": [
    "### This notebook focusses on training a CNN model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd97fd-c041-4ab5-9831-efa3ba0136f4",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353c2c01-f1c4-406e-9cb9-6606ebb67aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import socket\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_fscore_support\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215bbb5-081c-422e-9b6c-120c5a481704",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324147dd-9818-47cd-8f4e-f2d29c58a629",
   "metadata": {},
   "source": [
    "This MedicalCNN model, with 129 layers, uses a combination of convolutional, residual, multi-dilated, and fully connected blocks, tailored for glaucoma classification. Iâ€™ve included Residual Blocks to help with vanishing gradients, MultiDilated Blocks for enhanced feature extraction, and Squeeze-and-Excitation (SE) Blocks to fine-tune feature importance across channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a841cf9-2073-4400-8310-b2032027e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        # Convert numpy array to tensor and ensure float32 type\n",
    "        image = torch.from_numpy(image).float()\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.permute(2, 0, 1)  # Convert (H,W,C) to (C,H,W)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=kernel_size,\n",
    "            stride=stride, padding=padding, bias=False, dilation=dilation\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class MultiDilatedBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MultiDilatedBlock, self).__init__()\n",
    "        self.conv1 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv2 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv3 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=3, dilation=3)\n",
    "        self.conv4 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=4, dilation=4)\n",
    "        self.conv_fusion = ConvBlock(out_channels, out_channels, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "        x4 = self.conv4(x)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        return self.conv_fusion(x)\n",
    "\n",
    "class MedicalCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, base_filters=64):\n",
    "        super(MedicalCNN, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, base_filters, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(base_filters),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.stage1 = self._make_stage(base_filters, base_filters, blocks=3)\n",
    "        self.stage2 = self._make_stage(base_filters, base_filters*2, blocks=4, stride=2)\n",
    "        self.stage3_res = self._make_stage(base_filters*2, base_filters*4, blocks=6, stride=2)\n",
    "        self.stage3_md = MultiDilatedBlock(base_filters*4, base_filters*4)\n",
    "        self.stage4_res = self._make_stage(base_filters*4, base_filters*8, blocks=3, stride=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(base_filters*8, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_stage(self, in_channels, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3_res(x)\n",
    "        x = self.stage3_md(x)\n",
    "        x = self.stage4_res(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def count_layers(self):\n",
    "        # Stem: 4 layers\n",
    "        stem_layers = 4\n",
    "        # Stage 1: 3 ResBlocks x 6 = 18 layers\n",
    "        stage1_layers = 3 * 6\n",
    "        # Stage 2: 4 ResBlocks x 6 = 24 layers\n",
    "        stage2_layers = 4 * 6\n",
    "        # Stage 3: 6 ResBlocks x 6 + MultiDilated (10 layers) = 46 layers\n",
    "        stage3_layers = 6 * 6 + 10\n",
    "        # Stage 4: 3 ResBlocks x 6 + MultiDilated (10 layers) = 28 layers\n",
    "        stage4_layers = 3 * 6 + 10\n",
    "        # FC: 9 layers\n",
    "        fc_layers = 9\n",
    "        total_layers = stem_layers + stage1_layers + stage2_layers + stage3_layers + stage4_layers + fc_layers\n",
    "        return total_layers\n",
    "\n",
    "def get_medical_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05, hue=0.05),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return train_transform, val_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7044a-5e79-4687-ab59-252b69144e26",
   "metadata": {},
   "source": [
    "### Defining the function to train the model serially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77235ae4-1904-4035-9d37-2f181e6269a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Console handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "\n",
    "# File handler (log to a file)\n",
    "log_file = \"logs/training_using_cpus_1_log.txt\"\n",
    "fh = logging.FileHandler(log_file)\n",
    "fh.setLevel(logging.INFO)\n",
    "\n",
    "# Create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "def train_and_evaluate(data_dir=\"../../preprocessed_glaucoma_data\"):\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    logger.info(\"Loading data...\")\n",
    "    X_train = np.load(os.path.join(data_dir, 'X_train.npy'))\n",
    "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
    "    X_val = np.load(os.path.join(data_dir, 'X_val.npy'))\n",
    "    y_val = np.load(os.path.join(data_dir, 'y_val.npy'))\n",
    "    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))\n",
    "    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "    \n",
    "    logger.info(f\"Data shapes: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}\")\n",
    "    \n",
    "    train_transform, val_transform = get_medical_transforms()\n",
    "    \n",
    "    train_dataset = GlaucomaDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = GlaucomaDataset(X_val, y_val, transform=val_transform)\n",
    "    test_dataset = GlaucomaDataset(X_test, y_test, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders without DistributedSampler (for serial processing)\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "    class_weights = torch.FloatTensor([1.0 / count for count in class_counts])\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    num_classes = 2\n",
    "    model = MedicalCNN(num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_layers = model.count_layers()\n",
    "    logger.info(f\"Model architecture: MedicalCNN\")\n",
    "    logger.info(f\"Total layers: {total_layers}\")\n",
    "    logger.info(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    initial_lr = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "    num_epochs = 10\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Lists to record metrics for plotting\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item() * data.size(0)\n",
    "            total_train_samples += data.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                logger.info(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_loss = epoch_train_loss / total_train_samples\n",
    "        train_accuracy = 100.0 * correct_train / total_train_samples\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        logger.info(f'Epoch {epoch+1}/{num_epochs} Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "        \n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val_samples = 0\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                epoch_val_loss += loss.item() * data.size(0)\n",
    "                total_val_samples += data.size(0)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct_val += (predicted == target).sum().item()\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_probabilities.extend(F.softmax(output, dim=1).cpu().numpy())\n",
    "        \n",
    "        val_loss = epoch_val_loss / total_val_samples\n",
    "        val_accuracy = 100.0 * correct_val / total_val_samples\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        if num_classes == 2:\n",
    "            all_targets = np.array(all_targets)\n",
    "            all_probabilities = np.array(all_probabilities)\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(all_targets, all_probabilities[:, 1])\n",
    "                logger.info(f'Epoch {epoch+1}/{num_epochs} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, AUC-ROC: {roc_auc:.4f}')\n",
    "            except Exception as e:\n",
    "                logger.info(f'Epoch {epoch+1}/{num_epochs} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        else:\n",
    "            logger.info(f'Epoch {epoch+1}/{num_epochs} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            logger.info(f\"New best model at epoch {epoch+1} with Val Accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Total training time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        logger.info(f\"Loaded best model with Val Accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test_samples = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            epoch_test_loss += loss.item() * data.size(0)\n",
    "            total_test_samples += data.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_test += (predicted == target).sum().item()\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_probabilities.extend(F.softmax(output, dim=1).cpu().numpy())\n",
    "    \n",
    "    test_loss = epoch_test_loss / total_test_samples\n",
    "    test_accuracy = 100.0 * correct_test / total_test_samples\n",
    "    \n",
    "    if num_classes == 2:\n",
    "        try:\n",
    "            test_auc = roc_auc_score(np.array(all_targets), np.array(all_probabilities)[:, 1])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Test AUC-ROC: {e}\")\n",
    "            test_auc = None\n",
    "    else:\n",
    "        test_auc = None\n",
    "    \n",
    "    logger.info(\"\\n===== Final Test Results =====\")\n",
    "    logger.info(f\"Test Loss: {test_loss:.4f}\")\n",
    "    logger.info(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    if test_auc is not None:\n",
    "        logger.info(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    model_filename = \"models/training_using_cpus_1_model.pth\"\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    logger.info(f\"Model saved to {model_filename}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    epochs_range = range(1, num_epochs+1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    plot_filename = \"plots/training_using_cpus_1_results.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    logger.info(f\"Training plots saved as {plot_filename}\")\n",
    "    \n",
    "    # Save runtime parameters and metrics as JSON\n",
    "    result = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_auc_roc\": test_auc,\n",
    "        \"computing_time\": total_time,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"initial_lr\": initial_lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"num_classes\": num_classes\n",
    "        },\n",
    "        \"total_parameters\": total_params,\n",
    "        \"total_layers\": total_layers\n",
    "    }\n",
    "    os.makedirs('metrics', exist_ok=True)\n",
    "    params_filename = \"metrics/training_using_cpus_1_params.json\"\n",
    "    with open(params_filename, 'w') as f:\n",
    "        json.dump(result, f, indent=4)\n",
    "    logger.info(f\"Runtime parameters saved as {params_filename}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f141bae-0e94-4a2f-97fb-f8767c37900d",
   "metadata": {},
   "source": [
    "#### Defining Main Function to call and control the function flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b00fde-acee-49b4-ad73-b7b8ba767466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 13:42:16,941 - INFO - Running on node: d0010\n",
      "2025-04-03 13:42:16,943 - INFO - Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single CPU...\n",
      "2025-04-03 13:42:17,037 - INFO - Loading data...\n",
      "2025-04-03 13:42:29,316 - INFO - Data shapes: Train (23898, 224, 224, 3), Val (5747, 224, 224, 3), Test (2874, 224, 224, 3)\n",
      "2025-04-03 13:42:29,321 - INFO - Using device: cpu\n",
      "2025-04-03 13:42:29,581 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-03 13:42:29,582 - INFO - Total layers: 129\n",
      "2025-04-03 13:42:29,582 - INFO - Total parameters: 22,494,274\n",
      "2025-04-03 13:42:35,952 - INFO - Epoch [1/10], Batch [0/747], Loss: 0.7127\n",
      "2025-04-03 13:43:32,061 - INFO - Epoch [1/10], Batch [10/747], Loss: 0.6740\n",
      "2025-04-03 13:44:28,403 - INFO - Epoch [1/10], Batch [20/747], Loss: 0.6940\n",
      "2025-04-03 13:45:24,627 - INFO - Epoch [1/10], Batch [30/747], Loss: 0.5867\n",
      "2025-04-03 13:46:21,154 - INFO - Epoch [1/10], Batch [40/747], Loss: 0.7433\n",
      "2025-04-03 13:47:18,216 - INFO - Epoch [1/10], Batch [50/747], Loss: 0.6379\n",
      "2025-04-03 13:48:15,229 - INFO - Epoch [1/10], Batch [60/747], Loss: 0.6258\n",
      "2025-04-03 13:49:12,292 - INFO - Epoch [1/10], Batch [70/747], Loss: 0.7025\n",
      "2025-04-03 13:50:09,267 - INFO - Epoch [1/10], Batch [80/747], Loss: 0.6468\n",
      "2025-04-03 13:51:06,107 - INFO - Epoch [1/10], Batch [90/747], Loss: 0.6475\n",
      "2025-04-03 13:52:02,828 - INFO - Epoch [1/10], Batch [100/747], Loss: 0.7703\n",
      "2025-04-03 13:52:59,922 - INFO - Epoch [1/10], Batch [110/747], Loss: 0.6344\n",
      "2025-04-03 13:53:56,931 - INFO - Epoch [1/10], Batch [120/747], Loss: 0.7708\n",
      "2025-04-03 13:54:54,094 - INFO - Epoch [1/10], Batch [130/747], Loss: 0.6058\n",
      "2025-04-03 13:55:51,284 - INFO - Epoch [1/10], Batch [140/747], Loss: 0.6487\n",
      "2025-04-03 13:56:47,837 - INFO - Epoch [1/10], Batch [150/747], Loss: 0.6777\n",
      "2025-04-03 13:57:44,486 - INFO - Epoch [1/10], Batch [160/747], Loss: 0.6194\n",
      "2025-04-03 13:58:41,067 - INFO - Epoch [1/10], Batch [170/747], Loss: 0.5806\n",
      "2025-04-03 13:59:37,583 - INFO - Epoch [1/10], Batch [180/747], Loss: 0.5762\n",
      "2025-04-03 14:00:34,379 - INFO - Epoch [1/10], Batch [190/747], Loss: 0.5735\n",
      "2025-04-03 14:01:30,959 - INFO - Epoch [1/10], Batch [200/747], Loss: 0.6230\n",
      "2025-04-03 14:02:27,684 - INFO - Epoch [1/10], Batch [210/747], Loss: 0.6473\n",
      "2025-04-03 14:03:24,523 - INFO - Epoch [1/10], Batch [220/747], Loss: 0.6535\n",
      "2025-04-03 14:04:21,582 - INFO - Epoch [1/10], Batch [230/747], Loss: 0.6054\n",
      "2025-04-03 14:05:18,084 - INFO - Epoch [1/10], Batch [240/747], Loss: 0.7730\n",
      "2025-04-03 14:06:14,693 - INFO - Epoch [1/10], Batch [250/747], Loss: 0.5785\n",
      "2025-04-03 14:07:11,686 - INFO - Epoch [1/10], Batch [260/747], Loss: 0.5878\n",
      "2025-04-03 14:08:08,723 - INFO - Epoch [1/10], Batch [270/747], Loss: 0.6497\n",
      "2025-04-03 14:09:05,003 - INFO - Epoch [1/10], Batch [280/747], Loss: 0.6878\n",
      "2025-04-03 14:10:00,828 - INFO - Epoch [1/10], Batch [290/747], Loss: 0.5421\n",
      "2025-04-03 14:10:57,257 - INFO - Epoch [1/10], Batch [300/747], Loss: 0.5510\n",
      "2025-04-03 14:11:53,535 - INFO - Epoch [1/10], Batch [310/747], Loss: 0.6137\n",
      "2025-04-03 14:12:49,847 - INFO - Epoch [1/10], Batch [320/747], Loss: 0.5717\n",
      "2025-04-03 14:13:46,343 - INFO - Epoch [1/10], Batch [330/747], Loss: 0.5378\n",
      "2025-04-03 14:14:42,454 - INFO - Epoch [1/10], Batch [340/747], Loss: 0.7164\n",
      "2025-04-03 14:15:38,551 - INFO - Epoch [1/10], Batch [350/747], Loss: 0.6517\n",
      "2025-04-03 14:16:34,823 - INFO - Epoch [1/10], Batch [360/747], Loss: 0.6219\n",
      "2025-04-03 14:17:31,305 - INFO - Epoch [1/10], Batch [370/747], Loss: 0.6614\n",
      "2025-04-03 14:18:27,545 - INFO - Epoch [1/10], Batch [380/747], Loss: 0.5740\n",
      "2025-04-03 14:19:23,889 - INFO - Epoch [1/10], Batch [390/747], Loss: 0.5501\n",
      "2025-04-03 14:20:19,947 - INFO - Epoch [1/10], Batch [400/747], Loss: 0.5578\n",
      "2025-04-03 14:21:16,392 - INFO - Epoch [1/10], Batch [410/747], Loss: 0.6281\n",
      "2025-04-03 14:22:12,771 - INFO - Epoch [1/10], Batch [420/747], Loss: 0.6357\n",
      "2025-04-03 14:23:09,337 - INFO - Epoch [1/10], Batch [430/747], Loss: 0.6446\n",
      "2025-04-03 14:24:05,729 - INFO - Epoch [1/10], Batch [440/747], Loss: 0.6412\n",
      "2025-04-03 14:25:02,262 - INFO - Epoch [1/10], Batch [450/747], Loss: 0.5990\n",
      "2025-04-03 14:25:58,642 - INFO - Epoch [1/10], Batch [460/747], Loss: 0.7370\n",
      "2025-04-03 14:26:55,347 - INFO - Epoch [1/10], Batch [470/747], Loss: 0.8675\n",
      "2025-04-03 14:27:51,703 - INFO - Epoch [1/10], Batch [480/747], Loss: 0.7397\n",
      "2025-04-03 14:28:48,174 - INFO - Epoch [1/10], Batch [490/747], Loss: 0.5744\n",
      "2025-04-03 14:29:44,363 - INFO - Epoch [1/10], Batch [500/747], Loss: 0.7302\n",
      "2025-04-03 14:30:40,519 - INFO - Epoch [1/10], Batch [510/747], Loss: 0.6259\n",
      "2025-04-03 14:31:36,613 - INFO - Epoch [1/10], Batch [520/747], Loss: 0.7040\n",
      "2025-04-03 14:32:33,018 - INFO - Epoch [1/10], Batch [530/747], Loss: 0.6486\n",
      "2025-04-03 14:33:29,327 - INFO - Epoch [1/10], Batch [540/747], Loss: 0.6709\n",
      "2025-04-03 14:34:26,308 - INFO - Epoch [1/10], Batch [550/747], Loss: 0.6387\n",
      "2025-04-03 14:35:22,703 - INFO - Epoch [1/10], Batch [560/747], Loss: 0.7219\n",
      "2025-04-03 14:36:19,011 - INFO - Epoch [1/10], Batch [570/747], Loss: 0.5837\n",
      "2025-04-03 14:37:14,823 - INFO - Epoch [1/10], Batch [580/747], Loss: 0.8197\n",
      "2025-04-03 14:38:10,859 - INFO - Epoch [1/10], Batch [590/747], Loss: 0.5974\n",
      "2025-04-03 14:39:06,960 - INFO - Epoch [1/10], Batch [600/747], Loss: 0.5817\n",
      "2025-04-03 14:40:03,106 - INFO - Epoch [1/10], Batch [610/747], Loss: 0.7085\n",
      "2025-04-03 14:40:59,601 - INFO - Epoch [1/10], Batch [620/747], Loss: 0.6372\n",
      "2025-04-03 14:41:56,301 - INFO - Epoch [1/10], Batch [630/747], Loss: 0.6257\n",
      "2025-04-03 14:42:52,965 - INFO - Epoch [1/10], Batch [640/747], Loss: 0.5034\n",
      "2025-04-03 14:43:49,901 - INFO - Epoch [1/10], Batch [650/747], Loss: 0.6109\n",
      "2025-04-03 14:44:46,574 - INFO - Epoch [1/10], Batch [660/747], Loss: 0.5898\n",
      "2025-04-03 14:45:43,334 - INFO - Epoch [1/10], Batch [670/747], Loss: 0.6115\n",
      "2025-04-03 14:46:39,872 - INFO - Epoch [1/10], Batch [680/747], Loss: 0.5704\n",
      "2025-04-03 14:47:36,324 - INFO - Epoch [1/10], Batch [690/747], Loss: 0.5861\n",
      "2025-04-03 14:48:32,789 - INFO - Epoch [1/10], Batch [700/747], Loss: 0.6291\n",
      "2025-04-03 14:49:29,187 - INFO - Epoch [1/10], Batch [710/747], Loss: 0.5877\n",
      "2025-04-03 14:50:25,823 - INFO - Epoch [1/10], Batch [720/747], Loss: 0.6022\n",
      "2025-04-03 14:51:22,043 - INFO - Epoch [1/10], Batch [730/747], Loss: 0.6931\n",
      "2025-04-03 14:52:17,922 - INFO - Epoch [1/10], Batch [740/747], Loss: 0.6116\n",
      "2025-04-03 14:52:50,581 - INFO - Epoch 1/10 Train Loss: 0.6462, Train Accuracy: 61.29%\n",
      "2025-04-03 14:57:56,802 - INFO - Epoch 1/10 Val Loss: 0.5680, Val Accuracy: 72.28%, AUC-ROC: 0.7433\n",
      "2025-04-03 14:57:56,808 - INFO - New best model at epoch 1 with Val Accuracy: 72.28%\n",
      "2025-04-03 14:58:03,856 - INFO - Epoch [2/10], Batch [0/747], Loss: 0.6208\n",
      "2025-04-03 14:59:01,980 - INFO - Epoch [2/10], Batch [10/747], Loss: 0.6026\n",
      "2025-04-03 15:00:00,203 - INFO - Epoch [2/10], Batch [20/747], Loss: 0.6719\n",
      "2025-04-03 15:00:58,483 - INFO - Epoch [2/10], Batch [30/747], Loss: 0.6023\n",
      "2025-04-03 15:01:56,606 - INFO - Epoch [2/10], Batch [40/747], Loss: 0.5371\n",
      "2025-04-03 15:02:54,651 - INFO - Epoch [2/10], Batch [50/747], Loss: 0.6448\n",
      "2025-04-03 15:03:52,586 - INFO - Epoch [2/10], Batch [60/747], Loss: 0.4899\n",
      "2025-04-03 15:04:51,038 - INFO - Epoch [2/10], Batch [70/747], Loss: 0.6378\n",
      "2025-04-03 15:05:49,399 - INFO - Epoch [2/10], Batch [80/747], Loss: 0.6065\n",
      "2025-04-03 15:06:47,320 - INFO - Epoch [2/10], Batch [90/747], Loss: 0.6647\n",
      "2025-04-03 15:07:45,798 - INFO - Epoch [2/10], Batch [100/747], Loss: 0.6515\n",
      "2025-04-03 15:08:44,143 - INFO - Epoch [2/10], Batch [110/747], Loss: 0.6314\n",
      "2025-04-03 15:09:42,661 - INFO - Epoch [2/10], Batch [120/747], Loss: 0.6846\n",
      "2025-04-03 15:10:41,166 - INFO - Epoch [2/10], Batch [130/747], Loss: 0.7069\n",
      "2025-04-03 15:11:39,255 - INFO - Epoch [2/10], Batch [140/747], Loss: 0.6684\n",
      "2025-04-03 15:12:37,099 - INFO - Epoch [2/10], Batch [150/747], Loss: 0.5414\n",
      "2025-04-03 15:13:34,633 - INFO - Epoch [2/10], Batch [160/747], Loss: 0.4975\n",
      "2025-04-03 15:14:32,331 - INFO - Epoch [2/10], Batch [170/747], Loss: 0.5423\n",
      "2025-04-03 15:15:30,497 - INFO - Epoch [2/10], Batch [180/747], Loss: 0.5726\n",
      "2025-04-03 15:16:28,952 - INFO - Epoch [2/10], Batch [190/747], Loss: 0.5405\n",
      "2025-04-03 15:17:27,042 - INFO - Epoch [2/10], Batch [200/747], Loss: 0.8835\n",
      "2025-04-03 15:18:25,547 - INFO - Epoch [2/10], Batch [210/747], Loss: 0.5120\n",
      "2025-04-03 15:19:23,569 - INFO - Epoch [2/10], Batch [220/747], Loss: 0.5218\n",
      "2025-04-03 15:20:21,651 - INFO - Epoch [2/10], Batch [230/747], Loss: 0.6500\n",
      "2025-04-03 15:21:19,629 - INFO - Epoch [2/10], Batch [240/747], Loss: 0.5456\n",
      "2025-04-03 15:22:18,227 - INFO - Epoch [2/10], Batch [250/747], Loss: 0.4873\n",
      "2025-04-03 15:23:17,836 - INFO - Epoch [2/10], Batch [260/747], Loss: 0.5423\n",
      "2025-04-03 15:24:16,638 - INFO - Epoch [2/10], Batch [270/747], Loss: 0.6972\n",
      "2025-04-03 15:25:14,737 - INFO - Epoch [2/10], Batch [280/747], Loss: 0.6363\n",
      "2025-04-03 15:26:12,910 - INFO - Epoch [2/10], Batch [290/747], Loss: 0.5593\n",
      "2025-04-03 15:27:11,473 - INFO - Epoch [2/10], Batch [300/747], Loss: 0.6045\n",
      "2025-04-03 15:28:09,960 - INFO - Epoch [2/10], Batch [310/747], Loss: 0.6462\n",
      "2025-04-03 15:29:07,906 - INFO - Epoch [2/10], Batch [320/747], Loss: 0.5269\n",
      "2025-04-03 15:30:05,718 - INFO - Epoch [2/10], Batch [330/747], Loss: 0.6006\n",
      "2025-04-03 15:31:03,630 - INFO - Epoch [2/10], Batch [340/747], Loss: 0.5653\n",
      "2025-04-03 15:32:01,758 - INFO - Epoch [2/10], Batch [350/747], Loss: 0.5355\n",
      "2025-04-03 15:33:00,320 - INFO - Epoch [2/10], Batch [360/747], Loss: 0.6949\n",
      "2025-04-03 15:33:59,041 - INFO - Epoch [2/10], Batch [370/747], Loss: 0.6057\n",
      "2025-04-03 15:34:56,911 - INFO - Epoch [2/10], Batch [380/747], Loss: 0.6755\n",
      "2025-04-03 15:35:54,884 - INFO - Epoch [2/10], Batch [390/747], Loss: 0.5884\n",
      "2025-04-03 15:36:52,556 - INFO - Epoch [2/10], Batch [400/747], Loss: 0.5860\n",
      "2025-04-03 15:37:50,107 - INFO - Epoch [2/10], Batch [410/747], Loss: 0.5297\n",
      "2025-04-03 15:38:48,574 - INFO - Epoch [2/10], Batch [420/747], Loss: 0.6774\n",
      "2025-04-03 15:39:46,874 - INFO - Epoch [2/10], Batch [430/747], Loss: 0.4942\n",
      "2025-04-03 15:40:45,306 - INFO - Epoch [2/10], Batch [440/747], Loss: 0.5267\n",
      "2025-04-03 15:41:44,310 - INFO - Epoch [2/10], Batch [450/747], Loss: 0.5857\n",
      "2025-04-03 15:42:42,696 - INFO - Epoch [2/10], Batch [460/747], Loss: 0.5740\n",
      "2025-04-03 15:43:40,814 - INFO - Epoch [2/10], Batch [470/747], Loss: 0.5763\n",
      "2025-04-03 15:44:38,772 - INFO - Epoch [2/10], Batch [480/747], Loss: 0.5274\n",
      "2025-04-03 15:45:36,432 - INFO - Epoch [2/10], Batch [490/747], Loss: 0.5275\n",
      "2025-04-03 15:46:34,691 - INFO - Epoch [2/10], Batch [500/747], Loss: 0.6310\n",
      "2025-04-03 15:47:33,223 - INFO - Epoch [2/10], Batch [510/747], Loss: 0.8311\n",
      "2025-04-03 15:48:31,451 - INFO - Epoch [2/10], Batch [520/747], Loss: 0.5786\n",
      "2025-04-03 15:49:29,648 - INFO - Epoch [2/10], Batch [530/747], Loss: 0.6978\n",
      "2025-04-03 15:50:27,968 - INFO - Epoch [2/10], Batch [540/747], Loss: 0.5269\n",
      "2025-04-03 15:51:26,052 - INFO - Epoch [2/10], Batch [550/747], Loss: 0.4131\n",
      "2025-04-03 15:52:24,304 - INFO - Epoch [2/10], Batch [560/747], Loss: 0.5750\n",
      "2025-04-03 15:53:22,679 - INFO - Epoch [2/10], Batch [570/747], Loss: 0.6978\n",
      "2025-04-03 15:54:20,987 - INFO - Epoch [2/10], Batch [580/747], Loss: 0.5832\n",
      "2025-04-03 15:55:19,805 - INFO - Epoch [2/10], Batch [590/747], Loss: 0.6754\n",
      "2025-04-03 15:56:19,998 - INFO - Epoch [2/10], Batch [600/747], Loss: 0.6007\n",
      "2025-04-03 15:57:18,929 - INFO - Epoch [2/10], Batch [610/747], Loss: 0.5822\n",
      "2025-04-03 15:58:17,342 - INFO - Epoch [2/10], Batch [620/747], Loss: 0.4868\n",
      "2025-04-03 15:59:15,289 - INFO - Epoch [2/10], Batch [630/747], Loss: 0.5479\n",
      "2025-04-03 16:00:12,558 - INFO - Epoch [2/10], Batch [640/747], Loss: 0.6686\n",
      "2025-04-03 16:01:10,704 - INFO - Epoch [2/10], Batch [650/747], Loss: 0.5702\n",
      "2025-04-03 16:02:08,799 - INFO - Epoch [2/10], Batch [660/747], Loss: 0.5377\n",
      "2025-04-03 16:03:07,055 - INFO - Epoch [2/10], Batch [670/747], Loss: 0.6167\n",
      "2025-04-03 16:04:05,284 - INFO - Epoch [2/10], Batch [680/747], Loss: 0.5159\n",
      "2025-04-03 16:05:03,386 - INFO - Epoch [2/10], Batch [690/747], Loss: 0.5364\n",
      "2025-04-03 16:06:01,768 - INFO - Epoch [2/10], Batch [700/747], Loss: 0.5783\n",
      "2025-04-03 16:06:59,907 - INFO - Epoch [2/10], Batch [710/747], Loss: 0.5899\n",
      "2025-04-03 16:07:57,893 - INFO - Epoch [2/10], Batch [720/747], Loss: 0.5658\n",
      "2025-04-03 16:08:55,884 - INFO - Epoch [2/10], Batch [730/747], Loss: 0.5223\n",
      "2025-04-03 16:09:53,512 - INFO - Epoch [2/10], Batch [740/747], Loss: 0.5084\n",
      "2025-04-03 16:10:27,398 - INFO - Epoch 2/10 Train Loss: 0.6041, Train Accuracy: 66.42%\n",
      "2025-04-03 16:15:36,234 - INFO - Epoch 2/10 Val Loss: 0.5403, Val Accuracy: 74.35%, AUC-ROC: 0.7640\n",
      "2025-04-03 16:15:36,238 - INFO - New best model at epoch 2 with Val Accuracy: 74.35%\n",
      "2025-04-03 16:15:43,118 - INFO - Epoch [3/10], Batch [0/747], Loss: 0.5158\n",
      "2025-04-03 16:16:41,470 - INFO - Epoch [3/10], Batch [10/747], Loss: 0.7834\n",
      "2025-04-03 16:17:39,878 - INFO - Epoch [3/10], Batch [20/747], Loss: 0.5282\n",
      "2025-04-03 16:18:37,941 - INFO - Epoch [3/10], Batch [30/747], Loss: 0.4910\n",
      "2025-04-03 16:19:35,832 - INFO - Epoch [3/10], Batch [40/747], Loss: 0.8703\n",
      "2025-04-03 16:20:34,147 - INFO - Epoch [3/10], Batch [50/747], Loss: 0.5272\n",
      "2025-04-03 16:21:32,762 - INFO - Epoch [3/10], Batch [60/747], Loss: 0.5326\n",
      "2025-04-03 16:22:31,164 - INFO - Epoch [3/10], Batch [70/747], Loss: 0.6132\n",
      "2025-04-03 16:23:29,689 - INFO - Epoch [3/10], Batch [80/747], Loss: 0.5091\n",
      "2025-04-03 16:24:28,221 - INFO - Epoch [3/10], Batch [90/747], Loss: 0.5406\n",
      "2025-04-03 16:25:26,488 - INFO - Epoch [3/10], Batch [100/747], Loss: 0.5889\n",
      "2025-04-03 16:26:25,169 - INFO - Epoch [3/10], Batch [110/747], Loss: 0.5961\n",
      "2025-04-03 16:27:23,853 - INFO - Epoch [3/10], Batch [120/747], Loss: 0.5763\n",
      "2025-04-03 16:28:21,938 - INFO - Epoch [3/10], Batch [130/747], Loss: 0.6350\n",
      "2025-04-03 16:29:20,100 - INFO - Epoch [3/10], Batch [140/747], Loss: 0.5069\n",
      "2025-04-03 16:30:18,604 - INFO - Epoch [3/10], Batch [150/747], Loss: 0.4829\n",
      "2025-04-03 16:31:17,354 - INFO - Epoch [3/10], Batch [160/747], Loss: 0.5031\n",
      "2025-04-03 16:32:16,204 - INFO - Epoch [3/10], Batch [170/747], Loss: 0.5787\n",
      "2025-04-03 16:33:15,218 - INFO - Epoch [3/10], Batch [180/747], Loss: 0.5764\n",
      "2025-04-03 16:34:13,637 - INFO - Epoch [3/10], Batch [190/747], Loss: 0.5828\n",
      "2025-04-03 16:35:11,726 - INFO - Epoch [3/10], Batch [200/747], Loss: 0.5189\n",
      "2025-04-03 16:36:10,027 - INFO - Epoch [3/10], Batch [210/747], Loss: 0.5688\n",
      "2025-04-03 16:37:08,782 - INFO - Epoch [3/10], Batch [220/747], Loss: 0.7074\n",
      "2025-04-03 16:38:06,685 - INFO - Epoch [3/10], Batch [230/747], Loss: 0.5958\n",
      "2025-04-03 16:39:05,005 - INFO - Epoch [3/10], Batch [240/747], Loss: 0.6237\n",
      "2025-04-03 16:40:03,175 - INFO - Epoch [3/10], Batch [250/747], Loss: 0.6025\n",
      "2025-04-03 16:41:01,263 - INFO - Epoch [3/10], Batch [260/747], Loss: 0.5347\n",
      "2025-04-03 16:41:59,490 - INFO - Epoch [3/10], Batch [270/747], Loss: 0.5659\n",
      "2025-04-03 16:42:57,223 - INFO - Epoch [3/10], Batch [280/747], Loss: 0.5355\n",
      "2025-04-03 16:43:54,837 - INFO - Epoch [3/10], Batch [290/747], Loss: 0.4829\n",
      "2025-04-03 16:44:52,145 - INFO - Epoch [3/10], Batch [300/747], Loss: 0.5364\n",
      "2025-04-03 16:45:49,659 - INFO - Epoch [3/10], Batch [310/747], Loss: 0.4927\n",
      "2025-04-03 16:46:47,086 - INFO - Epoch [3/10], Batch [320/747], Loss: 0.5259\n",
      "2025-04-03 16:47:44,988 - INFO - Epoch [3/10], Batch [330/747], Loss: 0.5175\n",
      "2025-04-03 16:48:43,410 - INFO - Epoch [3/10], Batch [340/747], Loss: 0.6277\n",
      "2025-04-03 16:49:41,894 - INFO - Epoch [3/10], Batch [350/747], Loss: 0.5773\n",
      "2025-04-03 16:50:39,985 - INFO - Epoch [3/10], Batch [360/747], Loss: 0.5867\n",
      "2025-04-03 16:51:38,186 - INFO - Epoch [3/10], Batch [370/747], Loss: 0.5508\n",
      "2025-04-03 16:52:36,123 - INFO - Epoch [3/10], Batch [380/747], Loss: 0.4667\n",
      "2025-04-03 16:53:34,237 - INFO - Epoch [3/10], Batch [390/747], Loss: 0.6887\n",
      "2025-04-03 16:54:31,681 - INFO - Epoch [3/10], Batch [400/747], Loss: 0.5900\n",
      "2025-04-03 16:55:28,827 - INFO - Epoch [3/10], Batch [410/747], Loss: 0.4016\n",
      "2025-04-03 16:56:26,708 - INFO - Epoch [3/10], Batch [420/747], Loss: 0.6101\n",
      "2025-04-03 16:57:25,353 - INFO - Epoch [3/10], Batch [430/747], Loss: 0.5261\n",
      "2025-04-03 16:58:23,740 - INFO - Epoch [3/10], Batch [440/747], Loss: 0.4790\n",
      "2025-04-03 16:59:22,017 - INFO - Epoch [3/10], Batch [450/747], Loss: 0.4739\n",
      "2025-04-03 17:00:20,484 - INFO - Epoch [3/10], Batch [460/747], Loss: 0.6461\n",
      "2025-04-03 17:01:18,379 - INFO - Epoch [3/10], Batch [470/747], Loss: 0.7129\n",
      "2025-04-03 17:02:15,840 - INFO - Epoch [3/10], Batch [480/747], Loss: 0.4727\n",
      "2025-04-03 17:03:13,684 - INFO - Epoch [3/10], Batch [490/747], Loss: 0.5842\n",
      "2025-04-03 17:04:11,651 - INFO - Epoch [3/10], Batch [500/747], Loss: 0.5697\n",
      "2025-04-03 17:05:09,412 - INFO - Epoch [3/10], Batch [510/747], Loss: 0.5334\n",
      "2025-04-03 17:06:07,378 - INFO - Epoch [3/10], Batch [520/747], Loss: 0.5308\n",
      "2025-04-03 17:07:05,317 - INFO - Epoch [3/10], Batch [530/747], Loss: 0.5003\n",
      "2025-04-03 17:08:03,221 - INFO - Epoch [3/10], Batch [540/747], Loss: 0.6766\n",
      "2025-04-03 17:09:00,114 - INFO - Epoch [3/10], Batch [550/747], Loss: 0.6746\n",
      "2025-04-03 17:09:57,184 - INFO - Epoch [3/10], Batch [560/747], Loss: 0.5047\n",
      "2025-04-03 17:10:54,717 - INFO - Epoch [3/10], Batch [570/747], Loss: 0.6564\n",
      "2025-04-03 17:11:52,376 - INFO - Epoch [3/10], Batch [580/747], Loss: 0.5647\n",
      "2025-04-03 17:12:50,057 - INFO - Epoch [3/10], Batch [590/747], Loss: 0.5724\n",
      "2025-04-03 17:13:47,676 - INFO - Epoch [3/10], Batch [600/747], Loss: 0.6291\n",
      "2025-04-03 17:14:45,316 - INFO - Epoch [3/10], Batch [610/747], Loss: 0.5665\n",
      "2025-04-03 17:15:42,728 - INFO - Epoch [3/10], Batch [620/747], Loss: 0.6643\n",
      "2025-04-03 17:16:39,921 - INFO - Epoch [3/10], Batch [630/747], Loss: 0.4436\n",
      "2025-04-03 17:17:37,068 - INFO - Epoch [3/10], Batch [640/747], Loss: 0.5341\n",
      "2025-04-03 17:18:34,598 - INFO - Epoch [3/10], Batch [650/747], Loss: 0.6331\n",
      "2025-04-03 17:19:32,401 - INFO - Epoch [3/10], Batch [660/747], Loss: 0.5242\n",
      "2025-04-03 17:20:29,931 - INFO - Epoch [3/10], Batch [670/747], Loss: 0.4623\n",
      "2025-04-03 17:21:27,136 - INFO - Epoch [3/10], Batch [680/747], Loss: 0.6250\n",
      "2025-04-03 17:22:24,477 - INFO - Epoch [3/10], Batch [690/747], Loss: 0.4273\n",
      "2025-04-03 17:23:22,366 - INFO - Epoch [3/10], Batch [700/747], Loss: 0.6547\n",
      "2025-04-03 17:24:20,087 - INFO - Epoch [3/10], Batch [710/747], Loss: 0.5534\n",
      "2025-04-03 17:25:17,992 - INFO - Epoch [3/10], Batch [720/747], Loss: 0.6412\n",
      "2025-04-03 17:26:15,921 - INFO - Epoch [3/10], Batch [730/747], Loss: 0.4131\n",
      "2025-04-03 17:27:13,493 - INFO - Epoch [3/10], Batch [740/747], Loss: 0.4355\n",
      "2025-04-03 17:27:47,067 - INFO - Epoch 3/10 Train Loss: 0.5713, Train Accuracy: 69.29%\n",
      "2025-04-03 17:32:55,831 - INFO - Epoch 3/10 Val Loss: 0.5012, Val Accuracy: 76.63%, AUC-ROC: 0.8146\n",
      "2025-04-03 17:32:55,835 - INFO - New best model at epoch 3 with Val Accuracy: 76.63%\n",
      "2025-04-03 17:33:03,323 - INFO - Epoch [4/10], Batch [0/747], Loss: 0.5177\n",
      "2025-04-03 17:34:01,165 - INFO - Epoch [4/10], Batch [10/747], Loss: 0.4004\n",
      "2025-04-03 17:34:58,935 - INFO - Epoch [4/10], Batch [20/747], Loss: 0.6113\n",
      "2025-04-03 17:35:56,832 - INFO - Epoch [4/10], Batch [30/747], Loss: 0.4932\n",
      "2025-04-03 17:36:54,231 - INFO - Epoch [4/10], Batch [40/747], Loss: 0.5990\n",
      "2025-04-03 17:37:51,975 - INFO - Epoch [4/10], Batch [50/747], Loss: 0.5904\n",
      "2025-04-03 17:38:49,613 - INFO - Epoch [4/10], Batch [60/747], Loss: 0.5408\n",
      "2025-04-03 17:39:47,666 - INFO - Epoch [4/10], Batch [70/747], Loss: 0.6525\n",
      "2025-04-03 17:40:45,196 - INFO - Epoch [4/10], Batch [80/747], Loss: 0.5200\n",
      "2025-04-03 17:41:42,500 - INFO - Epoch [4/10], Batch [90/747], Loss: 0.6933\n",
      "2025-04-03 17:42:40,105 - INFO - Epoch [4/10], Batch [100/747], Loss: 0.4944\n",
      "2025-04-03 17:43:37,564 - INFO - Epoch [4/10], Batch [110/747], Loss: 0.6909\n",
      "2025-04-03 17:44:35,123 - INFO - Epoch [4/10], Batch [120/747], Loss: 0.4882\n",
      "2025-04-03 17:45:33,143 - INFO - Epoch [4/10], Batch [130/747], Loss: 0.3416\n",
      "2025-04-03 17:46:31,057 - INFO - Epoch [4/10], Batch [140/747], Loss: 0.5453\n",
      "2025-04-03 17:47:28,168 - INFO - Epoch [4/10], Batch [150/747], Loss: 0.4951\n",
      "2025-04-03 17:48:25,590 - INFO - Epoch [4/10], Batch [160/747], Loss: 0.4825\n",
      "2025-04-03 17:49:23,178 - INFO - Epoch [4/10], Batch [170/747], Loss: 0.6075\n",
      "2025-04-03 17:50:20,504 - INFO - Epoch [4/10], Batch [180/747], Loss: 0.4994\n",
      "2025-04-03 17:51:17,545 - INFO - Epoch [4/10], Batch [190/747], Loss: 0.5986\n",
      "2025-04-03 17:52:14,323 - INFO - Epoch [4/10], Batch [200/747], Loss: 0.5253\n",
      "2025-04-03 17:53:11,898 - INFO - Epoch [4/10], Batch [210/747], Loss: 0.7194\n",
      "2025-04-03 17:54:09,677 - INFO - Epoch [4/10], Batch [220/747], Loss: 0.4553\n",
      "2025-04-03 17:55:06,916 - INFO - Epoch [4/10], Batch [230/747], Loss: 0.4892\n",
      "2025-04-03 17:56:05,084 - INFO - Epoch [4/10], Batch [240/747], Loss: 0.5367\n",
      "2025-04-03 17:57:02,890 - INFO - Epoch [4/10], Batch [250/747], Loss: 0.5409\n",
      "2025-04-03 17:58:00,888 - INFO - Epoch [4/10], Batch [260/747], Loss: 0.4846\n",
      "2025-04-03 17:58:58,806 - INFO - Epoch [4/10], Batch [270/747], Loss: 0.5040\n",
      "2025-04-03 17:59:56,616 - INFO - Epoch [4/10], Batch [280/747], Loss: 0.5988\n",
      "2025-04-03 18:00:54,165 - INFO - Epoch [4/10], Batch [290/747], Loss: 0.4721\n",
      "2025-04-03 18:01:52,270 - INFO - Epoch [4/10], Batch [300/747], Loss: 0.4376\n",
      "2025-04-03 18:02:50,312 - INFO - Epoch [4/10], Batch [310/747], Loss: 0.4854\n",
      "2025-04-03 18:03:48,245 - INFO - Epoch [4/10], Batch [320/747], Loss: 0.5776\n",
      "2025-04-03 18:04:46,326 - INFO - Epoch [4/10], Batch [330/747], Loss: 0.6050\n",
      "2025-04-03 18:05:44,243 - INFO - Epoch [4/10], Batch [340/747], Loss: 0.5107\n",
      "2025-04-03 18:06:42,110 - INFO - Epoch [4/10], Batch [350/747], Loss: 0.4044\n",
      "2025-04-03 18:07:40,485 - INFO - Epoch [4/10], Batch [360/747], Loss: 0.4585\n",
      "2025-04-03 18:08:38,648 - INFO - Epoch [4/10], Batch [370/747], Loss: 0.4821\n",
      "2025-04-03 18:09:36,757 - INFO - Epoch [4/10], Batch [380/747], Loss: 0.5349\n",
      "2025-04-03 18:10:34,691 - INFO - Epoch [4/10], Batch [390/747], Loss: 0.4168\n",
      "2025-04-03 18:11:32,432 - INFO - Epoch [4/10], Batch [400/747], Loss: 0.3925\n",
      "2025-04-03 18:12:30,039 - INFO - Epoch [4/10], Batch [410/747], Loss: 0.5943\n",
      "2025-04-03 18:13:27,532 - INFO - Epoch [4/10], Batch [420/747], Loss: 0.4462\n",
      "2025-04-03 18:14:25,205 - INFO - Epoch [4/10], Batch [430/747], Loss: 0.4283\n",
      "2025-04-03 18:15:23,086 - INFO - Epoch [4/10], Batch [440/747], Loss: 0.3338\n",
      "2025-04-03 18:16:21,028 - INFO - Epoch [4/10], Batch [450/747], Loss: 0.4111\n",
      "2025-04-03 18:17:18,080 - INFO - Epoch [4/10], Batch [460/747], Loss: 0.3314\n",
      "2025-04-03 18:18:15,582 - INFO - Epoch [4/10], Batch [470/747], Loss: 0.5261\n",
      "2025-04-03 18:19:13,630 - INFO - Epoch [4/10], Batch [480/747], Loss: 0.4219\n",
      "2025-04-03 18:20:11,635 - INFO - Epoch [4/10], Batch [490/747], Loss: 0.5395\n",
      "2025-04-03 18:21:09,151 - INFO - Epoch [4/10], Batch [500/747], Loss: 0.6107\n",
      "2025-04-03 18:22:06,460 - INFO - Epoch [4/10], Batch [510/747], Loss: 0.3967\n",
      "2025-04-03 18:23:04,398 - INFO - Epoch [4/10], Batch [520/747], Loss: 0.5120\n",
      "2025-04-03 18:24:02,272 - INFO - Epoch [4/10], Batch [530/747], Loss: 0.5103\n",
      "2025-04-03 18:25:00,351 - INFO - Epoch [4/10], Batch [540/747], Loss: 0.3692\n",
      "2025-04-03 18:25:58,534 - INFO - Epoch [4/10], Batch [550/747], Loss: 0.6334\n",
      "2025-04-03 18:26:56,208 - INFO - Epoch [4/10], Batch [560/747], Loss: 0.4256\n",
      "2025-04-03 18:27:53,752 - INFO - Epoch [4/10], Batch [570/747], Loss: 0.4645\n",
      "2025-04-03 18:28:51,185 - INFO - Epoch [4/10], Batch [580/747], Loss: 0.4116\n",
      "2025-04-03 18:29:48,759 - INFO - Epoch [4/10], Batch [590/747], Loss: 0.3761\n",
      "2025-04-03 18:30:46,641 - INFO - Epoch [4/10], Batch [600/747], Loss: 0.5128\n",
      "2025-04-03 18:31:44,142 - INFO - Epoch [4/10], Batch [610/747], Loss: 0.4617\n",
      "2025-04-03 18:32:41,749 - INFO - Epoch [4/10], Batch [620/747], Loss: 0.4742\n",
      "2025-04-03 18:33:39,117 - INFO - Epoch [4/10], Batch [630/747], Loss: 0.7375\n",
      "2025-04-03 18:34:36,896 - INFO - Epoch [4/10], Batch [640/747], Loss: 0.7603\n",
      "2025-04-03 18:35:34,833 - INFO - Epoch [4/10], Batch [650/747], Loss: 0.4543\n",
      "2025-04-03 18:36:32,814 - INFO - Epoch [4/10], Batch [660/747], Loss: 0.4039\n",
      "2025-04-03 18:37:31,017 - INFO - Epoch [4/10], Batch [670/747], Loss: 0.4741\n",
      "2025-04-03 18:38:29,034 - INFO - Epoch [4/10], Batch [680/747], Loss: 0.5343\n",
      "2025-04-03 18:39:27,188 - INFO - Epoch [4/10], Batch [690/747], Loss: 0.3946\n",
      "2025-04-03 18:40:25,316 - INFO - Epoch [4/10], Batch [700/747], Loss: 0.5519\n",
      "2025-04-03 18:41:23,556 - INFO - Epoch [4/10], Batch [710/747], Loss: 0.5437\n",
      "2025-04-03 18:42:21,772 - INFO - Epoch [4/10], Batch [720/747], Loss: 0.5714\n",
      "2025-04-03 18:43:19,333 - INFO - Epoch [4/10], Batch [730/747], Loss: 0.3244\n",
      "2025-04-03 18:44:17,048 - INFO - Epoch [4/10], Batch [740/747], Loss: 0.2705\n",
      "2025-04-03 18:44:50,526 - INFO - Epoch 4/10 Train Loss: 0.5010, Train Accuracy: 74.90%\n",
      "2025-04-03 18:49:54,775 - INFO - Epoch 4/10 Val Loss: 0.4203, Val Accuracy: 81.35%, AUC-ROC: 0.8799\n",
      "2025-04-03 18:49:54,779 - INFO - New best model at epoch 4 with Val Accuracy: 81.35%\n",
      "2025-04-03 18:50:01,689 - INFO - Epoch [5/10], Batch [0/747], Loss: 0.5177\n",
      "2025-04-03 18:50:58,904 - INFO - Epoch [5/10], Batch [10/747], Loss: 0.6118\n",
      "2025-04-03 18:51:56,766 - INFO - Epoch [5/10], Batch [20/747], Loss: 0.4750\n",
      "2025-04-03 18:52:54,665 - INFO - Epoch [5/10], Batch [30/747], Loss: 0.5496\n",
      "2025-04-03 18:53:53,251 - INFO - Epoch [5/10], Batch [40/747], Loss: 0.4265\n",
      "2025-04-03 18:54:50,836 - INFO - Epoch [5/10], Batch [50/747], Loss: 0.4594\n",
      "2025-04-03 18:55:48,225 - INFO - Epoch [5/10], Batch [60/747], Loss: 0.2796\n",
      "2025-04-03 18:56:45,920 - INFO - Epoch [5/10], Batch [70/747], Loss: 0.4380\n",
      "2025-04-03 18:57:43,440 - INFO - Epoch [5/10], Batch [80/747], Loss: 0.4908\n",
      "2025-04-03 18:58:41,208 - INFO - Epoch [5/10], Batch [90/747], Loss: 0.5646\n",
      "2025-04-03 18:59:38,834 - INFO - Epoch [5/10], Batch [100/747], Loss: 0.4507\n",
      "2025-04-03 19:00:36,244 - INFO - Epoch [5/10], Batch [110/747], Loss: 0.6362\n",
      "2025-04-03 19:01:33,377 - INFO - Epoch [5/10], Batch [120/747], Loss: 0.3109\n",
      "2025-04-03 19:02:30,518 - INFO - Epoch [5/10], Batch [130/747], Loss: 0.3896\n",
      "2025-04-03 19:03:28,004 - INFO - Epoch [5/10], Batch [140/747], Loss: 0.4672\n",
      "2025-04-03 19:04:25,325 - INFO - Epoch [5/10], Batch [150/747], Loss: 0.3606\n",
      "2025-04-03 19:05:22,879 - INFO - Epoch [5/10], Batch [160/747], Loss: 0.9066\n",
      "2025-04-03 19:06:20,264 - INFO - Epoch [5/10], Batch [170/747], Loss: 0.3985\n",
      "2025-04-03 19:07:17,931 - INFO - Epoch [5/10], Batch [180/747], Loss: 0.3200\n",
      "2025-04-03 19:08:15,117 - INFO - Epoch [5/10], Batch [190/747], Loss: 0.3941\n",
      "2025-04-03 19:09:12,127 - INFO - Epoch [5/10], Batch [200/747], Loss: 0.5227\n",
      "2025-04-03 19:10:09,698 - INFO - Epoch [5/10], Batch [210/747], Loss: 0.4129\n",
      "2025-04-03 19:11:07,627 - INFO - Epoch [5/10], Batch [220/747], Loss: 0.4635\n",
      "2025-04-03 19:12:05,125 - INFO - Epoch [5/10], Batch [230/747], Loss: 0.5959\n",
      "2025-04-03 19:13:02,747 - INFO - Epoch [5/10], Batch [240/747], Loss: 0.4220\n",
      "2025-04-03 19:14:00,635 - INFO - Epoch [5/10], Batch [250/747], Loss: 0.4601\n",
      "2025-04-03 19:14:58,068 - INFO - Epoch [5/10], Batch [260/747], Loss: 0.5329\n",
      "2025-04-03 19:15:55,380 - INFO - Epoch [5/10], Batch [270/747], Loss: 0.4520\n",
      "2025-04-03 19:16:53,256 - INFO - Epoch [5/10], Batch [280/747], Loss: 0.3355\n",
      "2025-04-03 19:17:51,010 - INFO - Epoch [5/10], Batch [290/747], Loss: 0.5341\n",
      "2025-04-03 19:18:48,424 - INFO - Epoch [5/10], Batch [300/747], Loss: 0.3900\n",
      "2025-04-03 19:19:45,858 - INFO - Epoch [5/10], Batch [310/747], Loss: 0.5876\n",
      "2025-04-03 19:20:43,505 - INFO - Epoch [5/10], Batch [320/747], Loss: 0.4364\n",
      "2025-04-03 19:21:41,450 - INFO - Epoch [5/10], Batch [330/747], Loss: 0.3208\n",
      "2025-04-03 19:22:38,962 - INFO - Epoch [5/10], Batch [340/747], Loss: 0.5129\n",
      "2025-04-03 19:23:36,110 - INFO - Epoch [5/10], Batch [350/747], Loss: 0.5938\n",
      "2025-04-03 19:24:33,423 - INFO - Epoch [5/10], Batch [360/747], Loss: 0.3621\n",
      "2025-04-03 19:25:30,903 - INFO - Epoch [5/10], Batch [370/747], Loss: 0.6878\n",
      "2025-04-03 19:26:29,117 - INFO - Epoch [5/10], Batch [380/747], Loss: 0.5649\n",
      "2025-04-03 19:27:26,761 - INFO - Epoch [5/10], Batch [390/747], Loss: 0.4385\n",
      "2025-04-03 19:28:24,751 - INFO - Epoch [5/10], Batch [400/747], Loss: 0.5037\n",
      "2025-04-03 19:29:22,302 - INFO - Epoch [5/10], Batch [410/747], Loss: 0.3974\n",
      "2025-04-03 19:30:19,697 - INFO - Epoch [5/10], Batch [420/747], Loss: 0.4514\n",
      "2025-04-03 19:31:17,006 - INFO - Epoch [5/10], Batch [430/747], Loss: 0.5301\n",
      "2025-04-03 19:32:14,392 - INFO - Epoch [5/10], Batch [440/747], Loss: 0.4060\n",
      "2025-04-03 19:33:11,992 - INFO - Epoch [5/10], Batch [450/747], Loss: 0.4960\n",
      "2025-04-03 19:34:09,052 - INFO - Epoch [5/10], Batch [460/747], Loss: 0.7799\n",
      "2025-04-03 19:35:06,357 - INFO - Epoch [5/10], Batch [470/747], Loss: 0.4526\n",
      "2025-04-03 19:36:03,573 - INFO - Epoch [5/10], Batch [480/747], Loss: 0.3556\n",
      "2025-04-03 19:37:01,758 - INFO - Epoch [5/10], Batch [490/747], Loss: 0.4758\n",
      "2025-04-03 19:37:59,845 - INFO - Epoch [5/10], Batch [500/747], Loss: 0.5085\n",
      "2025-04-03 19:38:57,144 - INFO - Epoch [5/10], Batch [510/747], Loss: 0.5105\n",
      "2025-04-03 19:39:53,901 - INFO - Epoch [5/10], Batch [520/747], Loss: 0.6241\n",
      "2025-04-03 19:40:51,163 - INFO - Epoch [5/10], Batch [530/747], Loss: 0.5452\n",
      "2025-04-03 19:41:48,566 - INFO - Epoch [5/10], Batch [540/747], Loss: 0.4148\n",
      "2025-04-03 19:42:45,912 - INFO - Epoch [5/10], Batch [550/747], Loss: 0.3557\n",
      "2025-04-03 19:43:43,370 - INFO - Epoch [5/10], Batch [560/747], Loss: 0.2855\n",
      "2025-04-03 19:44:40,401 - INFO - Epoch [5/10], Batch [570/747], Loss: 0.5017\n",
      "2025-04-03 19:45:37,552 - INFO - Epoch [5/10], Batch [580/747], Loss: 0.4163\n",
      "2025-04-03 19:46:34,959 - INFO - Epoch [5/10], Batch [590/747], Loss: 0.4442\n",
      "2025-04-03 19:47:32,659 - INFO - Epoch [5/10], Batch [600/747], Loss: 0.3284\n",
      "2025-04-03 19:48:30,113 - INFO - Epoch [5/10], Batch [610/747], Loss: 0.3489\n",
      "2025-04-03 19:49:27,472 - INFO - Epoch [5/10], Batch [620/747], Loss: 0.3094\n",
      "2025-04-03 19:50:25,032 - INFO - Epoch [5/10], Batch [630/747], Loss: 0.5675\n",
      "2025-04-03 19:51:22,608 - INFO - Epoch [5/10], Batch [640/747], Loss: 0.4614\n",
      "2025-04-03 19:52:20,200 - INFO - Epoch [5/10], Batch [650/747], Loss: 0.3969\n",
      "2025-04-03 19:53:17,663 - INFO - Epoch [5/10], Batch [660/747], Loss: 0.4852\n",
      "2025-04-03 19:54:14,747 - INFO - Epoch [5/10], Batch [670/747], Loss: 0.3997\n",
      "2025-04-03 19:55:12,076 - INFO - Epoch [5/10], Batch [680/747], Loss: 0.3397\n",
      "2025-04-03 19:56:09,789 - INFO - Epoch [5/10], Batch [690/747], Loss: 0.5183\n",
      "2025-04-03 19:57:07,538 - INFO - Epoch [5/10], Batch [700/747], Loss: 0.6168\n",
      "2025-04-03 19:58:04,824 - INFO - Epoch [5/10], Batch [710/747], Loss: 0.4071\n",
      "2025-04-03 19:59:02,539 - INFO - Epoch [5/10], Batch [720/747], Loss: 0.3833\n",
      "2025-04-03 20:00:00,098 - INFO - Epoch [5/10], Batch [730/747], Loss: 0.3672\n",
      "2025-04-03 20:00:57,389 - INFO - Epoch [5/10], Batch [740/747], Loss: 0.5240\n",
      "2025-04-03 20:01:30,780 - INFO - Epoch 5/10 Train Loss: 0.4546, Train Accuracy: 77.89%\n",
      "2025-04-03 20:06:39,203 - INFO - Epoch 5/10 Val Loss: 0.3801, Val Accuracy: 82.65%, AUC-ROC: 0.8999\n",
      "2025-04-03 20:06:39,209 - INFO - New best model at epoch 5 with Val Accuracy: 82.65%\n",
      "2025-04-03 20:06:46,095 - INFO - Epoch [6/10], Batch [0/747], Loss: 0.3525\n",
      "2025-04-03 20:07:44,753 - INFO - Epoch [6/10], Batch [10/747], Loss: 0.4308\n",
      "2025-04-03 20:08:43,869 - INFO - Epoch [6/10], Batch [20/747], Loss: 0.3556\n",
      "2025-04-03 20:09:42,980 - INFO - Epoch [6/10], Batch [30/747], Loss: 0.4416\n",
      "2025-04-03 20:10:42,240 - INFO - Epoch [6/10], Batch [40/747], Loss: 0.3516\n",
      "2025-04-03 20:11:41,465 - INFO - Epoch [6/10], Batch [50/747], Loss: 0.4084\n",
      "2025-04-03 20:12:40,061 - INFO - Epoch [6/10], Batch [60/747], Loss: 0.4883\n",
      "2025-04-03 20:13:39,035 - INFO - Epoch [6/10], Batch [70/747], Loss: 0.4430\n",
      "2025-04-03 20:14:38,076 - INFO - Epoch [6/10], Batch [80/747], Loss: 0.5064\n",
      "2025-04-03 20:15:37,199 - INFO - Epoch [6/10], Batch [90/747], Loss: 0.3388\n",
      "2025-04-03 20:16:35,740 - INFO - Epoch [6/10], Batch [100/747], Loss: 0.4844\n",
      "2025-04-03 20:17:34,051 - INFO - Epoch [6/10], Batch [110/747], Loss: 0.5140\n",
      "2025-04-03 20:18:32,441 - INFO - Epoch [6/10], Batch [120/747], Loss: 0.4331\n",
      "2025-04-03 20:19:31,018 - INFO - Epoch [6/10], Batch [130/747], Loss: 0.2365\n",
      "2025-04-03 20:20:29,520 - INFO - Epoch [6/10], Batch [140/747], Loss: 0.3427\n",
      "2025-04-03 20:21:28,180 - INFO - Epoch [6/10], Batch [150/747], Loss: 0.4398\n",
      "2025-04-03 20:22:26,837 - INFO - Epoch [6/10], Batch [160/747], Loss: 0.2396\n",
      "2025-04-03 20:23:25,925 - INFO - Epoch [6/10], Batch [170/747], Loss: 0.2355\n",
      "2025-04-03 20:24:24,345 - INFO - Epoch [6/10], Batch [180/747], Loss: 0.3747\n",
      "2025-04-03 20:25:23,179 - INFO - Epoch [6/10], Batch [190/747], Loss: 0.5160\n",
      "2025-04-03 20:26:22,371 - INFO - Epoch [6/10], Batch [200/747], Loss: 0.4083\n",
      "2025-04-03 20:27:20,850 - INFO - Epoch [6/10], Batch [210/747], Loss: 0.4687\n",
      "2025-04-03 20:28:19,509 - INFO - Epoch [6/10], Batch [220/747], Loss: 0.4087\n",
      "2025-04-03 20:29:17,620 - INFO - Epoch [6/10], Batch [230/747], Loss: 0.4432\n",
      "2025-04-03 20:30:16,596 - INFO - Epoch [6/10], Batch [240/747], Loss: 0.5016\n",
      "2025-04-03 20:31:14,921 - INFO - Epoch [6/10], Batch [250/747], Loss: 0.4823\n",
      "2025-04-03 20:32:14,280 - INFO - Epoch [6/10], Batch [260/747], Loss: 0.4438\n",
      "2025-04-03 20:33:12,926 - INFO - Epoch [6/10], Batch [270/747], Loss: 0.5056\n",
      "2025-04-03 20:34:11,415 - INFO - Epoch [6/10], Batch [280/747], Loss: 0.3909\n",
      "2025-04-03 20:35:10,085 - INFO - Epoch [6/10], Batch [290/747], Loss: 0.5256\n",
      "2025-04-03 20:36:08,442 - INFO - Epoch [6/10], Batch [300/747], Loss: 0.5332\n",
      "2025-04-03 20:37:07,406 - INFO - Epoch [6/10], Batch [310/747], Loss: 0.5259\n",
      "2025-04-03 20:38:05,987 - INFO - Epoch [6/10], Batch [320/747], Loss: 0.5120\n",
      "2025-04-03 20:39:04,865 - INFO - Epoch [6/10], Batch [330/747], Loss: 0.3185\n",
      "2025-04-03 20:40:03,748 - INFO - Epoch [6/10], Batch [340/747], Loss: 0.3423\n",
      "2025-04-03 20:41:02,101 - INFO - Epoch [6/10], Batch [350/747], Loss: 0.3112\n",
      "2025-04-03 20:42:01,093 - INFO - Epoch [6/10], Batch [360/747], Loss: 0.4381\n",
      "2025-04-03 20:42:59,695 - INFO - Epoch [6/10], Batch [370/747], Loss: 0.4004\n",
      "2025-04-03 20:43:58,296 - INFO - Epoch [6/10], Batch [380/747], Loss: 0.4104\n",
      "2025-04-03 20:44:57,109 - INFO - Epoch [6/10], Batch [390/747], Loss: 0.4031\n",
      "2025-04-03 20:45:55,973 - INFO - Epoch [6/10], Batch [400/747], Loss: 0.2938\n",
      "2025-04-03 20:46:53,921 - INFO - Epoch [6/10], Batch [410/747], Loss: 0.4085\n",
      "2025-04-03 20:47:52,128 - INFO - Epoch [6/10], Batch [420/747], Loss: 0.2949\n",
      "2025-04-03 20:48:50,804 - INFO - Epoch [6/10], Batch [430/747], Loss: 0.4884\n",
      "2025-04-03 20:49:49,399 - INFO - Epoch [6/10], Batch [440/747], Loss: 0.3114\n",
      "2025-04-03 20:50:48,211 - INFO - Epoch [6/10], Batch [450/747], Loss: 0.4889\n",
      "2025-04-03 20:51:47,197 - INFO - Epoch [6/10], Batch [460/747], Loss: 0.4031\n",
      "2025-04-03 20:52:46,097 - INFO - Epoch [6/10], Batch [470/747], Loss: 0.3732\n",
      "2025-04-03 20:53:44,661 - INFO - Epoch [6/10], Batch [480/747], Loss: 0.3244\n",
      "2025-04-03 20:54:43,470 - INFO - Epoch [6/10], Batch [490/747], Loss: 0.5635\n",
      "2025-04-03 20:55:42,576 - INFO - Epoch [6/10], Batch [500/747], Loss: 0.3975\n",
      "2025-04-03 20:56:41,402 - INFO - Epoch [6/10], Batch [510/747], Loss: 0.6592\n",
      "2025-04-03 20:57:40,188 - INFO - Epoch [6/10], Batch [520/747], Loss: 0.3880\n",
      "2025-04-03 20:58:39,366 - INFO - Epoch [6/10], Batch [530/747], Loss: 0.4056\n",
      "2025-04-03 20:59:38,705 - INFO - Epoch [6/10], Batch [540/747], Loss: 0.4022\n",
      "2025-04-03 21:00:37,575 - INFO - Epoch [6/10], Batch [550/747], Loss: 0.4167\n",
      "2025-04-03 21:01:36,568 - INFO - Epoch [6/10], Batch [560/747], Loss: 0.4595\n",
      "2025-04-03 21:02:34,857 - INFO - Epoch [6/10], Batch [570/747], Loss: 0.4032\n",
      "2025-04-03 21:03:33,345 - INFO - Epoch [6/10], Batch [580/747], Loss: 0.2908\n",
      "2025-04-03 21:04:31,495 - INFO - Epoch [6/10], Batch [590/747], Loss: 0.2545\n",
      "2025-04-03 21:05:29,152 - INFO - Epoch [6/10], Batch [600/747], Loss: 0.2317\n",
      "2025-04-03 21:06:27,207 - INFO - Epoch [6/10], Batch [610/747], Loss: 0.3852\n",
      "2025-04-03 21:07:26,163 - INFO - Epoch [6/10], Batch [620/747], Loss: 0.2947\n",
      "2025-04-03 21:08:24,950 - INFO - Epoch [6/10], Batch [630/747], Loss: 0.3088\n",
      "2025-04-03 21:09:23,594 - INFO - Epoch [6/10], Batch [640/747], Loss: 0.4611\n",
      "2025-04-03 21:10:22,532 - INFO - Epoch [6/10], Batch [650/747], Loss: 0.2898\n",
      "2025-04-03 21:11:21,128 - INFO - Epoch [6/10], Batch [660/747], Loss: 0.3908\n",
      "2025-04-03 21:12:19,922 - INFO - Epoch [6/10], Batch [670/747], Loss: 0.4766\n",
      "2025-04-03 21:13:18,465 - INFO - Epoch [6/10], Batch [680/747], Loss: 0.2541\n",
      "2025-04-03 21:14:17,110 - INFO - Epoch [6/10], Batch [690/747], Loss: 0.3088\n",
      "2025-04-03 21:15:15,995 - INFO - Epoch [6/10], Batch [700/747], Loss: 0.3365\n",
      "2025-04-03 21:16:14,890 - INFO - Epoch [6/10], Batch [710/747], Loss: 0.4498\n",
      "2025-04-03 21:17:12,945 - INFO - Epoch [6/10], Batch [720/747], Loss: 0.2245\n",
      "2025-04-03 21:18:11,935 - INFO - Epoch [6/10], Batch [730/747], Loss: 0.3207\n",
      "2025-04-03 21:19:10,924 - INFO - Epoch [6/10], Batch [740/747], Loss: 0.2732\n",
      "2025-04-03 21:19:45,483 - INFO - Epoch 6/10 Train Loss: 0.4201, Train Accuracy: 80.00%\n",
      "2025-04-03 21:24:53,392 - INFO - Epoch 6/10 Val Loss: 0.3390, Val Accuracy: 85.40%, AUC-ROC: 0.9203\n",
      "2025-04-03 21:24:53,397 - INFO - New best model at epoch 6 with Val Accuracy: 85.40%\n",
      "2025-04-03 21:25:01,059 - INFO - Epoch [7/10], Batch [0/747], Loss: 0.3179\n",
      "2025-04-03 21:25:59,055 - INFO - Epoch [7/10], Batch [10/747], Loss: 0.4018\n",
      "2025-04-03 21:26:56,754 - INFO - Epoch [7/10], Batch [20/747], Loss: 0.6728\n",
      "2025-04-03 21:27:54,033 - INFO - Epoch [7/10], Batch [30/747], Loss: 0.4154\n",
      "2025-04-03 21:28:51,802 - INFO - Epoch [7/10], Batch [40/747], Loss: 0.4441\n",
      "2025-04-03 21:29:49,748 - INFO - Epoch [7/10], Batch [50/747], Loss: 0.3513\n",
      "2025-04-03 21:30:47,584 - INFO - Epoch [7/10], Batch [60/747], Loss: 0.4982\n",
      "2025-04-03 21:31:45,415 - INFO - Epoch [7/10], Batch [70/747], Loss: 0.3133\n",
      "2025-04-03 21:32:42,950 - INFO - Epoch [7/10], Batch [80/747], Loss: 0.4627\n",
      "2025-04-03 21:33:40,350 - INFO - Epoch [7/10], Batch [90/747], Loss: 0.2345\n",
      "2025-04-03 21:34:38,092 - INFO - Epoch [7/10], Batch [100/747], Loss: 0.3936\n",
      "2025-04-03 21:35:36,204 - INFO - Epoch [7/10], Batch [110/747], Loss: 0.3755\n",
      "2025-04-03 21:36:33,759 - INFO - Epoch [7/10], Batch [120/747], Loss: 0.8298\n",
      "2025-04-03 21:37:31,046 - INFO - Epoch [7/10], Batch [130/747], Loss: 0.3465\n",
      "2025-04-03 21:38:28,452 - INFO - Epoch [7/10], Batch [140/747], Loss: 0.3907\n",
      "2025-04-03 21:39:25,999 - INFO - Epoch [7/10], Batch [150/747], Loss: 0.3301\n",
      "2025-04-03 21:40:23,846 - INFO - Epoch [7/10], Batch [160/747], Loss: 0.6506\n",
      "2025-04-03 21:41:22,251 - INFO - Epoch [7/10], Batch [170/747], Loss: 0.3893\n",
      "2025-04-03 21:42:20,646 - INFO - Epoch [7/10], Batch [180/747], Loss: 0.5299\n",
      "2025-04-03 21:43:18,758 - INFO - Epoch [7/10], Batch [190/747], Loss: 0.4773\n",
      "2025-04-03 21:44:16,502 - INFO - Epoch [7/10], Batch [200/747], Loss: 0.3440\n",
      "2025-04-03 21:45:14,212 - INFO - Epoch [7/10], Batch [210/747], Loss: 0.3359\n",
      "2025-04-03 21:46:12,511 - INFO - Epoch [7/10], Batch [220/747], Loss: 0.4488\n",
      "2025-04-03 21:47:11,087 - INFO - Epoch [7/10], Batch [230/747], Loss: 0.2313\n",
      "2025-04-03 21:48:09,194 - INFO - Epoch [7/10], Batch [240/747], Loss: 0.4329\n",
      "2025-04-03 21:49:07,342 - INFO - Epoch [7/10], Batch [250/747], Loss: 0.4829\n",
      "2025-04-03 21:50:05,420 - INFO - Epoch [7/10], Batch [260/747], Loss: 0.5203\n",
      "2025-04-03 21:51:03,561 - INFO - Epoch [7/10], Batch [270/747], Loss: 0.4467\n",
      "2025-04-03 21:52:01,663 - INFO - Epoch [7/10], Batch [280/747], Loss: 0.2598\n",
      "2025-04-03 21:52:59,202 - INFO - Epoch [7/10], Batch [290/747], Loss: 0.2978\n",
      "2025-04-03 21:53:57,394 - INFO - Epoch [7/10], Batch [300/747], Loss: 0.3343\n",
      "2025-04-03 21:54:55,662 - INFO - Epoch [7/10], Batch [310/747], Loss: 0.2826\n",
      "2025-04-03 21:55:54,011 - INFO - Epoch [7/10], Batch [320/747], Loss: 0.3782\n",
      "2025-04-03 21:56:51,951 - INFO - Epoch [7/10], Batch [330/747], Loss: 0.2110\n",
      "2025-04-03 21:57:49,665 - INFO - Epoch [7/10], Batch [340/747], Loss: 0.6486\n",
      "2025-04-03 21:58:47,972 - INFO - Epoch [7/10], Batch [350/747], Loss: 0.3051\n",
      "2025-04-03 21:59:46,230 - INFO - Epoch [7/10], Batch [360/747], Loss: 0.2410\n",
      "2025-04-03 22:00:44,412 - INFO - Epoch [7/10], Batch [370/747], Loss: 0.3757\n",
      "2025-04-03 22:01:42,553 - INFO - Epoch [7/10], Batch [380/747], Loss: 0.2967\n",
      "2025-04-03 22:02:40,327 - INFO - Epoch [7/10], Batch [390/747], Loss: 0.1949\n",
      "2025-04-03 22:03:38,030 - INFO - Epoch [7/10], Batch [400/747], Loss: 0.3165\n",
      "2025-04-03 22:04:35,806 - INFO - Epoch [7/10], Batch [410/747], Loss: 0.4204\n",
      "2025-04-03 22:05:33,680 - INFO - Epoch [7/10], Batch [420/747], Loss: 0.3853\n",
      "2025-04-03 22:06:31,673 - INFO - Epoch [7/10], Batch [430/747], Loss: 0.3138\n",
      "2025-04-03 22:07:29,772 - INFO - Epoch [7/10], Batch [440/747], Loss: 0.3717\n",
      "2025-04-03 22:08:28,001 - INFO - Epoch [7/10], Batch [450/747], Loss: 0.3784\n",
      "2025-04-03 22:09:25,978 - INFO - Epoch [7/10], Batch [460/747], Loss: 0.3569\n",
      "2025-04-03 22:10:23,915 - INFO - Epoch [7/10], Batch [470/747], Loss: 0.3988\n",
      "2025-04-03 22:11:21,648 - INFO - Epoch [7/10], Batch [480/747], Loss: 0.6408\n",
      "2025-04-03 22:12:19,430 - INFO - Epoch [7/10], Batch [490/747], Loss: 0.4006\n",
      "2025-04-03 22:13:17,388 - INFO - Epoch [7/10], Batch [500/747], Loss: 0.3520\n",
      "2025-04-03 22:14:15,459 - INFO - Epoch [7/10], Batch [510/747], Loss: 0.3500\n",
      "2025-04-03 22:15:13,406 - INFO - Epoch [7/10], Batch [520/747], Loss: 0.2843\n",
      "2025-04-03 22:16:11,139 - INFO - Epoch [7/10], Batch [530/747], Loss: 0.4036\n",
      "2025-04-03 22:17:09,286 - INFO - Epoch [7/10], Batch [540/747], Loss: 0.4991\n",
      "2025-04-03 22:18:07,685 - INFO - Epoch [7/10], Batch [550/747], Loss: 0.3791\n",
      "2025-04-03 22:19:05,875 - INFO - Epoch [7/10], Batch [560/747], Loss: 0.4177\n",
      "2025-04-03 22:20:03,830 - INFO - Epoch [7/10], Batch [570/747], Loss: 0.2631\n",
      "2025-04-03 22:21:01,482 - INFO - Epoch [7/10], Batch [580/747], Loss: 1.1253\n",
      "2025-04-03 22:21:59,289 - INFO - Epoch [7/10], Batch [590/747], Loss: 0.3662\n",
      "2025-04-03 22:22:56,753 - INFO - Epoch [7/10], Batch [600/747], Loss: 0.4020\n",
      "2025-04-03 22:23:53,995 - INFO - Epoch [7/10], Batch [610/747], Loss: 0.3246\n",
      "2025-04-03 22:24:51,444 - INFO - Epoch [7/10], Batch [620/747], Loss: 0.2139\n",
      "2025-04-03 22:25:49,489 - INFO - Epoch [7/10], Batch [630/747], Loss: 0.3662\n",
      "2025-04-03 22:26:47,391 - INFO - Epoch [7/10], Batch [640/747], Loss: 0.6691\n",
      "2025-04-03 22:27:44,950 - INFO - Epoch [7/10], Batch [650/747], Loss: 0.3735\n",
      "2025-04-03 22:28:42,396 - INFO - Epoch [7/10], Batch [660/747], Loss: 0.4766\n",
      "2025-04-03 22:29:40,262 - INFO - Epoch [7/10], Batch [670/747], Loss: 0.2895\n",
      "2025-04-03 22:30:38,183 - INFO - Epoch [7/10], Batch [680/747], Loss: 0.3292\n",
      "2025-04-03 22:31:35,914 - INFO - Epoch [7/10], Batch [690/747], Loss: 0.3273\n",
      "2025-04-03 22:32:33,604 - INFO - Epoch [7/10], Batch [700/747], Loss: 0.3958\n",
      "2025-04-03 22:33:31,429 - INFO - Epoch [7/10], Batch [710/747], Loss: 0.4292\n",
      "2025-04-03 22:34:29,592 - INFO - Epoch [7/10], Batch [720/747], Loss: 0.4032\n",
      "2025-04-03 22:35:27,523 - INFO - Epoch [7/10], Batch [730/747], Loss: 0.2271\n",
      "2025-04-03 22:36:25,832 - INFO - Epoch [7/10], Batch [740/747], Loss: 0.6079\n",
      "2025-04-03 22:36:59,675 - INFO - Epoch 7/10 Train Loss: 0.3945, Train Accuracy: 81.94%\n",
      "2025-04-03 22:42:05,946 - INFO - Epoch 7/10 Val Loss: 0.3309, Val Accuracy: 86.06%, AUC-ROC: 0.9232\n",
      "2025-04-03 22:42:05,949 - INFO - New best model at epoch 7 with Val Accuracy: 86.06%\n",
      "2025-04-03 22:42:13,683 - INFO - Epoch [8/10], Batch [0/747], Loss: 0.4360\n",
      "2025-04-03 22:43:11,522 - INFO - Epoch [8/10], Batch [10/747], Loss: 0.3611\n",
      "2025-04-03 22:44:09,371 - INFO - Epoch [8/10], Batch [20/747], Loss: 0.3872\n",
      "2025-04-03 22:45:06,997 - INFO - Epoch [8/10], Batch [30/747], Loss: 0.3404\n",
      "2025-04-03 22:46:04,440 - INFO - Epoch [8/10], Batch [40/747], Loss: 0.3786\n",
      "2025-04-03 22:47:01,940 - INFO - Epoch [8/10], Batch [50/747], Loss: 0.3764\n",
      "2025-04-03 22:47:59,486 - INFO - Epoch [8/10], Batch [60/747], Loss: 0.2661\n",
      "2025-04-03 22:48:57,466 - INFO - Epoch [8/10], Batch [70/747], Loss: 0.3736\n",
      "2025-04-03 22:49:55,233 - INFO - Epoch [8/10], Batch [80/747], Loss: 0.5804\n",
      "2025-04-03 22:50:53,448 - INFO - Epoch [8/10], Batch [90/747], Loss: 0.4151\n",
      "2025-04-03 22:51:51,054 - INFO - Epoch [8/10], Batch [100/747], Loss: 0.2960\n",
      "2025-04-03 22:52:49,042 - INFO - Epoch [8/10], Batch [110/747], Loss: 0.3291\n",
      "2025-04-03 22:53:47,050 - INFO - Epoch [8/10], Batch [120/747], Loss: 0.6328\n",
      "2025-04-03 22:54:44,944 - INFO - Epoch [8/10], Batch [130/747], Loss: 0.3812\n",
      "2025-04-03 22:55:42,913 - INFO - Epoch [8/10], Batch [140/747], Loss: 0.2517\n",
      "2025-04-03 22:56:40,833 - INFO - Epoch [8/10], Batch [150/747], Loss: 0.2861\n",
      "2025-04-03 22:57:38,713 - INFO - Epoch [8/10], Batch [160/747], Loss: 0.3873\n",
      "2025-04-03 22:58:36,597 - INFO - Epoch [8/10], Batch [170/747], Loss: 0.4784\n",
      "2025-04-03 22:59:34,154 - INFO - Epoch [8/10], Batch [180/747], Loss: 0.3408\n",
      "2025-04-03 23:00:32,250 - INFO - Epoch [8/10], Batch [190/747], Loss: 0.3540\n",
      "2025-04-03 23:01:30,424 - INFO - Epoch [8/10], Batch [200/747], Loss: 0.3135\n",
      "2025-04-03 23:02:27,919 - INFO - Epoch [8/10], Batch [210/747], Loss: 0.2318\n",
      "2025-04-03 23:03:25,489 - INFO - Epoch [8/10], Batch [220/747], Loss: 0.4256\n",
      "2025-04-03 23:04:23,189 - INFO - Epoch [8/10], Batch [230/747], Loss: 0.4372\n",
      "2025-04-03 23:05:20,666 - INFO - Epoch [8/10], Batch [240/747], Loss: 0.3516\n",
      "2025-04-03 23:06:18,586 - INFO - Epoch [8/10], Batch [250/747], Loss: 0.5807\n",
      "2025-04-03 23:07:16,969 - INFO - Epoch [8/10], Batch [260/747], Loss: 0.5308\n",
      "2025-04-03 23:08:14,932 - INFO - Epoch [8/10], Batch [270/747], Loss: 0.4012\n",
      "2025-04-03 23:09:12,946 - INFO - Epoch [8/10], Batch [280/747], Loss: 0.4346\n",
      "2025-04-03 23:10:10,825 - INFO - Epoch [8/10], Batch [290/747], Loss: 0.3344\n",
      "2025-04-03 23:11:08,886 - INFO - Epoch [8/10], Batch [300/747], Loss: 0.3831\n",
      "2025-04-03 23:12:06,929 - INFO - Epoch [8/10], Batch [310/747], Loss: 0.5139\n",
      "2025-04-03 23:13:04,884 - INFO - Epoch [8/10], Batch [320/747], Loss: 0.4493\n",
      "2025-04-03 23:14:02,821 - INFO - Epoch [8/10], Batch [330/747], Loss: 0.3535\n",
      "2025-04-03 23:15:00,616 - INFO - Epoch [8/10], Batch [340/747], Loss: 0.3322\n",
      "2025-04-03 23:15:58,420 - INFO - Epoch [8/10], Batch [350/747], Loss: 0.4488\n",
      "2025-04-03 23:16:56,632 - INFO - Epoch [8/10], Batch [360/747], Loss: 0.3662\n",
      "2025-04-03 23:17:54,557 - INFO - Epoch [8/10], Batch [370/747], Loss: 0.2952\n",
      "2025-04-03 23:18:52,889 - INFO - Epoch [8/10], Batch [380/747], Loss: 0.3386\n",
      "2025-04-03 23:19:51,049 - INFO - Epoch [8/10], Batch [390/747], Loss: 0.2934\n",
      "2025-04-03 23:20:48,882 - INFO - Epoch [8/10], Batch [400/747], Loss: 0.5202\n",
      "2025-04-03 23:21:46,596 - INFO - Epoch [8/10], Batch [410/747], Loss: 0.1978\n",
      "2025-04-03 23:22:44,327 - INFO - Epoch [8/10], Batch [420/747], Loss: 0.5603\n",
      "2025-04-03 23:23:42,646 - INFO - Epoch [8/10], Batch [430/747], Loss: 0.6850\n",
      "2025-04-03 23:24:40,657 - INFO - Epoch [8/10], Batch [440/747], Loss: 0.3693\n",
      "2025-04-03 23:25:38,619 - INFO - Epoch [8/10], Batch [450/747], Loss: 0.4274\n",
      "2025-04-03 23:26:36,382 - INFO - Epoch [8/10], Batch [460/747], Loss: 0.1649\n",
      "2025-04-03 23:27:34,076 - INFO - Epoch [8/10], Batch [470/747], Loss: 0.4508\n",
      "2025-04-03 23:28:31,815 - INFO - Epoch [8/10], Batch [480/747], Loss: 0.3870\n",
      "2025-04-03 23:29:29,468 - INFO - Epoch [8/10], Batch [490/747], Loss: 0.3999\n",
      "2025-04-03 23:30:27,655 - INFO - Epoch [8/10], Batch [500/747], Loss: 0.2957\n",
      "2025-04-03 23:31:25,689 - INFO - Epoch [8/10], Batch [510/747], Loss: 0.4174\n",
      "2025-04-03 23:32:23,626 - INFO - Epoch [8/10], Batch [520/747], Loss: 0.2190\n",
      "2025-04-03 23:33:21,415 - INFO - Epoch [8/10], Batch [530/747], Loss: 0.2843\n",
      "2025-04-03 23:34:18,985 - INFO - Epoch [8/10], Batch [540/747], Loss: 0.1503\n",
      "2025-04-03 23:35:16,724 - INFO - Epoch [8/10], Batch [550/747], Loss: 0.3605\n",
      "2025-04-03 23:36:14,447 - INFO - Epoch [8/10], Batch [560/747], Loss: 0.4355\n",
      "2025-04-03 23:37:12,316 - INFO - Epoch [8/10], Batch [570/747], Loss: 0.2880\n",
      "2025-04-03 23:38:10,193 - INFO - Epoch [8/10], Batch [580/747], Loss: 0.3076\n",
      "2025-04-03 23:39:08,341 - INFO - Epoch [8/10], Batch [590/747], Loss: 0.4032\n",
      "2025-04-03 23:40:06,332 - INFO - Epoch [8/10], Batch [600/747], Loss: 0.2796\n",
      "2025-04-03 23:41:03,944 - INFO - Epoch [8/10], Batch [610/747], Loss: 0.5891\n",
      "2025-04-03 23:42:01,777 - INFO - Epoch [8/10], Batch [620/747], Loss: 0.5706\n",
      "2025-04-03 23:42:59,792 - INFO - Epoch [8/10], Batch [630/747], Loss: 0.3359\n",
      "2025-04-03 23:43:57,407 - INFO - Epoch [8/10], Batch [640/747], Loss: 0.3436\n",
      "2025-04-03 23:44:54,971 - INFO - Epoch [8/10], Batch [650/747], Loss: 0.2372\n",
      "2025-04-03 23:45:52,185 - INFO - Epoch [8/10], Batch [660/747], Loss: 0.2702\n",
      "2025-04-03 23:46:50,123 - INFO - Epoch [8/10], Batch [670/747], Loss: 0.2271\n",
      "2025-04-03 23:47:48,113 - INFO - Epoch [8/10], Batch [680/747], Loss: 0.2652\n",
      "2025-04-03 23:48:46,165 - INFO - Epoch [8/10], Batch [690/747], Loss: 0.2514\n",
      "2025-04-03 23:49:43,924 - INFO - Epoch [8/10], Batch [700/747], Loss: 0.4243\n",
      "2025-04-03 23:50:41,851 - INFO - Epoch [8/10], Batch [710/747], Loss: 0.2987\n",
      "2025-04-03 23:51:39,801 - INFO - Epoch [8/10], Batch [720/747], Loss: 0.2311\n",
      "2025-04-03 23:52:37,726 - INFO - Epoch [8/10], Batch [730/747], Loss: 0.5381\n",
      "2025-04-03 23:53:35,447 - INFO - Epoch [8/10], Batch [740/747], Loss: 0.3558\n",
      "2025-04-03 23:54:08,859 - INFO - Epoch 8/10 Train Loss: 0.3743, Train Accuracy: 82.78%\n",
      "2025-04-03 23:59:15,693 - INFO - Epoch 8/10 Val Loss: 0.3025, Val Accuracy: 87.26%, AUC-ROC: 0.9374\n",
      "2025-04-03 23:59:15,698 - INFO - New best model at epoch 8 with Val Accuracy: 87.26%\n",
      "2025-04-03 23:59:22,652 - INFO - Epoch [9/10], Batch [0/747], Loss: 0.3547\n",
      "2025-04-04 00:00:20,359 - INFO - Epoch [9/10], Batch [10/747], Loss: 0.2713\n",
      "2025-04-04 00:01:17,565 - INFO - Epoch [9/10], Batch [20/747], Loss: 0.3791\n",
      "2025-04-04 00:02:15,374 - INFO - Epoch [9/10], Batch [30/747], Loss: 0.3437\n",
      "2025-04-04 00:03:12,931 - INFO - Epoch [9/10], Batch [40/747], Loss: 0.3302\n",
      "2025-04-04 00:04:10,609 - INFO - Epoch [9/10], Batch [50/747], Loss: 0.4233\n",
      "2025-04-04 00:05:07,902 - INFO - Epoch [9/10], Batch [60/747], Loss: 0.4108\n",
      "2025-04-04 00:06:05,110 - INFO - Epoch [9/10], Batch [70/747], Loss: 0.3352\n",
      "2025-04-04 00:07:02,476 - INFO - Epoch [9/10], Batch [80/747], Loss: 0.4211\n",
      "2025-04-04 00:08:00,186 - INFO - Epoch [9/10], Batch [90/747], Loss: 0.3648\n",
      "2025-04-04 00:08:57,921 - INFO - Epoch [9/10], Batch [100/747], Loss: 0.2962\n",
      "2025-04-04 00:09:56,002 - INFO - Epoch [9/10], Batch [110/747], Loss: 0.2152\n",
      "2025-04-04 00:10:53,047 - INFO - Epoch [9/10], Batch [120/747], Loss: 0.4031\n",
      "2025-04-04 00:11:50,190 - INFO - Epoch [9/10], Batch [130/747], Loss: 0.2283\n",
      "2025-04-04 00:12:47,889 - INFO - Epoch [9/10], Batch [140/747], Loss: 0.2697\n",
      "2025-04-04 00:13:45,335 - INFO - Epoch [9/10], Batch [150/747], Loss: 0.3554\n",
      "2025-04-04 00:14:42,917 - INFO - Epoch [9/10], Batch [160/747], Loss: 0.4611\n",
      "2025-04-04 00:15:40,708 - INFO - Epoch [9/10], Batch [170/747], Loss: 0.4917\n",
      "2025-04-04 00:16:38,508 - INFO - Epoch [9/10], Batch [180/747], Loss: 0.2708\n",
      "2025-04-04 00:17:36,552 - INFO - Epoch [9/10], Batch [190/747], Loss: 0.3963\n",
      "2025-04-04 00:18:34,311 - INFO - Epoch [9/10], Batch [200/747], Loss: 0.3324\n",
      "2025-04-04 00:19:32,251 - INFO - Epoch [9/10], Batch [210/747], Loss: 0.4458\n",
      "2025-04-04 00:20:30,147 - INFO - Epoch [9/10], Batch [220/747], Loss: 0.3056\n",
      "2025-04-04 00:21:28,320 - INFO - Epoch [9/10], Batch [230/747], Loss: 0.4102\n",
      "2025-04-04 00:22:26,120 - INFO - Epoch [9/10], Batch [240/747], Loss: 0.4517\n",
      "2025-04-04 00:23:24,017 - INFO - Epoch [9/10], Batch [250/747], Loss: 0.3900\n",
      "2025-04-04 00:24:22,133 - INFO - Epoch [9/10], Batch [260/747], Loss: 0.2197\n",
      "2025-04-04 00:25:19,604 - INFO - Epoch [9/10], Batch [270/747], Loss: 0.3087\n",
      "2025-04-04 00:26:18,137 - INFO - Epoch [9/10], Batch [280/747], Loss: 0.3190\n",
      "2025-04-04 00:27:16,587 - INFO - Epoch [9/10], Batch [290/747], Loss: 0.4389\n",
      "2025-04-04 00:28:15,216 - INFO - Epoch [9/10], Batch [300/747], Loss: 0.2514\n",
      "2025-04-04 00:29:13,080 - INFO - Epoch [9/10], Batch [310/747], Loss: 0.2734\n",
      "2025-04-04 00:30:10,628 - INFO - Epoch [9/10], Batch [320/747], Loss: 0.5016\n",
      "2025-04-04 00:31:08,499 - INFO - Epoch [9/10], Batch [330/747], Loss: 0.3487\n",
      "2025-04-04 00:32:05,919 - INFO - Epoch [9/10], Batch [340/747], Loss: 0.4767\n",
      "2025-04-04 00:33:03,720 - INFO - Epoch [9/10], Batch [350/747], Loss: 0.3642\n",
      "2025-04-04 00:34:01,570 - INFO - Epoch [9/10], Batch [360/747], Loss: 0.1333\n",
      "2025-04-04 00:34:59,613 - INFO - Epoch [9/10], Batch [370/747], Loss: 0.2621\n",
      "2025-04-04 00:35:57,242 - INFO - Epoch [9/10], Batch [380/747], Loss: 0.3073\n",
      "2025-04-04 00:36:54,632 - INFO - Epoch [9/10], Batch [390/747], Loss: 0.2771\n",
      "2025-04-04 00:37:52,229 - INFO - Epoch [9/10], Batch [400/747], Loss: 0.3453\n",
      "2025-04-04 00:38:49,955 - INFO - Epoch [9/10], Batch [410/747], Loss: 0.3077\n",
      "2025-04-04 00:39:47,294 - INFO - Epoch [9/10], Batch [420/747], Loss: 0.4036\n",
      "2025-04-04 00:40:44,897 - INFO - Epoch [9/10], Batch [430/747], Loss: 0.2475\n",
      "2025-04-04 00:41:42,446 - INFO - Epoch [9/10], Batch [440/747], Loss: 0.3021\n",
      "2025-04-04 00:42:39,983 - INFO - Epoch [9/10], Batch [450/747], Loss: 0.5350\n",
      "2025-04-04 00:43:37,758 - INFO - Epoch [9/10], Batch [460/747], Loss: 0.2026\n",
      "2025-04-04 00:44:35,418 - INFO - Epoch [9/10], Batch [470/747], Loss: 0.2631\n",
      "2025-04-04 00:45:33,006 - INFO - Epoch [9/10], Batch [480/747], Loss: 0.2300\n",
      "2025-04-04 00:46:30,432 - INFO - Epoch [9/10], Batch [490/747], Loss: 0.4253\n",
      "2025-04-04 00:47:28,203 - INFO - Epoch [9/10], Batch [500/747], Loss: 0.4609\n",
      "2025-04-04 00:48:25,620 - INFO - Epoch [9/10], Batch [510/747], Loss: 0.3255\n",
      "2025-04-04 00:49:23,444 - INFO - Epoch [9/10], Batch [520/747], Loss: 0.4576\n",
      "2025-04-04 00:50:21,059 - INFO - Epoch [9/10], Batch [530/747], Loss: 0.2158\n",
      "2025-04-04 00:51:18,667 - INFO - Epoch [9/10], Batch [540/747], Loss: 0.2577\n",
      "2025-04-04 00:52:16,217 - INFO - Epoch [9/10], Batch [550/747], Loss: 0.4597\n",
      "2025-04-04 00:53:13,499 - INFO - Epoch [9/10], Batch [560/747], Loss: 0.2479\n",
      "2025-04-04 00:54:10,964 - INFO - Epoch [9/10], Batch [570/747], Loss: 0.2992\n",
      "2025-04-04 00:55:08,367 - INFO - Epoch [9/10], Batch [580/747], Loss: 0.5805\n",
      "2025-04-04 00:56:06,808 - INFO - Epoch [9/10], Batch [590/747], Loss: 0.1875\n",
      "2025-04-04 00:57:04,653 - INFO - Epoch [9/10], Batch [600/747], Loss: 0.3491\n",
      "2025-04-04 00:58:02,168 - INFO - Epoch [9/10], Batch [610/747], Loss: 0.2527\n",
      "2025-04-04 00:58:59,750 - INFO - Epoch [9/10], Batch [620/747], Loss: 0.3506\n",
      "2025-04-04 00:59:57,644 - INFO - Epoch [9/10], Batch [630/747], Loss: 0.3010\n",
      "2025-04-04 01:00:55,048 - INFO - Epoch [9/10], Batch [640/747], Loss: 0.3373\n",
      "2025-04-04 01:01:52,361 - INFO - Epoch [9/10], Batch [650/747], Loss: 0.4046\n",
      "2025-04-04 01:02:50,225 - INFO - Epoch [9/10], Batch [660/747], Loss: 0.3306\n",
      "2025-04-04 01:03:48,031 - INFO - Epoch [9/10], Batch [670/747], Loss: 0.5911\n",
      "2025-04-04 01:04:45,458 - INFO - Epoch [9/10], Batch [680/747], Loss: 0.3879\n",
      "2025-04-04 01:05:43,103 - INFO - Epoch [9/10], Batch [690/747], Loss: 0.2114\n",
      "2025-04-04 01:06:40,979 - INFO - Epoch [9/10], Batch [700/747], Loss: 0.3300\n",
      "2025-04-04 01:07:38,732 - INFO - Epoch [9/10], Batch [710/747], Loss: 0.3809\n",
      "2025-04-04 01:08:36,281 - INFO - Epoch [9/10], Batch [720/747], Loss: 0.2739\n",
      "2025-04-04 01:09:33,467 - INFO - Epoch [9/10], Batch [730/747], Loss: 0.2539\n",
      "2025-04-04 01:10:30,767 - INFO - Epoch [9/10], Batch [740/747], Loss: 0.3756\n",
      "2025-04-04 01:11:04,502 - INFO - Epoch 9/10 Train Loss: 0.3580, Train Accuracy: 83.56%\n",
      "2025-04-04 01:16:13,653 - INFO - Epoch 9/10 Val Loss: 0.2883, Val Accuracy: 87.68%, AUC-ROC: 0.9417\n",
      "2025-04-04 01:16:13,657 - INFO - New best model at epoch 9 with Val Accuracy: 87.68%\n",
      "2025-04-04 01:16:20,463 - INFO - Epoch [10/10], Batch [0/747], Loss: 0.4114\n",
      "2025-04-04 01:17:18,476 - INFO - Epoch [10/10], Batch [10/747], Loss: 0.4661\n",
      "2025-04-04 01:18:16,851 - INFO - Epoch [10/10], Batch [20/747], Loss: 0.4851\n",
      "2025-04-04 01:19:15,285 - INFO - Epoch [10/10], Batch [30/747], Loss: 0.3610\n",
      "2025-04-04 01:20:12,880 - INFO - Epoch [10/10], Batch [40/747], Loss: 0.4642\n",
      "2025-04-04 01:21:09,799 - INFO - Epoch [10/10], Batch [50/747], Loss: 0.3154\n",
      "2025-04-04 01:22:07,616 - INFO - Epoch [10/10], Batch [60/747], Loss: 0.3630\n",
      "2025-04-04 01:23:04,929 - INFO - Epoch [10/10], Batch [70/747], Loss: 0.2984\n",
      "2025-04-04 01:24:03,093 - INFO - Epoch [10/10], Batch [80/747], Loss: 0.5070\n",
      "2025-04-04 01:25:00,877 - INFO - Epoch [10/10], Batch [90/747], Loss: 0.2557\n",
      "2025-04-04 01:25:58,777 - INFO - Epoch [10/10], Batch [100/747], Loss: 0.3320\n",
      "2025-04-04 01:26:56,668 - INFO - Epoch [10/10], Batch [110/747], Loss: 0.3276\n",
      "2025-04-04 01:27:54,476 - INFO - Epoch [10/10], Batch [120/747], Loss: 0.2876\n",
      "2025-04-04 01:28:52,210 - INFO - Epoch [10/10], Batch [130/747], Loss: 0.3187\n",
      "2025-04-04 01:29:49,984 - INFO - Epoch [10/10], Batch [140/747], Loss: 0.3906\n",
      "2025-04-04 01:30:47,499 - INFO - Epoch [10/10], Batch [150/747], Loss: 0.2464\n",
      "2025-04-04 01:31:45,320 - INFO - Epoch [10/10], Batch [160/747], Loss: 0.4804\n",
      "2025-04-04 01:32:42,996 - INFO - Epoch [10/10], Batch [170/747], Loss: 0.3372\n",
      "2025-04-04 01:33:40,832 - INFO - Epoch [10/10], Batch [180/747], Loss: 0.2773\n",
      "2025-04-04 01:34:38,552 - INFO - Epoch [10/10], Batch [190/747], Loss: 0.3295\n",
      "2025-04-04 01:35:36,074 - INFO - Epoch [10/10], Batch [200/747], Loss: 0.2787\n",
      "2025-04-04 01:36:33,466 - INFO - Epoch [10/10], Batch [210/747], Loss: 0.1805\n",
      "2025-04-04 01:37:30,338 - INFO - Epoch [10/10], Batch [220/747], Loss: 0.4504\n",
      "2025-04-04 01:38:27,783 - INFO - Epoch [10/10], Batch [230/747], Loss: 0.3954\n",
      "2025-04-04 01:39:25,331 - INFO - Epoch [10/10], Batch [240/747], Loss: 0.3554\n",
      "2025-04-04 01:40:22,193 - INFO - Epoch [10/10], Batch [250/747], Loss: 0.3199\n",
      "2025-04-04 01:41:18,984 - INFO - Epoch [10/10], Batch [260/747], Loss: 0.4500\n",
      "2025-04-04 01:42:15,856 - INFO - Epoch [10/10], Batch [270/747], Loss: 0.2604\n",
      "2025-04-04 01:43:12,842 - INFO - Epoch [10/10], Batch [280/747], Loss: 0.2924\n",
      "2025-04-04 01:44:10,195 - INFO - Epoch [10/10], Batch [290/747], Loss: 0.4482\n",
      "2025-04-04 01:45:07,911 - INFO - Epoch [10/10], Batch [300/747], Loss: 0.4198\n",
      "2025-04-04 01:46:05,313 - INFO - Epoch [10/10], Batch [310/747], Loss: 0.2867\n",
      "2025-04-04 01:47:02,622 - INFO - Epoch [10/10], Batch [320/747], Loss: 0.3827\n",
      "2025-04-04 01:48:00,297 - INFO - Epoch [10/10], Batch [330/747], Loss: 0.3606\n",
      "2025-04-04 01:48:56,950 - INFO - Epoch [10/10], Batch [340/747], Loss: 0.3122\n",
      "2025-04-04 01:49:54,125 - INFO - Epoch [10/10], Batch [350/747], Loss: 0.4577\n",
      "2025-04-04 01:50:52,430 - INFO - Epoch [10/10], Batch [360/747], Loss: 0.3942\n",
      "2025-04-04 01:51:50,533 - INFO - Epoch [10/10], Batch [370/747], Loss: 0.2511\n",
      "2025-04-04 01:52:48,612 - INFO - Epoch [10/10], Batch [380/747], Loss: 0.2397\n",
      "2025-04-04 01:53:46,771 - INFO - Epoch [10/10], Batch [390/747], Loss: 0.3516\n",
      "2025-04-04 01:54:44,510 - INFO - Epoch [10/10], Batch [400/747], Loss: 0.2572\n",
      "2025-04-04 01:55:42,086 - INFO - Epoch [10/10], Batch [410/747], Loss: 0.3475\n",
      "2025-04-04 01:56:39,404 - INFO - Epoch [10/10], Batch [420/747], Loss: 0.2368\n",
      "2025-04-04 01:57:37,382 - INFO - Epoch [10/10], Batch [430/747], Loss: 0.4894\n",
      "2025-04-04 01:58:35,010 - INFO - Epoch [10/10], Batch [440/747], Loss: 0.3344\n",
      "2025-04-04 01:59:32,964 - INFO - Epoch [10/10], Batch [450/747], Loss: 0.3570\n",
      "2025-04-04 02:00:30,742 - INFO - Epoch [10/10], Batch [460/747], Loss: 0.3848\n",
      "2025-04-04 02:01:28,603 - INFO - Epoch [10/10], Batch [470/747], Loss: 0.2259\n",
      "2025-04-04 02:02:26,516 - INFO - Epoch [10/10], Batch [480/747], Loss: 0.2571\n",
      "2025-04-04 02:03:24,120 - INFO - Epoch [10/10], Batch [490/747], Loss: 0.3536\n",
      "2025-04-04 02:04:21,881 - INFO - Epoch [10/10], Batch [500/747], Loss: 0.3168\n",
      "2025-04-04 02:05:19,594 - INFO - Epoch [10/10], Batch [510/747], Loss: 0.2330\n",
      "2025-04-04 02:06:17,241 - INFO - Epoch [10/10], Batch [520/747], Loss: 0.3439\n",
      "2025-04-04 02:07:15,059 - INFO - Epoch [10/10], Batch [530/747], Loss: 0.2650\n",
      "2025-04-04 02:08:13,168 - INFO - Epoch [10/10], Batch [540/747], Loss: 0.2445\n",
      "2025-04-04 02:09:10,777 - INFO - Epoch [10/10], Batch [550/747], Loss: 0.2320\n",
      "2025-04-04 02:10:08,699 - INFO - Epoch [10/10], Batch [560/747], Loss: 0.3396\n",
      "2025-04-04 02:11:06,587 - INFO - Epoch [10/10], Batch [570/747], Loss: 0.4135\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    hostname = socket.gethostname()\n",
    "    logger.info(f\"Running on node: {hostname}\")\n",
    "    data_dir = \"../../preprocessed_glaucoma_data\"\n",
    "    logger.info(\"Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single CPU...\")\n",
    "    results = train_and_evaluate(data_dir)\n",
    "    logger.info(\"Training completed.\")\n",
    "    logger.info(f\"Test Accuracy: {results['test_accuracy']:.2f}%\")\n",
    "    logger.info(f\"Total Layers: {results['total_layers']}\")\n",
    "    logger.info(f\"Total Parameters: {results['total_parameters']:,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb03360-a517-4812-9469-ac28ef878493",
   "metadata": {},
   "source": [
    "The train accuracy during the 10 epochs steadily improved, reaching 84.1% by the end of training. Initially, the model had a 61.29% accuracy, which indicates that the model was initially underfitting or struggling to learn meaningful patterns from the data. As training progressed, the accuracy increased with each epoch, showcasing the model's ability to learn from the data over time.\n",
    "\n",
    "The test accuracy achieved was 86.99%, which is a strong result, indicating that the model generalizes well on the unseen data. This reflects that the model not only learned the training data well but also managed to adapt to the test data without overfitting. The AUC-ROC score of 0.94 further supports the high quality of predictions, showing that the model performs very well in distinguishing between the positive and negative glaucoma classes.\n",
    "\n",
    "Overall, the steady increase in accuracy during training and the strong test performance suggest that the model architecture, combined with effective training, was able to achieve high performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
