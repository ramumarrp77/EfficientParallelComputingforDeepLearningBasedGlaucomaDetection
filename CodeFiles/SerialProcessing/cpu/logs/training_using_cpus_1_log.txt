2025-04-03 13:42:16,941 - INFO - Running on node: d0010
2025-04-03 13:42:16,943 - INFO - Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single CPU...
2025-04-03 13:42:17,037 - INFO - Loading data...
2025-04-03 13:42:29,316 - INFO - Data shapes: Train (23898, 224, 224, 3), Val (5747, 224, 224, 3), Test (2874, 224, 224, 3)
2025-04-03 13:42:29,321 - INFO - Using device: cpu
2025-04-03 13:42:29,581 - INFO - Model architecture: MedicalCNN
2025-04-03 13:42:29,582 - INFO - Total layers: 129
2025-04-03 13:42:29,582 - INFO - Total parameters: 22,494,274
2025-04-03 13:42:35,952 - INFO - Epoch [1/10], Batch [0/747], Loss: 0.7127
2025-04-03 13:43:32,061 - INFO - Epoch [1/10], Batch [10/747], Loss: 0.6740
2025-04-03 13:44:28,403 - INFO - Epoch [1/10], Batch [20/747], Loss: 0.6940
2025-04-03 13:45:24,627 - INFO - Epoch [1/10], Batch [30/747], Loss: 0.5867
2025-04-03 13:46:21,154 - INFO - Epoch [1/10], Batch [40/747], Loss: 0.7433
2025-04-03 13:47:18,216 - INFO - Epoch [1/10], Batch [50/747], Loss: 0.6379
2025-04-03 13:48:15,229 - INFO - Epoch [1/10], Batch [60/747], Loss: 0.6258
2025-04-03 13:49:12,292 - INFO - Epoch [1/10], Batch [70/747], Loss: 0.7025
2025-04-03 13:50:09,267 - INFO - Epoch [1/10], Batch [80/747], Loss: 0.6468
2025-04-03 13:51:06,107 - INFO - Epoch [1/10], Batch [90/747], Loss: 0.6475
2025-04-03 13:52:02,828 - INFO - Epoch [1/10], Batch [100/747], Loss: 0.7703
2025-04-03 13:52:59,922 - INFO - Epoch [1/10], Batch [110/747], Loss: 0.6344
2025-04-03 13:53:56,931 - INFO - Epoch [1/10], Batch [120/747], Loss: 0.7708
2025-04-03 13:54:54,094 - INFO - Epoch [1/10], Batch [130/747], Loss: 0.6058
2025-04-03 13:55:51,284 - INFO - Epoch [1/10], Batch [140/747], Loss: 0.6487
2025-04-03 13:56:47,837 - INFO - Epoch [1/10], Batch [150/747], Loss: 0.6777
2025-04-03 13:57:44,486 - INFO - Epoch [1/10], Batch [160/747], Loss: 0.6194
2025-04-03 13:58:41,067 - INFO - Epoch [1/10], Batch [170/747], Loss: 0.5806
2025-04-03 13:59:37,583 - INFO - Epoch [1/10], Batch [180/747], Loss: 0.5762
2025-04-03 14:00:34,379 - INFO - Epoch [1/10], Batch [190/747], Loss: 0.5735
2025-04-03 14:01:30,959 - INFO - Epoch [1/10], Batch [200/747], Loss: 0.6230
2025-04-03 14:02:27,684 - INFO - Epoch [1/10], Batch [210/747], Loss: 0.6473
2025-04-03 14:03:24,523 - INFO - Epoch [1/10], Batch [220/747], Loss: 0.6535
2025-04-03 14:04:21,582 - INFO - Epoch [1/10], Batch [230/747], Loss: 0.6054
2025-04-03 14:05:18,084 - INFO - Epoch [1/10], Batch [240/747], Loss: 0.7730
2025-04-03 14:06:14,693 - INFO - Epoch [1/10], Batch [250/747], Loss: 0.5785
2025-04-03 14:07:11,686 - INFO - Epoch [1/10], Batch [260/747], Loss: 0.5878
2025-04-03 14:08:08,723 - INFO - Epoch [1/10], Batch [270/747], Loss: 0.6497
2025-04-03 14:09:05,003 - INFO - Epoch [1/10], Batch [280/747], Loss: 0.6878
2025-04-03 14:10:00,828 - INFO - Epoch [1/10], Batch [290/747], Loss: 0.5421
2025-04-03 14:10:57,257 - INFO - Epoch [1/10], Batch [300/747], Loss: 0.5510
2025-04-03 14:11:53,535 - INFO - Epoch [1/10], Batch [310/747], Loss: 0.6137
2025-04-03 14:12:49,847 - INFO - Epoch [1/10], Batch [320/747], Loss: 0.5717
2025-04-03 14:13:46,343 - INFO - Epoch [1/10], Batch [330/747], Loss: 0.5378
2025-04-03 14:14:42,454 - INFO - Epoch [1/10], Batch [340/747], Loss: 0.7164
2025-04-03 14:15:38,551 - INFO - Epoch [1/10], Batch [350/747], Loss: 0.6517
2025-04-03 14:16:34,823 - INFO - Epoch [1/10], Batch [360/747], Loss: 0.6219
2025-04-03 14:17:31,305 - INFO - Epoch [1/10], Batch [370/747], Loss: 0.6614
2025-04-03 14:18:27,545 - INFO - Epoch [1/10], Batch [380/747], Loss: 0.5740
2025-04-03 14:19:23,889 - INFO - Epoch [1/10], Batch [390/747], Loss: 0.5501
2025-04-03 14:20:19,947 - INFO - Epoch [1/10], Batch [400/747], Loss: 0.5578
2025-04-03 14:21:16,392 - INFO - Epoch [1/10], Batch [410/747], Loss: 0.6281
2025-04-03 14:22:12,771 - INFO - Epoch [1/10], Batch [420/747], Loss: 0.6357
2025-04-03 14:23:09,337 - INFO - Epoch [1/10], Batch [430/747], Loss: 0.6446
2025-04-03 14:24:05,729 - INFO - Epoch [1/10], Batch [440/747], Loss: 0.6412
2025-04-03 14:25:02,262 - INFO - Epoch [1/10], Batch [450/747], Loss: 0.5990
2025-04-03 14:25:58,642 - INFO - Epoch [1/10], Batch [460/747], Loss: 0.7370
2025-04-03 14:26:55,347 - INFO - Epoch [1/10], Batch [470/747], Loss: 0.8675
2025-04-03 14:27:51,703 - INFO - Epoch [1/10], Batch [480/747], Loss: 0.7397
2025-04-03 14:28:48,174 - INFO - Epoch [1/10], Batch [490/747], Loss: 0.5744
2025-04-03 14:29:44,363 - INFO - Epoch [1/10], Batch [500/747], Loss: 0.7302
2025-04-03 14:30:40,519 - INFO - Epoch [1/10], Batch [510/747], Loss: 0.6259
2025-04-03 14:31:36,613 - INFO - Epoch [1/10], Batch [520/747], Loss: 0.7040
2025-04-03 14:32:33,018 - INFO - Epoch [1/10], Batch [530/747], Loss: 0.6486
2025-04-03 14:33:29,327 - INFO - Epoch [1/10], Batch [540/747], Loss: 0.6709
2025-04-03 14:34:26,308 - INFO - Epoch [1/10], Batch [550/747], Loss: 0.6387
2025-04-03 14:35:22,703 - INFO - Epoch [1/10], Batch [560/747], Loss: 0.7219
2025-04-03 14:36:19,011 - INFO - Epoch [1/10], Batch [570/747], Loss: 0.5837
2025-04-03 14:37:14,823 - INFO - Epoch [1/10], Batch [580/747], Loss: 0.8197
2025-04-03 14:38:10,859 - INFO - Epoch [1/10], Batch [590/747], Loss: 0.5974
2025-04-03 14:39:06,960 - INFO - Epoch [1/10], Batch [600/747], Loss: 0.5817
2025-04-03 14:40:03,106 - INFO - Epoch [1/10], Batch [610/747], Loss: 0.7085
2025-04-03 14:40:59,601 - INFO - Epoch [1/10], Batch [620/747], Loss: 0.6372
2025-04-03 14:41:56,301 - INFO - Epoch [1/10], Batch [630/747], Loss: 0.6257
2025-04-03 14:42:52,965 - INFO - Epoch [1/10], Batch [640/747], Loss: 0.5034
2025-04-03 14:43:49,901 - INFO - Epoch [1/10], Batch [650/747], Loss: 0.6109
2025-04-03 14:44:46,574 - INFO - Epoch [1/10], Batch [660/747], Loss: 0.5898
2025-04-03 14:45:43,334 - INFO - Epoch [1/10], Batch [670/747], Loss: 0.6115
2025-04-03 14:46:39,872 - INFO - Epoch [1/10], Batch [680/747], Loss: 0.5704
2025-04-03 14:47:36,324 - INFO - Epoch [1/10], Batch [690/747], Loss: 0.5861
2025-04-03 14:48:32,789 - INFO - Epoch [1/10], Batch [700/747], Loss: 0.6291
2025-04-03 14:49:29,187 - INFO - Epoch [1/10], Batch [710/747], Loss: 0.5877
2025-04-03 14:50:25,823 - INFO - Epoch [1/10], Batch [720/747], Loss: 0.6022
2025-04-03 14:51:22,043 - INFO - Epoch [1/10], Batch [730/747], Loss: 0.6931
2025-04-03 14:52:17,922 - INFO - Epoch [1/10], Batch [740/747], Loss: 0.6116
2025-04-03 14:52:50,581 - INFO - Epoch 1/10 Train Loss: 0.6462, Train Accuracy: 61.29%
2025-04-03 14:57:56,802 - INFO - Epoch 1/10 Val Loss: 0.5680, Val Accuracy: 72.28%, AUC-ROC: 0.7433
2025-04-03 14:57:56,808 - INFO - New best model at epoch 1 with Val Accuracy: 72.28%
2025-04-03 14:58:03,856 - INFO - Epoch [2/10], Batch [0/747], Loss: 0.6208
2025-04-03 14:59:01,980 - INFO - Epoch [2/10], Batch [10/747], Loss: 0.6026
2025-04-03 15:00:00,203 - INFO - Epoch [2/10], Batch [20/747], Loss: 0.6719
2025-04-03 15:00:58,483 - INFO - Epoch [2/10], Batch [30/747], Loss: 0.6023
2025-04-03 15:01:56,606 - INFO - Epoch [2/10], Batch [40/747], Loss: 0.5371
2025-04-03 15:02:54,651 - INFO - Epoch [2/10], Batch [50/747], Loss: 0.6448
2025-04-03 15:03:52,586 - INFO - Epoch [2/10], Batch [60/747], Loss: 0.4899
2025-04-03 15:04:51,038 - INFO - Epoch [2/10], Batch [70/747], Loss: 0.6378
2025-04-03 15:05:49,399 - INFO - Epoch [2/10], Batch [80/747], Loss: 0.6065
2025-04-03 15:06:47,320 - INFO - Epoch [2/10], Batch [90/747], Loss: 0.6647
2025-04-03 15:07:45,798 - INFO - Epoch [2/10], Batch [100/747], Loss: 0.6515
2025-04-03 15:08:44,143 - INFO - Epoch [2/10], Batch [110/747], Loss: 0.6314
2025-04-03 15:09:42,661 - INFO - Epoch [2/10], Batch [120/747], Loss: 0.6846
2025-04-03 15:10:41,166 - INFO - Epoch [2/10], Batch [130/747], Loss: 0.7069
2025-04-03 15:11:39,255 - INFO - Epoch [2/10], Batch [140/747], Loss: 0.6684
2025-04-03 15:12:37,099 - INFO - Epoch [2/10], Batch [150/747], Loss: 0.5414
2025-04-03 15:13:34,633 - INFO - Epoch [2/10], Batch [160/747], Loss: 0.4975
2025-04-03 15:14:32,331 - INFO - Epoch [2/10], Batch [170/747], Loss: 0.5423
2025-04-03 15:15:30,497 - INFO - Epoch [2/10], Batch [180/747], Loss: 0.5726
2025-04-03 15:16:28,952 - INFO - Epoch [2/10], Batch [190/747], Loss: 0.5405
2025-04-03 15:17:27,042 - INFO - Epoch [2/10], Batch [200/747], Loss: 0.8835
2025-04-03 15:18:25,547 - INFO - Epoch [2/10], Batch [210/747], Loss: 0.5120
2025-04-03 15:19:23,569 - INFO - Epoch [2/10], Batch [220/747], Loss: 0.5218
2025-04-03 15:20:21,651 - INFO - Epoch [2/10], Batch [230/747], Loss: 0.6500
2025-04-03 15:21:19,629 - INFO - Epoch [2/10], Batch [240/747], Loss: 0.5456
2025-04-03 15:22:18,227 - INFO - Epoch [2/10], Batch [250/747], Loss: 0.4873
2025-04-03 15:23:17,836 - INFO - Epoch [2/10], Batch [260/747], Loss: 0.5423
2025-04-03 15:24:16,638 - INFO - Epoch [2/10], Batch [270/747], Loss: 0.6972
2025-04-03 15:25:14,737 - INFO - Epoch [2/10], Batch [280/747], Loss: 0.6363
2025-04-03 15:26:12,910 - INFO - Epoch [2/10], Batch [290/747], Loss: 0.5593
2025-04-03 15:27:11,473 - INFO - Epoch [2/10], Batch [300/747], Loss: 0.6045
2025-04-03 15:28:09,960 - INFO - Epoch [2/10], Batch [310/747], Loss: 0.6462
2025-04-03 15:29:07,906 - INFO - Epoch [2/10], Batch [320/747], Loss: 0.5269
2025-04-03 15:30:05,718 - INFO - Epoch [2/10], Batch [330/747], Loss: 0.6006
2025-04-03 15:31:03,630 - INFO - Epoch [2/10], Batch [340/747], Loss: 0.5653
2025-04-03 15:32:01,758 - INFO - Epoch [2/10], Batch [350/747], Loss: 0.5355
2025-04-03 15:33:00,320 - INFO - Epoch [2/10], Batch [360/747], Loss: 0.6949
2025-04-03 15:33:59,041 - INFO - Epoch [2/10], Batch [370/747], Loss: 0.6057
2025-04-03 15:34:56,911 - INFO - Epoch [2/10], Batch [380/747], Loss: 0.6755
2025-04-03 15:35:54,884 - INFO - Epoch [2/10], Batch [390/747], Loss: 0.5884
2025-04-03 15:36:52,556 - INFO - Epoch [2/10], Batch [400/747], Loss: 0.5860
2025-04-03 15:37:50,107 - INFO - Epoch [2/10], Batch [410/747], Loss: 0.5297
2025-04-03 15:38:48,574 - INFO - Epoch [2/10], Batch [420/747], Loss: 0.6774
2025-04-03 15:39:46,874 - INFO - Epoch [2/10], Batch [430/747], Loss: 0.4942
2025-04-03 15:40:45,306 - INFO - Epoch [2/10], Batch [440/747], Loss: 0.5267
2025-04-03 15:41:44,310 - INFO - Epoch [2/10], Batch [450/747], Loss: 0.5857
2025-04-03 15:42:42,696 - INFO - Epoch [2/10], Batch [460/747], Loss: 0.5740
2025-04-03 15:43:40,814 - INFO - Epoch [2/10], Batch [470/747], Loss: 0.5763
2025-04-03 15:44:38,772 - INFO - Epoch [2/10], Batch [480/747], Loss: 0.5274
2025-04-03 15:45:36,432 - INFO - Epoch [2/10], Batch [490/747], Loss: 0.5275
2025-04-03 15:46:34,691 - INFO - Epoch [2/10], Batch [500/747], Loss: 0.6310
2025-04-03 15:47:33,223 - INFO - Epoch [2/10], Batch [510/747], Loss: 0.8311
2025-04-03 15:48:31,451 - INFO - Epoch [2/10], Batch [520/747], Loss: 0.5786
2025-04-03 15:49:29,648 - INFO - Epoch [2/10], Batch [530/747], Loss: 0.6978
2025-04-03 15:50:27,968 - INFO - Epoch [2/10], Batch [540/747], Loss: 0.5269
2025-04-03 15:51:26,052 - INFO - Epoch [2/10], Batch [550/747], Loss: 0.4131
2025-04-03 15:52:24,304 - INFO - Epoch [2/10], Batch [560/747], Loss: 0.5750
2025-04-03 15:53:22,679 - INFO - Epoch [2/10], Batch [570/747], Loss: 0.6978
2025-04-03 15:54:20,987 - INFO - Epoch [2/10], Batch [580/747], Loss: 0.5832
2025-04-03 15:55:19,805 - INFO - Epoch [2/10], Batch [590/747], Loss: 0.6754
2025-04-03 15:56:19,998 - INFO - Epoch [2/10], Batch [600/747], Loss: 0.6007
2025-04-03 15:57:18,929 - INFO - Epoch [2/10], Batch [610/747], Loss: 0.5822
2025-04-03 15:58:17,342 - INFO - Epoch [2/10], Batch [620/747], Loss: 0.4868
2025-04-03 15:59:15,289 - INFO - Epoch [2/10], Batch [630/747], Loss: 0.5479
2025-04-03 16:00:12,558 - INFO - Epoch [2/10], Batch [640/747], Loss: 0.6686
2025-04-03 16:01:10,704 - INFO - Epoch [2/10], Batch [650/747], Loss: 0.5702
2025-04-03 16:02:08,799 - INFO - Epoch [2/10], Batch [660/747], Loss: 0.5377
2025-04-03 16:03:07,055 - INFO - Epoch [2/10], Batch [670/747], Loss: 0.6167
2025-04-03 16:04:05,284 - INFO - Epoch [2/10], Batch [680/747], Loss: 0.5159
2025-04-03 16:05:03,386 - INFO - Epoch [2/10], Batch [690/747], Loss: 0.5364
2025-04-03 16:06:01,768 - INFO - Epoch [2/10], Batch [700/747], Loss: 0.5783
2025-04-03 16:06:59,907 - INFO - Epoch [2/10], Batch [710/747], Loss: 0.5899
2025-04-03 16:07:57,893 - INFO - Epoch [2/10], Batch [720/747], Loss: 0.5658
2025-04-03 16:08:55,884 - INFO - Epoch [2/10], Batch [730/747], Loss: 0.5223
2025-04-03 16:09:53,512 - INFO - Epoch [2/10], Batch [740/747], Loss: 0.5084
2025-04-03 16:10:27,398 - INFO - Epoch 2/10 Train Loss: 0.6041, Train Accuracy: 66.42%
2025-04-03 16:15:36,234 - INFO - Epoch 2/10 Val Loss: 0.5403, Val Accuracy: 74.35%, AUC-ROC: 0.7640
2025-04-03 16:15:36,238 - INFO - New best model at epoch 2 with Val Accuracy: 74.35%
2025-04-03 16:15:43,118 - INFO - Epoch [3/10], Batch [0/747], Loss: 0.5158
2025-04-03 16:16:41,470 - INFO - Epoch [3/10], Batch [10/747], Loss: 0.7834
2025-04-03 16:17:39,878 - INFO - Epoch [3/10], Batch [20/747], Loss: 0.5282
2025-04-03 16:18:37,941 - INFO - Epoch [3/10], Batch [30/747], Loss: 0.4910
2025-04-03 16:19:35,832 - INFO - Epoch [3/10], Batch [40/747], Loss: 0.8703
2025-04-03 16:20:34,147 - INFO - Epoch [3/10], Batch [50/747], Loss: 0.5272
2025-04-03 16:21:32,762 - INFO - Epoch [3/10], Batch [60/747], Loss: 0.5326
2025-04-03 16:22:31,164 - INFO - Epoch [3/10], Batch [70/747], Loss: 0.6132
2025-04-03 16:23:29,689 - INFO - Epoch [3/10], Batch [80/747], Loss: 0.5091
2025-04-03 16:24:28,221 - INFO - Epoch [3/10], Batch [90/747], Loss: 0.5406
2025-04-03 16:25:26,488 - INFO - Epoch [3/10], Batch [100/747], Loss: 0.5889
2025-04-03 16:26:25,169 - INFO - Epoch [3/10], Batch [110/747], Loss: 0.5961
2025-04-03 16:27:23,853 - INFO - Epoch [3/10], Batch [120/747], Loss: 0.5763
2025-04-03 16:28:21,938 - INFO - Epoch [3/10], Batch [130/747], Loss: 0.6350
2025-04-03 16:29:20,100 - INFO - Epoch [3/10], Batch [140/747], Loss: 0.5069
2025-04-03 16:30:18,604 - INFO - Epoch [3/10], Batch [150/747], Loss: 0.4829
2025-04-03 16:31:17,354 - INFO - Epoch [3/10], Batch [160/747], Loss: 0.5031
2025-04-03 16:32:16,204 - INFO - Epoch [3/10], Batch [170/747], Loss: 0.5787
2025-04-03 16:33:15,218 - INFO - Epoch [3/10], Batch [180/747], Loss: 0.5764
2025-04-03 16:34:13,637 - INFO - Epoch [3/10], Batch [190/747], Loss: 0.5828
2025-04-03 16:35:11,726 - INFO - Epoch [3/10], Batch [200/747], Loss: 0.5189
2025-04-03 16:36:10,027 - INFO - Epoch [3/10], Batch [210/747], Loss: 0.5688
2025-04-03 16:37:08,782 - INFO - Epoch [3/10], Batch [220/747], Loss: 0.7074
2025-04-03 16:38:06,685 - INFO - Epoch [3/10], Batch [230/747], Loss: 0.5958
2025-04-03 16:39:05,005 - INFO - Epoch [3/10], Batch [240/747], Loss: 0.6237
2025-04-03 16:40:03,175 - INFO - Epoch [3/10], Batch [250/747], Loss: 0.6025
2025-04-03 16:41:01,263 - INFO - Epoch [3/10], Batch [260/747], Loss: 0.5347
2025-04-03 16:41:59,490 - INFO - Epoch [3/10], Batch [270/747], Loss: 0.5659
2025-04-03 16:42:57,223 - INFO - Epoch [3/10], Batch [280/747], Loss: 0.5355
2025-04-03 16:43:54,837 - INFO - Epoch [3/10], Batch [290/747], Loss: 0.4829
2025-04-03 16:44:52,145 - INFO - Epoch [3/10], Batch [300/747], Loss: 0.5364
2025-04-03 16:45:49,659 - INFO - Epoch [3/10], Batch [310/747], Loss: 0.4927
2025-04-03 16:46:47,086 - INFO - Epoch [3/10], Batch [320/747], Loss: 0.5259
2025-04-03 16:47:44,988 - INFO - Epoch [3/10], Batch [330/747], Loss: 0.5175
2025-04-03 16:48:43,410 - INFO - Epoch [3/10], Batch [340/747], Loss: 0.6277
2025-04-03 16:49:41,894 - INFO - Epoch [3/10], Batch [350/747], Loss: 0.5773
2025-04-03 16:50:39,985 - INFO - Epoch [3/10], Batch [360/747], Loss: 0.5867
2025-04-03 16:51:38,186 - INFO - Epoch [3/10], Batch [370/747], Loss: 0.5508
2025-04-03 16:52:36,123 - INFO - Epoch [3/10], Batch [380/747], Loss: 0.4667
2025-04-03 16:53:34,237 - INFO - Epoch [3/10], Batch [390/747], Loss: 0.6887
2025-04-03 16:54:31,681 - INFO - Epoch [3/10], Batch [400/747], Loss: 0.5900
2025-04-03 16:55:28,827 - INFO - Epoch [3/10], Batch [410/747], Loss: 0.4016
2025-04-03 16:56:26,708 - INFO - Epoch [3/10], Batch [420/747], Loss: 0.6101
2025-04-03 16:57:25,353 - INFO - Epoch [3/10], Batch [430/747], Loss: 0.5261
2025-04-03 16:58:23,740 - INFO - Epoch [3/10], Batch [440/747], Loss: 0.4790
2025-04-03 16:59:22,017 - INFO - Epoch [3/10], Batch [450/747], Loss: 0.4739
2025-04-03 17:00:20,484 - INFO - Epoch [3/10], Batch [460/747], Loss: 0.6461
2025-04-03 17:01:18,379 - INFO - Epoch [3/10], Batch [470/747], Loss: 0.7129
2025-04-03 17:02:15,840 - INFO - Epoch [3/10], Batch [480/747], Loss: 0.4727
2025-04-03 17:03:13,684 - INFO - Epoch [3/10], Batch [490/747], Loss: 0.5842
2025-04-03 17:04:11,651 - INFO - Epoch [3/10], Batch [500/747], Loss: 0.5697
2025-04-03 17:05:09,412 - INFO - Epoch [3/10], Batch [510/747], Loss: 0.5334
2025-04-03 17:06:07,378 - INFO - Epoch [3/10], Batch [520/747], Loss: 0.5308
2025-04-03 17:07:05,317 - INFO - Epoch [3/10], Batch [530/747], Loss: 0.5003
2025-04-03 17:08:03,221 - INFO - Epoch [3/10], Batch [540/747], Loss: 0.6766
2025-04-03 17:09:00,114 - INFO - Epoch [3/10], Batch [550/747], Loss: 0.6746
2025-04-03 17:09:57,184 - INFO - Epoch [3/10], Batch [560/747], Loss: 0.5047
2025-04-03 17:10:54,717 - INFO - Epoch [3/10], Batch [570/747], Loss: 0.6564
2025-04-03 17:11:52,376 - INFO - Epoch [3/10], Batch [580/747], Loss: 0.5647
2025-04-03 17:12:50,057 - INFO - Epoch [3/10], Batch [590/747], Loss: 0.5724
2025-04-03 17:13:47,676 - INFO - Epoch [3/10], Batch [600/747], Loss: 0.6291
2025-04-03 17:14:45,316 - INFO - Epoch [3/10], Batch [610/747], Loss: 0.5665
2025-04-03 17:15:42,728 - INFO - Epoch [3/10], Batch [620/747], Loss: 0.6643
2025-04-03 17:16:39,921 - INFO - Epoch [3/10], Batch [630/747], Loss: 0.4436
2025-04-03 17:17:37,068 - INFO - Epoch [3/10], Batch [640/747], Loss: 0.5341
2025-04-03 17:18:34,598 - INFO - Epoch [3/10], Batch [650/747], Loss: 0.6331
2025-04-03 17:19:32,401 - INFO - Epoch [3/10], Batch [660/747], Loss: 0.5242
2025-04-03 17:20:29,931 - INFO - Epoch [3/10], Batch [670/747], Loss: 0.4623
2025-04-03 17:21:27,136 - INFO - Epoch [3/10], Batch [680/747], Loss: 0.6250
2025-04-03 17:22:24,477 - INFO - Epoch [3/10], Batch [690/747], Loss: 0.4273
2025-04-03 17:23:22,366 - INFO - Epoch [3/10], Batch [700/747], Loss: 0.6547
2025-04-03 17:24:20,087 - INFO - Epoch [3/10], Batch [710/747], Loss: 0.5534
2025-04-03 17:25:17,992 - INFO - Epoch [3/10], Batch [720/747], Loss: 0.6412
2025-04-03 17:26:15,921 - INFO - Epoch [3/10], Batch [730/747], Loss: 0.4131
2025-04-03 17:27:13,493 - INFO - Epoch [3/10], Batch [740/747], Loss: 0.4355
2025-04-03 17:27:47,067 - INFO - Epoch 3/10 Train Loss: 0.5713, Train Accuracy: 69.29%
2025-04-03 17:32:55,831 - INFO - Epoch 3/10 Val Loss: 0.5012, Val Accuracy: 76.63%, AUC-ROC: 0.8146
2025-04-03 17:32:55,835 - INFO - New best model at epoch 3 with Val Accuracy: 76.63%
2025-04-03 17:33:03,323 - INFO - Epoch [4/10], Batch [0/747], Loss: 0.5177
2025-04-03 17:34:01,165 - INFO - Epoch [4/10], Batch [10/747], Loss: 0.4004
2025-04-03 17:34:58,935 - INFO - Epoch [4/10], Batch [20/747], Loss: 0.6113
2025-04-03 17:35:56,832 - INFO - Epoch [4/10], Batch [30/747], Loss: 0.4932
2025-04-03 17:36:54,231 - INFO - Epoch [4/10], Batch [40/747], Loss: 0.5990
2025-04-03 17:37:51,975 - INFO - Epoch [4/10], Batch [50/747], Loss: 0.5904
2025-04-03 17:38:49,613 - INFO - Epoch [4/10], Batch [60/747], Loss: 0.5408
2025-04-03 17:39:47,666 - INFO - Epoch [4/10], Batch [70/747], Loss: 0.6525
2025-04-03 17:40:45,196 - INFO - Epoch [4/10], Batch [80/747], Loss: 0.5200
2025-04-03 17:41:42,500 - INFO - Epoch [4/10], Batch [90/747], Loss: 0.6933
2025-04-03 17:42:40,105 - INFO - Epoch [4/10], Batch [100/747], Loss: 0.4944
2025-04-03 17:43:37,564 - INFO - Epoch [4/10], Batch [110/747], Loss: 0.6909
2025-04-03 17:44:35,123 - INFO - Epoch [4/10], Batch [120/747], Loss: 0.4882
2025-04-03 17:45:33,143 - INFO - Epoch [4/10], Batch [130/747], Loss: 0.3416
2025-04-03 17:46:31,057 - INFO - Epoch [4/10], Batch [140/747], Loss: 0.5453
2025-04-03 17:47:28,168 - INFO - Epoch [4/10], Batch [150/747], Loss: 0.4951
2025-04-03 17:48:25,590 - INFO - Epoch [4/10], Batch [160/747], Loss: 0.4825
2025-04-03 17:49:23,178 - INFO - Epoch [4/10], Batch [170/747], Loss: 0.6075
2025-04-03 17:50:20,504 - INFO - Epoch [4/10], Batch [180/747], Loss: 0.4994
2025-04-03 17:51:17,545 - INFO - Epoch [4/10], Batch [190/747], Loss: 0.5986
2025-04-03 17:52:14,323 - INFO - Epoch [4/10], Batch [200/747], Loss: 0.5253
2025-04-03 17:53:11,898 - INFO - Epoch [4/10], Batch [210/747], Loss: 0.7194
2025-04-03 17:54:09,677 - INFO - Epoch [4/10], Batch [220/747], Loss: 0.4553
2025-04-03 17:55:06,916 - INFO - Epoch [4/10], Batch [230/747], Loss: 0.4892
2025-04-03 17:56:05,084 - INFO - Epoch [4/10], Batch [240/747], Loss: 0.5367
2025-04-03 17:57:02,890 - INFO - Epoch [4/10], Batch [250/747], Loss: 0.5409
2025-04-03 17:58:00,888 - INFO - Epoch [4/10], Batch [260/747], Loss: 0.4846
2025-04-03 17:58:58,806 - INFO - Epoch [4/10], Batch [270/747], Loss: 0.5040
2025-04-03 17:59:56,616 - INFO - Epoch [4/10], Batch [280/747], Loss: 0.5988
2025-04-03 18:00:54,165 - INFO - Epoch [4/10], Batch [290/747], Loss: 0.4721
2025-04-03 18:01:52,270 - INFO - Epoch [4/10], Batch [300/747], Loss: 0.4376
2025-04-03 18:02:50,312 - INFO - Epoch [4/10], Batch [310/747], Loss: 0.4854
2025-04-03 18:03:48,245 - INFO - Epoch [4/10], Batch [320/747], Loss: 0.5776
2025-04-03 18:04:46,326 - INFO - Epoch [4/10], Batch [330/747], Loss: 0.6050
2025-04-03 18:05:44,243 - INFO - Epoch [4/10], Batch [340/747], Loss: 0.5107
2025-04-03 18:06:42,110 - INFO - Epoch [4/10], Batch [350/747], Loss: 0.4044
2025-04-03 18:07:40,485 - INFO - Epoch [4/10], Batch [360/747], Loss: 0.4585
2025-04-03 18:08:38,648 - INFO - Epoch [4/10], Batch [370/747], Loss: 0.4821
2025-04-03 18:09:36,757 - INFO - Epoch [4/10], Batch [380/747], Loss: 0.5349
2025-04-03 18:10:34,691 - INFO - Epoch [4/10], Batch [390/747], Loss: 0.4168
2025-04-03 18:11:32,432 - INFO - Epoch [4/10], Batch [400/747], Loss: 0.3925
2025-04-03 18:12:30,039 - INFO - Epoch [4/10], Batch [410/747], Loss: 0.5943
2025-04-03 18:13:27,532 - INFO - Epoch [4/10], Batch [420/747], Loss: 0.4462
2025-04-03 18:14:25,205 - INFO - Epoch [4/10], Batch [430/747], Loss: 0.4283
2025-04-03 18:15:23,086 - INFO - Epoch [4/10], Batch [440/747], Loss: 0.3338
2025-04-03 18:16:21,028 - INFO - Epoch [4/10], Batch [450/747], Loss: 0.4111
2025-04-03 18:17:18,080 - INFO - Epoch [4/10], Batch [460/747], Loss: 0.3314
2025-04-03 18:18:15,582 - INFO - Epoch [4/10], Batch [470/747], Loss: 0.5261
2025-04-03 18:19:13,630 - INFO - Epoch [4/10], Batch [480/747], Loss: 0.4219
2025-04-03 18:20:11,635 - INFO - Epoch [4/10], Batch [490/747], Loss: 0.5395
2025-04-03 18:21:09,151 - INFO - Epoch [4/10], Batch [500/747], Loss: 0.6107
2025-04-03 18:22:06,460 - INFO - Epoch [4/10], Batch [510/747], Loss: 0.3967
2025-04-03 18:23:04,398 - INFO - Epoch [4/10], Batch [520/747], Loss: 0.5120
2025-04-03 18:24:02,272 - INFO - Epoch [4/10], Batch [530/747], Loss: 0.5103
2025-04-03 18:25:00,351 - INFO - Epoch [4/10], Batch [540/747], Loss: 0.3692
2025-04-03 18:25:58,534 - INFO - Epoch [4/10], Batch [550/747], Loss: 0.6334
2025-04-03 18:26:56,208 - INFO - Epoch [4/10], Batch [560/747], Loss: 0.4256
2025-04-03 18:27:53,752 - INFO - Epoch [4/10], Batch [570/747], Loss: 0.4645
2025-04-03 18:28:51,185 - INFO - Epoch [4/10], Batch [580/747], Loss: 0.4116
2025-04-03 18:29:48,759 - INFO - Epoch [4/10], Batch [590/747], Loss: 0.3761
2025-04-03 18:30:46,641 - INFO - Epoch [4/10], Batch [600/747], Loss: 0.5128
2025-04-03 18:31:44,142 - INFO - Epoch [4/10], Batch [610/747], Loss: 0.4617
2025-04-03 18:32:41,749 - INFO - Epoch [4/10], Batch [620/747], Loss: 0.4742
2025-04-03 18:33:39,117 - INFO - Epoch [4/10], Batch [630/747], Loss: 0.7375
2025-04-03 18:34:36,896 - INFO - Epoch [4/10], Batch [640/747], Loss: 0.7603
2025-04-03 18:35:34,833 - INFO - Epoch [4/10], Batch [650/747], Loss: 0.4543
2025-04-03 18:36:32,814 - INFO - Epoch [4/10], Batch [660/747], Loss: 0.4039
2025-04-03 18:37:31,017 - INFO - Epoch [4/10], Batch [670/747], Loss: 0.4741
2025-04-03 18:38:29,034 - INFO - Epoch [4/10], Batch [680/747], Loss: 0.5343
2025-04-03 18:39:27,188 - INFO - Epoch [4/10], Batch [690/747], Loss: 0.3946
2025-04-03 18:40:25,316 - INFO - Epoch [4/10], Batch [700/747], Loss: 0.5519
2025-04-03 18:41:23,556 - INFO - Epoch [4/10], Batch [710/747], Loss: 0.5437
2025-04-03 18:42:21,772 - INFO - Epoch [4/10], Batch [720/747], Loss: 0.5714
2025-04-03 18:43:19,333 - INFO - Epoch [4/10], Batch [730/747], Loss: 0.3244
2025-04-03 18:44:17,048 - INFO - Epoch [4/10], Batch [740/747], Loss: 0.2705
2025-04-03 18:44:50,526 - INFO - Epoch 4/10 Train Loss: 0.5010, Train Accuracy: 74.90%
2025-04-03 18:49:54,775 - INFO - Epoch 4/10 Val Loss: 0.4203, Val Accuracy: 81.35%, AUC-ROC: 0.8799
2025-04-03 18:49:54,779 - INFO - New best model at epoch 4 with Val Accuracy: 81.35%
2025-04-03 18:50:01,689 - INFO - Epoch [5/10], Batch [0/747], Loss: 0.5177
2025-04-03 18:50:58,904 - INFO - Epoch [5/10], Batch [10/747], Loss: 0.6118
2025-04-03 18:51:56,766 - INFO - Epoch [5/10], Batch [20/747], Loss: 0.4750
2025-04-03 18:52:54,665 - INFO - Epoch [5/10], Batch [30/747], Loss: 0.5496
2025-04-03 18:53:53,251 - INFO - Epoch [5/10], Batch [40/747], Loss: 0.4265
2025-04-03 18:54:50,836 - INFO - Epoch [5/10], Batch [50/747], Loss: 0.4594
2025-04-03 18:55:48,225 - INFO - Epoch [5/10], Batch [60/747], Loss: 0.2796
2025-04-03 18:56:45,920 - INFO - Epoch [5/10], Batch [70/747], Loss: 0.4380
2025-04-03 18:57:43,440 - INFO - Epoch [5/10], Batch [80/747], Loss: 0.4908
2025-04-03 18:58:41,208 - INFO - Epoch [5/10], Batch [90/747], Loss: 0.5646
2025-04-03 18:59:38,834 - INFO - Epoch [5/10], Batch [100/747], Loss: 0.4507
2025-04-03 19:00:36,244 - INFO - Epoch [5/10], Batch [110/747], Loss: 0.6362
2025-04-03 19:01:33,377 - INFO - Epoch [5/10], Batch [120/747], Loss: 0.3109
2025-04-03 19:02:30,518 - INFO - Epoch [5/10], Batch [130/747], Loss: 0.3896
2025-04-03 19:03:28,004 - INFO - Epoch [5/10], Batch [140/747], Loss: 0.4672
2025-04-03 19:04:25,325 - INFO - Epoch [5/10], Batch [150/747], Loss: 0.3606
2025-04-03 19:05:22,879 - INFO - Epoch [5/10], Batch [160/747], Loss: 0.9066
2025-04-03 19:06:20,264 - INFO - Epoch [5/10], Batch [170/747], Loss: 0.3985
2025-04-03 19:07:17,931 - INFO - Epoch [5/10], Batch [180/747], Loss: 0.3200
2025-04-03 19:08:15,117 - INFO - Epoch [5/10], Batch [190/747], Loss: 0.3941
2025-04-03 19:09:12,127 - INFO - Epoch [5/10], Batch [200/747], Loss: 0.5227
2025-04-03 19:10:09,698 - INFO - Epoch [5/10], Batch [210/747], Loss: 0.4129
2025-04-03 19:11:07,627 - INFO - Epoch [5/10], Batch [220/747], Loss: 0.4635
2025-04-03 19:12:05,125 - INFO - Epoch [5/10], Batch [230/747], Loss: 0.5959
2025-04-03 19:13:02,747 - INFO - Epoch [5/10], Batch [240/747], Loss: 0.4220
2025-04-03 19:14:00,635 - INFO - Epoch [5/10], Batch [250/747], Loss: 0.4601
2025-04-03 19:14:58,068 - INFO - Epoch [5/10], Batch [260/747], Loss: 0.5329
2025-04-03 19:15:55,380 - INFO - Epoch [5/10], Batch [270/747], Loss: 0.4520
2025-04-03 19:16:53,256 - INFO - Epoch [5/10], Batch [280/747], Loss: 0.3355
2025-04-03 19:17:51,010 - INFO - Epoch [5/10], Batch [290/747], Loss: 0.5341
2025-04-03 19:18:48,424 - INFO - Epoch [5/10], Batch [300/747], Loss: 0.3900
2025-04-03 19:19:45,858 - INFO - Epoch [5/10], Batch [310/747], Loss: 0.5876
2025-04-03 19:20:43,505 - INFO - Epoch [5/10], Batch [320/747], Loss: 0.4364
2025-04-03 19:21:41,450 - INFO - Epoch [5/10], Batch [330/747], Loss: 0.3208
2025-04-03 19:22:38,962 - INFO - Epoch [5/10], Batch [340/747], Loss: 0.5129
2025-04-03 19:23:36,110 - INFO - Epoch [5/10], Batch [350/747], Loss: 0.5938
2025-04-03 19:24:33,423 - INFO - Epoch [5/10], Batch [360/747], Loss: 0.3621
2025-04-03 19:25:30,903 - INFO - Epoch [5/10], Batch [370/747], Loss: 0.6878
2025-04-03 19:26:29,117 - INFO - Epoch [5/10], Batch [380/747], Loss: 0.5649
2025-04-03 19:27:26,761 - INFO - Epoch [5/10], Batch [390/747], Loss: 0.4385
2025-04-03 19:28:24,751 - INFO - Epoch [5/10], Batch [400/747], Loss: 0.5037
2025-04-03 19:29:22,302 - INFO - Epoch [5/10], Batch [410/747], Loss: 0.3974
2025-04-03 19:30:19,697 - INFO - Epoch [5/10], Batch [420/747], Loss: 0.4514
2025-04-03 19:31:17,006 - INFO - Epoch [5/10], Batch [430/747], Loss: 0.5301
2025-04-03 19:32:14,392 - INFO - Epoch [5/10], Batch [440/747], Loss: 0.4060
2025-04-03 19:33:11,992 - INFO - Epoch [5/10], Batch [450/747], Loss: 0.4960
2025-04-03 19:34:09,052 - INFO - Epoch [5/10], Batch [460/747], Loss: 0.7799
2025-04-03 19:35:06,357 - INFO - Epoch [5/10], Batch [470/747], Loss: 0.4526
2025-04-03 19:36:03,573 - INFO - Epoch [5/10], Batch [480/747], Loss: 0.3556
2025-04-03 19:37:01,758 - INFO - Epoch [5/10], Batch [490/747], Loss: 0.4758
2025-04-03 19:37:59,845 - INFO - Epoch [5/10], Batch [500/747], Loss: 0.5085
2025-04-03 19:38:57,144 - INFO - Epoch [5/10], Batch [510/747], Loss: 0.5105
2025-04-03 19:39:53,901 - INFO - Epoch [5/10], Batch [520/747], Loss: 0.6241
2025-04-03 19:40:51,163 - INFO - Epoch [5/10], Batch [530/747], Loss: 0.5452
2025-04-03 19:41:48,566 - INFO - Epoch [5/10], Batch [540/747], Loss: 0.4148
2025-04-03 19:42:45,912 - INFO - Epoch [5/10], Batch [550/747], Loss: 0.3557
2025-04-03 19:43:43,370 - INFO - Epoch [5/10], Batch [560/747], Loss: 0.2855
2025-04-03 19:44:40,401 - INFO - Epoch [5/10], Batch [570/747], Loss: 0.5017
2025-04-03 19:45:37,552 - INFO - Epoch [5/10], Batch [580/747], Loss: 0.4163
2025-04-03 19:46:34,959 - INFO - Epoch [5/10], Batch [590/747], Loss: 0.4442
2025-04-03 19:47:32,659 - INFO - Epoch [5/10], Batch [600/747], Loss: 0.3284
2025-04-03 19:48:30,113 - INFO - Epoch [5/10], Batch [610/747], Loss: 0.3489
2025-04-03 19:49:27,472 - INFO - Epoch [5/10], Batch [620/747], Loss: 0.3094
2025-04-03 19:50:25,032 - INFO - Epoch [5/10], Batch [630/747], Loss: 0.5675
2025-04-03 19:51:22,608 - INFO - Epoch [5/10], Batch [640/747], Loss: 0.4614
2025-04-03 19:52:20,200 - INFO - Epoch [5/10], Batch [650/747], Loss: 0.3969
2025-04-03 19:53:17,663 - INFO - Epoch [5/10], Batch [660/747], Loss: 0.4852
2025-04-03 19:54:14,747 - INFO - Epoch [5/10], Batch [670/747], Loss: 0.3997
2025-04-03 19:55:12,076 - INFO - Epoch [5/10], Batch [680/747], Loss: 0.3397
2025-04-03 19:56:09,789 - INFO - Epoch [5/10], Batch [690/747], Loss: 0.5183
2025-04-03 19:57:07,538 - INFO - Epoch [5/10], Batch [700/747], Loss: 0.6168
2025-04-03 19:58:04,824 - INFO - Epoch [5/10], Batch [710/747], Loss: 0.4071
2025-04-03 19:59:02,539 - INFO - Epoch [5/10], Batch [720/747], Loss: 0.3833
2025-04-03 20:00:00,098 - INFO - Epoch [5/10], Batch [730/747], Loss: 0.3672
2025-04-03 20:00:57,389 - INFO - Epoch [5/10], Batch [740/747], Loss: 0.5240
2025-04-03 20:01:30,780 - INFO - Epoch 5/10 Train Loss: 0.4546, Train Accuracy: 77.89%
2025-04-03 20:06:39,203 - INFO - Epoch 5/10 Val Loss: 0.3801, Val Accuracy: 82.65%, AUC-ROC: 0.8999
2025-04-03 20:06:39,209 - INFO - New best model at epoch 5 with Val Accuracy: 82.65%
2025-04-03 20:06:46,095 - INFO - Epoch [6/10], Batch [0/747], Loss: 0.3525
2025-04-03 20:07:44,753 - INFO - Epoch [6/10], Batch [10/747], Loss: 0.4308
2025-04-03 20:08:43,869 - INFO - Epoch [6/10], Batch [20/747], Loss: 0.3556
2025-04-03 20:09:42,980 - INFO - Epoch [6/10], Batch [30/747], Loss: 0.4416
2025-04-03 20:10:42,240 - INFO - Epoch [6/10], Batch [40/747], Loss: 0.3516
2025-04-03 20:11:41,465 - INFO - Epoch [6/10], Batch [50/747], Loss: 0.4084
2025-04-03 20:12:40,061 - INFO - Epoch [6/10], Batch [60/747], Loss: 0.4883
2025-04-03 20:13:39,035 - INFO - Epoch [6/10], Batch [70/747], Loss: 0.4430
2025-04-03 20:14:38,076 - INFO - Epoch [6/10], Batch [80/747], Loss: 0.5064
2025-04-03 20:15:37,199 - INFO - Epoch [6/10], Batch [90/747], Loss: 0.3388
2025-04-03 20:16:35,740 - INFO - Epoch [6/10], Batch [100/747], Loss: 0.4844
2025-04-03 20:17:34,051 - INFO - Epoch [6/10], Batch [110/747], Loss: 0.5140
2025-04-03 20:18:32,441 - INFO - Epoch [6/10], Batch [120/747], Loss: 0.4331
2025-04-03 20:19:31,018 - INFO - Epoch [6/10], Batch [130/747], Loss: 0.2365
2025-04-03 20:20:29,520 - INFO - Epoch [6/10], Batch [140/747], Loss: 0.3427
2025-04-03 20:21:28,180 - INFO - Epoch [6/10], Batch [150/747], Loss: 0.4398
2025-04-03 20:22:26,837 - INFO - Epoch [6/10], Batch [160/747], Loss: 0.2396
2025-04-03 20:23:25,925 - INFO - Epoch [6/10], Batch [170/747], Loss: 0.2355
2025-04-03 20:24:24,345 - INFO - Epoch [6/10], Batch [180/747], Loss: 0.3747
2025-04-03 20:25:23,179 - INFO - Epoch [6/10], Batch [190/747], Loss: 0.5160
2025-04-03 20:26:22,371 - INFO - Epoch [6/10], Batch [200/747], Loss: 0.4083
2025-04-03 20:27:20,850 - INFO - Epoch [6/10], Batch [210/747], Loss: 0.4687
2025-04-03 20:28:19,509 - INFO - Epoch [6/10], Batch [220/747], Loss: 0.4087
2025-04-03 20:29:17,620 - INFO - Epoch [6/10], Batch [230/747], Loss: 0.4432
2025-04-03 20:30:16,596 - INFO - Epoch [6/10], Batch [240/747], Loss: 0.5016
2025-04-03 20:31:14,921 - INFO - Epoch [6/10], Batch [250/747], Loss: 0.4823
2025-04-03 20:32:14,280 - INFO - Epoch [6/10], Batch [260/747], Loss: 0.4438
2025-04-03 20:33:12,926 - INFO - Epoch [6/10], Batch [270/747], Loss: 0.5056
2025-04-03 20:34:11,415 - INFO - Epoch [6/10], Batch [280/747], Loss: 0.3909
2025-04-03 20:35:10,085 - INFO - Epoch [6/10], Batch [290/747], Loss: 0.5256
2025-04-03 20:36:08,442 - INFO - Epoch [6/10], Batch [300/747], Loss: 0.5332
2025-04-03 20:37:07,406 - INFO - Epoch [6/10], Batch [310/747], Loss: 0.5259
2025-04-03 20:38:05,987 - INFO - Epoch [6/10], Batch [320/747], Loss: 0.5120
2025-04-03 20:39:04,865 - INFO - Epoch [6/10], Batch [330/747], Loss: 0.3185
2025-04-03 20:40:03,748 - INFO - Epoch [6/10], Batch [340/747], Loss: 0.3423
2025-04-03 20:41:02,101 - INFO - Epoch [6/10], Batch [350/747], Loss: 0.3112
2025-04-03 20:42:01,093 - INFO - Epoch [6/10], Batch [360/747], Loss: 0.4381
2025-04-03 20:42:59,695 - INFO - Epoch [6/10], Batch [370/747], Loss: 0.4004
2025-04-03 20:43:58,296 - INFO - Epoch [6/10], Batch [380/747], Loss: 0.4104
2025-04-03 20:44:57,109 - INFO - Epoch [6/10], Batch [390/747], Loss: 0.4031
2025-04-03 20:45:55,973 - INFO - Epoch [6/10], Batch [400/747], Loss: 0.2938
2025-04-03 20:46:53,921 - INFO - Epoch [6/10], Batch [410/747], Loss: 0.4085
2025-04-03 20:47:52,128 - INFO - Epoch [6/10], Batch [420/747], Loss: 0.2949
2025-04-03 20:48:50,804 - INFO - Epoch [6/10], Batch [430/747], Loss: 0.4884
2025-04-03 20:49:49,399 - INFO - Epoch [6/10], Batch [440/747], Loss: 0.3114
2025-04-03 20:50:48,211 - INFO - Epoch [6/10], Batch [450/747], Loss: 0.4889
2025-04-03 20:51:47,197 - INFO - Epoch [6/10], Batch [460/747], Loss: 0.4031
2025-04-03 20:52:46,097 - INFO - Epoch [6/10], Batch [470/747], Loss: 0.3732
2025-04-03 20:53:44,661 - INFO - Epoch [6/10], Batch [480/747], Loss: 0.3244
2025-04-03 20:54:43,470 - INFO - Epoch [6/10], Batch [490/747], Loss: 0.5635
2025-04-03 20:55:42,576 - INFO - Epoch [6/10], Batch [500/747], Loss: 0.3975
2025-04-03 20:56:41,402 - INFO - Epoch [6/10], Batch [510/747], Loss: 0.6592
2025-04-03 20:57:40,188 - INFO - Epoch [6/10], Batch [520/747], Loss: 0.3880
2025-04-03 20:58:39,366 - INFO - Epoch [6/10], Batch [530/747], Loss: 0.4056
2025-04-03 20:59:38,705 - INFO - Epoch [6/10], Batch [540/747], Loss: 0.4022
2025-04-03 21:00:37,575 - INFO - Epoch [6/10], Batch [550/747], Loss: 0.4167
2025-04-03 21:01:36,568 - INFO - Epoch [6/10], Batch [560/747], Loss: 0.4595
2025-04-03 21:02:34,857 - INFO - Epoch [6/10], Batch [570/747], Loss: 0.4032
2025-04-03 21:03:33,345 - INFO - Epoch [6/10], Batch [580/747], Loss: 0.2908
2025-04-03 21:04:31,495 - INFO - Epoch [6/10], Batch [590/747], Loss: 0.2545
2025-04-03 21:05:29,152 - INFO - Epoch [6/10], Batch [600/747], Loss: 0.2317
2025-04-03 21:06:27,207 - INFO - Epoch [6/10], Batch [610/747], Loss: 0.3852
2025-04-03 21:07:26,163 - INFO - Epoch [6/10], Batch [620/747], Loss: 0.2947
2025-04-03 21:08:24,950 - INFO - Epoch [6/10], Batch [630/747], Loss: 0.3088
2025-04-03 21:09:23,594 - INFO - Epoch [6/10], Batch [640/747], Loss: 0.4611
2025-04-03 21:10:22,532 - INFO - Epoch [6/10], Batch [650/747], Loss: 0.2898
2025-04-03 21:11:21,128 - INFO - Epoch [6/10], Batch [660/747], Loss: 0.3908
2025-04-03 21:12:19,922 - INFO - Epoch [6/10], Batch [670/747], Loss: 0.4766
2025-04-03 21:13:18,465 - INFO - Epoch [6/10], Batch [680/747], Loss: 0.2541
2025-04-03 21:14:17,110 - INFO - Epoch [6/10], Batch [690/747], Loss: 0.3088
2025-04-03 21:15:15,995 - INFO - Epoch [6/10], Batch [700/747], Loss: 0.3365
2025-04-03 21:16:14,890 - INFO - Epoch [6/10], Batch [710/747], Loss: 0.4498
2025-04-03 21:17:12,945 - INFO - Epoch [6/10], Batch [720/747], Loss: 0.2245
2025-04-03 21:18:11,935 - INFO - Epoch [6/10], Batch [730/747], Loss: 0.3207
2025-04-03 21:19:10,924 - INFO - Epoch [6/10], Batch [740/747], Loss: 0.2732
2025-04-03 21:19:45,483 - INFO - Epoch 6/10 Train Loss: 0.4201, Train Accuracy: 80.00%
2025-04-03 21:24:53,392 - INFO - Epoch 6/10 Val Loss: 0.3390, Val Accuracy: 85.40%, AUC-ROC: 0.9203
2025-04-03 21:24:53,397 - INFO - New best model at epoch 6 with Val Accuracy: 85.40%
2025-04-03 21:25:01,059 - INFO - Epoch [7/10], Batch [0/747], Loss: 0.3179
2025-04-03 21:25:59,055 - INFO - Epoch [7/10], Batch [10/747], Loss: 0.4018
2025-04-03 21:26:56,754 - INFO - Epoch [7/10], Batch [20/747], Loss: 0.6728
2025-04-03 21:27:54,033 - INFO - Epoch [7/10], Batch [30/747], Loss: 0.4154
2025-04-03 21:28:51,802 - INFO - Epoch [7/10], Batch [40/747], Loss: 0.4441
2025-04-03 21:29:49,748 - INFO - Epoch [7/10], Batch [50/747], Loss: 0.3513
2025-04-03 21:30:47,584 - INFO - Epoch [7/10], Batch [60/747], Loss: 0.4982
2025-04-03 21:31:45,415 - INFO - Epoch [7/10], Batch [70/747], Loss: 0.3133
2025-04-03 21:32:42,950 - INFO - Epoch [7/10], Batch [80/747], Loss: 0.4627
2025-04-03 21:33:40,350 - INFO - Epoch [7/10], Batch [90/747], Loss: 0.2345
2025-04-03 21:34:38,092 - INFO - Epoch [7/10], Batch [100/747], Loss: 0.3936
2025-04-03 21:35:36,204 - INFO - Epoch [7/10], Batch [110/747], Loss: 0.3755
2025-04-03 21:36:33,759 - INFO - Epoch [7/10], Batch [120/747], Loss: 0.8298
2025-04-03 21:37:31,046 - INFO - Epoch [7/10], Batch [130/747], Loss: 0.3465
2025-04-03 21:38:28,452 - INFO - Epoch [7/10], Batch [140/747], Loss: 0.3907
2025-04-03 21:39:25,999 - INFO - Epoch [7/10], Batch [150/747], Loss: 0.3301
2025-04-03 21:40:23,846 - INFO - Epoch [7/10], Batch [160/747], Loss: 0.6506
2025-04-03 21:41:22,251 - INFO - Epoch [7/10], Batch [170/747], Loss: 0.3893
2025-04-03 21:42:20,646 - INFO - Epoch [7/10], Batch [180/747], Loss: 0.5299
2025-04-03 21:43:18,758 - INFO - Epoch [7/10], Batch [190/747], Loss: 0.4773
2025-04-03 21:44:16,502 - INFO - Epoch [7/10], Batch [200/747], Loss: 0.3440
2025-04-03 21:45:14,212 - INFO - Epoch [7/10], Batch [210/747], Loss: 0.3359
2025-04-03 21:46:12,511 - INFO - Epoch [7/10], Batch [220/747], Loss: 0.4488
2025-04-03 21:47:11,087 - INFO - Epoch [7/10], Batch [230/747], Loss: 0.2313
2025-04-03 21:48:09,194 - INFO - Epoch [7/10], Batch [240/747], Loss: 0.4329
2025-04-03 21:49:07,342 - INFO - Epoch [7/10], Batch [250/747], Loss: 0.4829
2025-04-03 21:50:05,420 - INFO - Epoch [7/10], Batch [260/747], Loss: 0.5203
2025-04-03 21:51:03,561 - INFO - Epoch [7/10], Batch [270/747], Loss: 0.4467
2025-04-03 21:52:01,663 - INFO - Epoch [7/10], Batch [280/747], Loss: 0.2598
2025-04-03 21:52:59,202 - INFO - Epoch [7/10], Batch [290/747], Loss: 0.2978
2025-04-03 21:53:57,394 - INFO - Epoch [7/10], Batch [300/747], Loss: 0.3343
2025-04-03 21:54:55,662 - INFO - Epoch [7/10], Batch [310/747], Loss: 0.2826
2025-04-03 21:55:54,011 - INFO - Epoch [7/10], Batch [320/747], Loss: 0.3782
2025-04-03 21:56:51,951 - INFO - Epoch [7/10], Batch [330/747], Loss: 0.2110
2025-04-03 21:57:49,665 - INFO - Epoch [7/10], Batch [340/747], Loss: 0.6486
2025-04-03 21:58:47,972 - INFO - Epoch [7/10], Batch [350/747], Loss: 0.3051
2025-04-03 21:59:46,230 - INFO - Epoch [7/10], Batch [360/747], Loss: 0.2410
2025-04-03 22:00:44,412 - INFO - Epoch [7/10], Batch [370/747], Loss: 0.3757
2025-04-03 22:01:42,553 - INFO - Epoch [7/10], Batch [380/747], Loss: 0.2967
2025-04-03 22:02:40,327 - INFO - Epoch [7/10], Batch [390/747], Loss: 0.1949
2025-04-03 22:03:38,030 - INFO - Epoch [7/10], Batch [400/747], Loss: 0.3165
2025-04-03 22:04:35,806 - INFO - Epoch [7/10], Batch [410/747], Loss: 0.4204
2025-04-03 22:05:33,680 - INFO - Epoch [7/10], Batch [420/747], Loss: 0.3853
2025-04-03 22:06:31,673 - INFO - Epoch [7/10], Batch [430/747], Loss: 0.3138
2025-04-03 22:07:29,772 - INFO - Epoch [7/10], Batch [440/747], Loss: 0.3717
2025-04-03 22:08:28,001 - INFO - Epoch [7/10], Batch [450/747], Loss: 0.3784
2025-04-03 22:09:25,978 - INFO - Epoch [7/10], Batch [460/747], Loss: 0.3569
2025-04-03 22:10:23,915 - INFO - Epoch [7/10], Batch [470/747], Loss: 0.3988
2025-04-03 22:11:21,648 - INFO - Epoch [7/10], Batch [480/747], Loss: 0.6408
2025-04-03 22:12:19,430 - INFO - Epoch [7/10], Batch [490/747], Loss: 0.4006
2025-04-03 22:13:17,388 - INFO - Epoch [7/10], Batch [500/747], Loss: 0.3520
2025-04-03 22:14:15,459 - INFO - Epoch [7/10], Batch [510/747], Loss: 0.3500
2025-04-03 22:15:13,406 - INFO - Epoch [7/10], Batch [520/747], Loss: 0.2843
2025-04-03 22:16:11,139 - INFO - Epoch [7/10], Batch [530/747], Loss: 0.4036
2025-04-03 22:17:09,286 - INFO - Epoch [7/10], Batch [540/747], Loss: 0.4991
2025-04-03 22:18:07,685 - INFO - Epoch [7/10], Batch [550/747], Loss: 0.3791
2025-04-03 22:19:05,875 - INFO - Epoch [7/10], Batch [560/747], Loss: 0.4177
2025-04-03 22:20:03,830 - INFO - Epoch [7/10], Batch [570/747], Loss: 0.2631
2025-04-03 22:21:01,482 - INFO - Epoch [7/10], Batch [580/747], Loss: 1.1253
2025-04-03 22:21:59,289 - INFO - Epoch [7/10], Batch [590/747], Loss: 0.3662
2025-04-03 22:22:56,753 - INFO - Epoch [7/10], Batch [600/747], Loss: 0.4020
2025-04-03 22:23:53,995 - INFO - Epoch [7/10], Batch [610/747], Loss: 0.3246
2025-04-03 22:24:51,444 - INFO - Epoch [7/10], Batch [620/747], Loss: 0.2139
2025-04-03 22:25:49,489 - INFO - Epoch [7/10], Batch [630/747], Loss: 0.3662
2025-04-03 22:26:47,391 - INFO - Epoch [7/10], Batch [640/747], Loss: 0.6691
2025-04-03 22:27:44,950 - INFO - Epoch [7/10], Batch [650/747], Loss: 0.3735
2025-04-03 22:28:42,396 - INFO - Epoch [7/10], Batch [660/747], Loss: 0.4766
2025-04-03 22:29:40,262 - INFO - Epoch [7/10], Batch [670/747], Loss: 0.2895
2025-04-03 22:30:38,183 - INFO - Epoch [7/10], Batch [680/747], Loss: 0.3292
2025-04-03 22:31:35,914 - INFO - Epoch [7/10], Batch [690/747], Loss: 0.3273
2025-04-03 22:32:33,604 - INFO - Epoch [7/10], Batch [700/747], Loss: 0.3958
2025-04-03 22:33:31,429 - INFO - Epoch [7/10], Batch [710/747], Loss: 0.4292
2025-04-03 22:34:29,592 - INFO - Epoch [7/10], Batch [720/747], Loss: 0.4032
2025-04-03 22:35:27,523 - INFO - Epoch [7/10], Batch [730/747], Loss: 0.2271
2025-04-03 22:36:25,832 - INFO - Epoch [7/10], Batch [740/747], Loss: 0.6079
2025-04-03 22:36:59,675 - INFO - Epoch 7/10 Train Loss: 0.3945, Train Accuracy: 81.94%
2025-04-03 22:42:05,946 - INFO - Epoch 7/10 Val Loss: 0.3309, Val Accuracy: 86.06%, AUC-ROC: 0.9232
2025-04-03 22:42:05,949 - INFO - New best model at epoch 7 with Val Accuracy: 86.06%
2025-04-03 22:42:13,683 - INFO - Epoch [8/10], Batch [0/747], Loss: 0.4360
2025-04-03 22:43:11,522 - INFO - Epoch [8/10], Batch [10/747], Loss: 0.3611
2025-04-03 22:44:09,371 - INFO - Epoch [8/10], Batch [20/747], Loss: 0.3872
2025-04-03 22:45:06,997 - INFO - Epoch [8/10], Batch [30/747], Loss: 0.3404
2025-04-03 22:46:04,440 - INFO - Epoch [8/10], Batch [40/747], Loss: 0.3786
2025-04-03 22:47:01,940 - INFO - Epoch [8/10], Batch [50/747], Loss: 0.3764
2025-04-03 22:47:59,486 - INFO - Epoch [8/10], Batch [60/747], Loss: 0.2661
2025-04-03 22:48:57,466 - INFO - Epoch [8/10], Batch [70/747], Loss: 0.3736
2025-04-03 22:49:55,233 - INFO - Epoch [8/10], Batch [80/747], Loss: 0.5804
2025-04-03 22:50:53,448 - INFO - Epoch [8/10], Batch [90/747], Loss: 0.4151
2025-04-03 22:51:51,054 - INFO - Epoch [8/10], Batch [100/747], Loss: 0.2960
2025-04-03 22:52:49,042 - INFO - Epoch [8/10], Batch [110/747], Loss: 0.3291
2025-04-03 22:53:47,050 - INFO - Epoch [8/10], Batch [120/747], Loss: 0.6328
2025-04-03 22:54:44,944 - INFO - Epoch [8/10], Batch [130/747], Loss: 0.3812
2025-04-03 22:55:42,913 - INFO - Epoch [8/10], Batch [140/747], Loss: 0.2517
2025-04-03 22:56:40,833 - INFO - Epoch [8/10], Batch [150/747], Loss: 0.2861
2025-04-03 22:57:38,713 - INFO - Epoch [8/10], Batch [160/747], Loss: 0.3873
2025-04-03 22:58:36,597 - INFO - Epoch [8/10], Batch [170/747], Loss: 0.4784
2025-04-03 22:59:34,154 - INFO - Epoch [8/10], Batch [180/747], Loss: 0.3408
2025-04-03 23:00:32,250 - INFO - Epoch [8/10], Batch [190/747], Loss: 0.3540
2025-04-03 23:01:30,424 - INFO - Epoch [8/10], Batch [200/747], Loss: 0.3135
2025-04-03 23:02:27,919 - INFO - Epoch [8/10], Batch [210/747], Loss: 0.2318
2025-04-03 23:03:25,489 - INFO - Epoch [8/10], Batch [220/747], Loss: 0.4256
2025-04-03 23:04:23,189 - INFO - Epoch [8/10], Batch [230/747], Loss: 0.4372
2025-04-03 23:05:20,666 - INFO - Epoch [8/10], Batch [240/747], Loss: 0.3516
2025-04-03 23:06:18,586 - INFO - Epoch [8/10], Batch [250/747], Loss: 0.5807
2025-04-03 23:07:16,969 - INFO - Epoch [8/10], Batch [260/747], Loss: 0.5308
2025-04-03 23:08:14,932 - INFO - Epoch [8/10], Batch [270/747], Loss: 0.4012
2025-04-03 23:09:12,946 - INFO - Epoch [8/10], Batch [280/747], Loss: 0.4346
2025-04-03 23:10:10,825 - INFO - Epoch [8/10], Batch [290/747], Loss: 0.3344
2025-04-03 23:11:08,886 - INFO - Epoch [8/10], Batch [300/747], Loss: 0.3831
2025-04-03 23:12:06,929 - INFO - Epoch [8/10], Batch [310/747], Loss: 0.5139
2025-04-03 23:13:04,884 - INFO - Epoch [8/10], Batch [320/747], Loss: 0.4493
2025-04-03 23:14:02,821 - INFO - Epoch [8/10], Batch [330/747], Loss: 0.3535
2025-04-03 23:15:00,616 - INFO - Epoch [8/10], Batch [340/747], Loss: 0.3322
2025-04-03 23:15:58,420 - INFO - Epoch [8/10], Batch [350/747], Loss: 0.4488
2025-04-03 23:16:56,632 - INFO - Epoch [8/10], Batch [360/747], Loss: 0.3662
2025-04-03 23:17:54,557 - INFO - Epoch [8/10], Batch [370/747], Loss: 0.2952
2025-04-03 23:18:52,889 - INFO - Epoch [8/10], Batch [380/747], Loss: 0.3386
2025-04-03 23:19:51,049 - INFO - Epoch [8/10], Batch [390/747], Loss: 0.2934
2025-04-03 23:20:48,882 - INFO - Epoch [8/10], Batch [400/747], Loss: 0.5202
2025-04-03 23:21:46,596 - INFO - Epoch [8/10], Batch [410/747], Loss: 0.1978
2025-04-03 23:22:44,327 - INFO - Epoch [8/10], Batch [420/747], Loss: 0.5603
2025-04-03 23:23:42,646 - INFO - Epoch [8/10], Batch [430/747], Loss: 0.6850
2025-04-03 23:24:40,657 - INFO - Epoch [8/10], Batch [440/747], Loss: 0.3693
2025-04-03 23:25:38,619 - INFO - Epoch [8/10], Batch [450/747], Loss: 0.4274
2025-04-03 23:26:36,382 - INFO - Epoch [8/10], Batch [460/747], Loss: 0.1649
2025-04-03 23:27:34,076 - INFO - Epoch [8/10], Batch [470/747], Loss: 0.4508
2025-04-03 23:28:31,815 - INFO - Epoch [8/10], Batch [480/747], Loss: 0.3870
2025-04-03 23:29:29,468 - INFO - Epoch [8/10], Batch [490/747], Loss: 0.3999
2025-04-03 23:30:27,655 - INFO - Epoch [8/10], Batch [500/747], Loss: 0.2957
2025-04-03 23:31:25,689 - INFO - Epoch [8/10], Batch [510/747], Loss: 0.4174
2025-04-03 23:32:23,626 - INFO - Epoch [8/10], Batch [520/747], Loss: 0.2190
2025-04-03 23:33:21,415 - INFO - Epoch [8/10], Batch [530/747], Loss: 0.2843
2025-04-03 23:34:18,985 - INFO - Epoch [8/10], Batch [540/747], Loss: 0.1503
2025-04-03 23:35:16,724 - INFO - Epoch [8/10], Batch [550/747], Loss: 0.3605
2025-04-03 23:36:14,447 - INFO - Epoch [8/10], Batch [560/747], Loss: 0.4355
2025-04-03 23:37:12,316 - INFO - Epoch [8/10], Batch [570/747], Loss: 0.2880
2025-04-03 23:38:10,193 - INFO - Epoch [8/10], Batch [580/747], Loss: 0.3076
2025-04-03 23:39:08,341 - INFO - Epoch [8/10], Batch [590/747], Loss: 0.4032
2025-04-03 23:40:06,332 - INFO - Epoch [8/10], Batch [600/747], Loss: 0.2796
2025-04-03 23:41:03,944 - INFO - Epoch [8/10], Batch [610/747], Loss: 0.5891
2025-04-03 23:42:01,777 - INFO - Epoch [8/10], Batch [620/747], Loss: 0.5706
2025-04-03 23:42:59,792 - INFO - Epoch [8/10], Batch [630/747], Loss: 0.3359
2025-04-03 23:43:57,407 - INFO - Epoch [8/10], Batch [640/747], Loss: 0.3436
2025-04-03 23:44:54,971 - INFO - Epoch [8/10], Batch [650/747], Loss: 0.2372
2025-04-03 23:45:52,185 - INFO - Epoch [8/10], Batch [660/747], Loss: 0.2702
2025-04-03 23:46:50,123 - INFO - Epoch [8/10], Batch [670/747], Loss: 0.2271
2025-04-03 23:47:48,113 - INFO - Epoch [8/10], Batch [680/747], Loss: 0.2652
2025-04-03 23:48:46,165 - INFO - Epoch [8/10], Batch [690/747], Loss: 0.2514
2025-04-03 23:49:43,924 - INFO - Epoch [8/10], Batch [700/747], Loss: 0.4243
2025-04-03 23:50:41,851 - INFO - Epoch [8/10], Batch [710/747], Loss: 0.2987
2025-04-03 23:51:39,801 - INFO - Epoch [8/10], Batch [720/747], Loss: 0.2311
2025-04-03 23:52:37,726 - INFO - Epoch [8/10], Batch [730/747], Loss: 0.5381
2025-04-03 23:53:35,447 - INFO - Epoch [8/10], Batch [740/747], Loss: 0.3558
2025-04-03 23:54:08,859 - INFO - Epoch 8/10 Train Loss: 0.3743, Train Accuracy: 82.78%
2025-04-03 23:59:15,693 - INFO - Epoch 8/10 Val Loss: 0.3025, Val Accuracy: 87.26%, AUC-ROC: 0.9374
2025-04-03 23:59:15,698 - INFO - New best model at epoch 8 with Val Accuracy: 87.26%
2025-04-03 23:59:22,652 - INFO - Epoch [9/10], Batch [0/747], Loss: 0.3547
2025-04-04 00:00:20,359 - INFO - Epoch [9/10], Batch [10/747], Loss: 0.2713
2025-04-04 00:01:17,565 - INFO - Epoch [9/10], Batch [20/747], Loss: 0.3791
2025-04-04 00:02:15,374 - INFO - Epoch [9/10], Batch [30/747], Loss: 0.3437
2025-04-04 00:03:12,931 - INFO - Epoch [9/10], Batch [40/747], Loss: 0.3302
2025-04-04 00:04:10,609 - INFO - Epoch [9/10], Batch [50/747], Loss: 0.4233
2025-04-04 00:05:07,902 - INFO - Epoch [9/10], Batch [60/747], Loss: 0.4108
2025-04-04 00:06:05,110 - INFO - Epoch [9/10], Batch [70/747], Loss: 0.3352
2025-04-04 00:07:02,476 - INFO - Epoch [9/10], Batch [80/747], Loss: 0.4211
2025-04-04 00:08:00,186 - INFO - Epoch [9/10], Batch [90/747], Loss: 0.3648
2025-04-04 00:08:57,921 - INFO - Epoch [9/10], Batch [100/747], Loss: 0.2962
2025-04-04 00:09:56,002 - INFO - Epoch [9/10], Batch [110/747], Loss: 0.2152
2025-04-04 00:10:53,047 - INFO - Epoch [9/10], Batch [120/747], Loss: 0.4031
2025-04-04 00:11:50,190 - INFO - Epoch [9/10], Batch [130/747], Loss: 0.2283
2025-04-04 00:12:47,889 - INFO - Epoch [9/10], Batch [140/747], Loss: 0.2697
2025-04-04 00:13:45,335 - INFO - Epoch [9/10], Batch [150/747], Loss: 0.3554
2025-04-04 00:14:42,917 - INFO - Epoch [9/10], Batch [160/747], Loss: 0.4611
2025-04-04 00:15:40,708 - INFO - Epoch [9/10], Batch [170/747], Loss: 0.4917
2025-04-04 00:16:38,508 - INFO - Epoch [9/10], Batch [180/747], Loss: 0.2708
2025-04-04 00:17:36,552 - INFO - Epoch [9/10], Batch [190/747], Loss: 0.3963
2025-04-04 00:18:34,311 - INFO - Epoch [9/10], Batch [200/747], Loss: 0.3324
2025-04-04 00:19:32,251 - INFO - Epoch [9/10], Batch [210/747], Loss: 0.4458
2025-04-04 00:20:30,147 - INFO - Epoch [9/10], Batch [220/747], Loss: 0.3056
2025-04-04 00:21:28,320 - INFO - Epoch [9/10], Batch [230/747], Loss: 0.4102
2025-04-04 00:22:26,120 - INFO - Epoch [9/10], Batch [240/747], Loss: 0.4517
2025-04-04 00:23:24,017 - INFO - Epoch [9/10], Batch [250/747], Loss: 0.3900
2025-04-04 00:24:22,133 - INFO - Epoch [9/10], Batch [260/747], Loss: 0.2197
2025-04-04 00:25:19,604 - INFO - Epoch [9/10], Batch [270/747], Loss: 0.3087
2025-04-04 00:26:18,137 - INFO - Epoch [9/10], Batch [280/747], Loss: 0.3190
2025-04-04 00:27:16,587 - INFO - Epoch [9/10], Batch [290/747], Loss: 0.4389
2025-04-04 00:28:15,216 - INFO - Epoch [9/10], Batch [300/747], Loss: 0.2514
2025-04-04 00:29:13,080 - INFO - Epoch [9/10], Batch [310/747], Loss: 0.2734
2025-04-04 00:30:10,628 - INFO - Epoch [9/10], Batch [320/747], Loss: 0.5016
2025-04-04 00:31:08,499 - INFO - Epoch [9/10], Batch [330/747], Loss: 0.3487
2025-04-04 00:32:05,919 - INFO - Epoch [9/10], Batch [340/747], Loss: 0.4767
2025-04-04 00:33:03,720 - INFO - Epoch [9/10], Batch [350/747], Loss: 0.3642
2025-04-04 00:34:01,570 - INFO - Epoch [9/10], Batch [360/747], Loss: 0.1333
2025-04-04 00:34:59,613 - INFO - Epoch [9/10], Batch [370/747], Loss: 0.2621
2025-04-04 00:35:57,242 - INFO - Epoch [9/10], Batch [380/747], Loss: 0.3073
2025-04-04 00:36:54,632 - INFO - Epoch [9/10], Batch [390/747], Loss: 0.2771
2025-04-04 00:37:52,229 - INFO - Epoch [9/10], Batch [400/747], Loss: 0.3453
2025-04-04 00:38:49,955 - INFO - Epoch [9/10], Batch [410/747], Loss: 0.3077
2025-04-04 00:39:47,294 - INFO - Epoch [9/10], Batch [420/747], Loss: 0.4036
2025-04-04 00:40:44,897 - INFO - Epoch [9/10], Batch [430/747], Loss: 0.2475
2025-04-04 00:41:42,446 - INFO - Epoch [9/10], Batch [440/747], Loss: 0.3021
2025-04-04 00:42:39,983 - INFO - Epoch [9/10], Batch [450/747], Loss: 0.5350
2025-04-04 00:43:37,758 - INFO - Epoch [9/10], Batch [460/747], Loss: 0.2026
2025-04-04 00:44:35,418 - INFO - Epoch [9/10], Batch [470/747], Loss: 0.2631
2025-04-04 00:45:33,006 - INFO - Epoch [9/10], Batch [480/747], Loss: 0.2300
2025-04-04 00:46:30,432 - INFO - Epoch [9/10], Batch [490/747], Loss: 0.4253
2025-04-04 00:47:28,203 - INFO - Epoch [9/10], Batch [500/747], Loss: 0.4609
2025-04-04 00:48:25,620 - INFO - Epoch [9/10], Batch [510/747], Loss: 0.3255
2025-04-04 00:49:23,444 - INFO - Epoch [9/10], Batch [520/747], Loss: 0.4576
2025-04-04 00:50:21,059 - INFO - Epoch [9/10], Batch [530/747], Loss: 0.2158
2025-04-04 00:51:18,667 - INFO - Epoch [9/10], Batch [540/747], Loss: 0.2577
2025-04-04 00:52:16,217 - INFO - Epoch [9/10], Batch [550/747], Loss: 0.4597
2025-04-04 00:53:13,499 - INFO - Epoch [9/10], Batch [560/747], Loss: 0.2479
2025-04-04 00:54:10,964 - INFO - Epoch [9/10], Batch [570/747], Loss: 0.2992
2025-04-04 00:55:08,367 - INFO - Epoch [9/10], Batch [580/747], Loss: 0.5805
2025-04-04 00:56:06,808 - INFO - Epoch [9/10], Batch [590/747], Loss: 0.1875
2025-04-04 00:57:04,653 - INFO - Epoch [9/10], Batch [600/747], Loss: 0.3491
2025-04-04 00:58:02,168 - INFO - Epoch [9/10], Batch [610/747], Loss: 0.2527
2025-04-04 00:58:59,750 - INFO - Epoch [9/10], Batch [620/747], Loss: 0.3506
2025-04-04 00:59:57,644 - INFO - Epoch [9/10], Batch [630/747], Loss: 0.3010
2025-04-04 01:00:55,048 - INFO - Epoch [9/10], Batch [640/747], Loss: 0.3373
2025-04-04 01:01:52,361 - INFO - Epoch [9/10], Batch [650/747], Loss: 0.4046
2025-04-04 01:02:50,225 - INFO - Epoch [9/10], Batch [660/747], Loss: 0.3306
2025-04-04 01:03:48,031 - INFO - Epoch [9/10], Batch [670/747], Loss: 0.5911
2025-04-04 01:04:45,458 - INFO - Epoch [9/10], Batch [680/747], Loss: 0.3879
2025-04-04 01:05:43,103 - INFO - Epoch [9/10], Batch [690/747], Loss: 0.2114
2025-04-04 01:06:40,979 - INFO - Epoch [9/10], Batch [700/747], Loss: 0.3300
2025-04-04 01:07:38,732 - INFO - Epoch [9/10], Batch [710/747], Loss: 0.3809
2025-04-04 01:08:36,281 - INFO - Epoch [9/10], Batch [720/747], Loss: 0.2739
2025-04-04 01:09:33,467 - INFO - Epoch [9/10], Batch [730/747], Loss: 0.2539
2025-04-04 01:10:30,767 - INFO - Epoch [9/10], Batch [740/747], Loss: 0.3756
2025-04-04 01:11:04,502 - INFO - Epoch 9/10 Train Loss: 0.3580, Train Accuracy: 83.56%
2025-04-04 01:16:13,653 - INFO - Epoch 9/10 Val Loss: 0.2883, Val Accuracy: 87.68%, AUC-ROC: 0.9417
2025-04-04 01:16:13,657 - INFO - New best model at epoch 9 with Val Accuracy: 87.68%
2025-04-04 01:16:20,463 - INFO - Epoch [10/10], Batch [0/747], Loss: 0.4114
2025-04-04 01:17:18,476 - INFO - Epoch [10/10], Batch [10/747], Loss: 0.4661
2025-04-04 01:18:16,851 - INFO - Epoch [10/10], Batch [20/747], Loss: 0.4851
2025-04-04 01:19:15,285 - INFO - Epoch [10/10], Batch [30/747], Loss: 0.3610
2025-04-04 01:20:12,880 - INFO - Epoch [10/10], Batch [40/747], Loss: 0.4642
2025-04-04 01:21:09,799 - INFO - Epoch [10/10], Batch [50/747], Loss: 0.3154
2025-04-04 01:22:07,616 - INFO - Epoch [10/10], Batch [60/747], Loss: 0.3630
2025-04-04 01:23:04,929 - INFO - Epoch [10/10], Batch [70/747], Loss: 0.2984
2025-04-04 01:24:03,093 - INFO - Epoch [10/10], Batch [80/747], Loss: 0.5070
2025-04-04 01:25:00,877 - INFO - Epoch [10/10], Batch [90/747], Loss: 0.2557
2025-04-04 01:25:58,777 - INFO - Epoch [10/10], Batch [100/747], Loss: 0.3320
2025-04-04 01:26:56,668 - INFO - Epoch [10/10], Batch [110/747], Loss: 0.3276
2025-04-04 01:27:54,476 - INFO - Epoch [10/10], Batch [120/747], Loss: 0.2876
2025-04-04 01:28:52,210 - INFO - Epoch [10/10], Batch [130/747], Loss: 0.3187
2025-04-04 01:29:49,984 - INFO - Epoch [10/10], Batch [140/747], Loss: 0.3906
2025-04-04 01:30:47,499 - INFO - Epoch [10/10], Batch [150/747], Loss: 0.2464
2025-04-04 01:31:45,320 - INFO - Epoch [10/10], Batch [160/747], Loss: 0.4804
2025-04-04 01:32:42,996 - INFO - Epoch [10/10], Batch [170/747], Loss: 0.3372
2025-04-04 01:33:40,832 - INFO - Epoch [10/10], Batch [180/747], Loss: 0.2773
2025-04-04 01:34:38,552 - INFO - Epoch [10/10], Batch [190/747], Loss: 0.3295
2025-04-04 01:35:36,074 - INFO - Epoch [10/10], Batch [200/747], Loss: 0.2787
2025-04-04 01:36:33,466 - INFO - Epoch [10/10], Batch [210/747], Loss: 0.1805
2025-04-04 01:37:30,338 - INFO - Epoch [10/10], Batch [220/747], Loss: 0.4504
2025-04-04 01:38:27,783 - INFO - Epoch [10/10], Batch [230/747], Loss: 0.3954
2025-04-04 01:39:25,331 - INFO - Epoch [10/10], Batch [240/747], Loss: 0.3554
2025-04-04 01:40:22,193 - INFO - Epoch [10/10], Batch [250/747], Loss: 0.3199
2025-04-04 01:41:18,984 - INFO - Epoch [10/10], Batch [260/747], Loss: 0.4500
2025-04-04 01:42:15,856 - INFO - Epoch [10/10], Batch [270/747], Loss: 0.2604
2025-04-04 01:43:12,842 - INFO - Epoch [10/10], Batch [280/747], Loss: 0.2924
2025-04-04 01:44:10,195 - INFO - Epoch [10/10], Batch [290/747], Loss: 0.4482
2025-04-04 01:45:07,911 - INFO - Epoch [10/10], Batch [300/747], Loss: 0.4198
2025-04-04 01:46:05,313 - INFO - Epoch [10/10], Batch [310/747], Loss: 0.2867
2025-04-04 01:47:02,622 - INFO - Epoch [10/10], Batch [320/747], Loss: 0.3827
2025-04-04 01:48:00,297 - INFO - Epoch [10/10], Batch [330/747], Loss: 0.3606
2025-04-04 01:48:56,950 - INFO - Epoch [10/10], Batch [340/747], Loss: 0.3122
2025-04-04 01:49:54,125 - INFO - Epoch [10/10], Batch [350/747], Loss: 0.4577
2025-04-04 01:50:52,430 - INFO - Epoch [10/10], Batch [360/747], Loss: 0.3942
2025-04-04 01:51:50,533 - INFO - Epoch [10/10], Batch [370/747], Loss: 0.2511
2025-04-04 01:52:48,612 - INFO - Epoch [10/10], Batch [380/747], Loss: 0.2397
2025-04-04 01:53:46,771 - INFO - Epoch [10/10], Batch [390/747], Loss: 0.3516
2025-04-04 01:54:44,510 - INFO - Epoch [10/10], Batch [400/747], Loss: 0.2572
2025-04-04 01:55:42,086 - INFO - Epoch [10/10], Batch [410/747], Loss: 0.3475
2025-04-04 01:56:39,404 - INFO - Epoch [10/10], Batch [420/747], Loss: 0.2368
2025-04-04 01:57:37,382 - INFO - Epoch [10/10], Batch [430/747], Loss: 0.4894
2025-04-04 01:58:35,010 - INFO - Epoch [10/10], Batch [440/747], Loss: 0.3344
2025-04-04 01:59:32,964 - INFO - Epoch [10/10], Batch [450/747], Loss: 0.3570
2025-04-04 02:00:30,742 - INFO - Epoch [10/10], Batch [460/747], Loss: 0.3848
2025-04-04 02:01:28,603 - INFO - Epoch [10/10], Batch [470/747], Loss: 0.2259
2025-04-04 02:02:26,516 - INFO - Epoch [10/10], Batch [480/747], Loss: 0.2571
2025-04-04 02:03:24,120 - INFO - Epoch [10/10], Batch [490/747], Loss: 0.3536
2025-04-04 02:04:21,881 - INFO - Epoch [10/10], Batch [500/747], Loss: 0.3168
2025-04-04 02:05:19,594 - INFO - Epoch [10/10], Batch [510/747], Loss: 0.2330
2025-04-04 02:06:17,241 - INFO - Epoch [10/10], Batch [520/747], Loss: 0.3439
2025-04-04 02:07:15,059 - INFO - Epoch [10/10], Batch [530/747], Loss: 0.2650
2025-04-04 02:08:13,168 - INFO - Epoch [10/10], Batch [540/747], Loss: 0.2445
2025-04-04 02:09:10,777 - INFO - Epoch [10/10], Batch [550/747], Loss: 0.2320
2025-04-04 02:10:08,699 - INFO - Epoch [10/10], Batch [560/747], Loss: 0.3396
2025-04-04 02:11:06,587 - INFO - Epoch [10/10], Batch [570/747], Loss: 0.4135
2025-04-04 02:12:04,015 - INFO - Epoch [10/10], Batch [580/747], Loss: 0.2928
2025-04-04 02:13:01,530 - INFO - Epoch [10/10], Batch [590/747], Loss: 0.3154
2025-04-04 02:13:58,982 - INFO - Epoch [10/10], Batch [600/747], Loss: 0.4135
2025-04-04 02:14:57,162 - INFO - Epoch [10/10], Batch [610/747], Loss: 0.2968
2025-04-04 02:15:55,057 - INFO - Epoch [10/10], Batch [620/747], Loss: 0.2102
2025-04-04 02:16:52,237 - INFO - Epoch [10/10], Batch [630/747], Loss: 0.3956
2025-04-04 02:17:49,521 - INFO - Epoch [10/10], Batch [640/747], Loss: 0.3936
2025-04-04 02:18:47,053 - INFO - Epoch [10/10], Batch [650/747], Loss: 0.3016
2025-04-04 02:19:45,010 - INFO - Epoch [10/10], Batch [660/747], Loss: 0.3322
2025-04-04 02:20:42,142 - INFO - Epoch [10/10], Batch [670/747], Loss: 0.5201
2025-04-04 02:21:38,765 - INFO - Epoch [10/10], Batch [680/747], Loss: 0.3264
2025-04-04 02:22:36,248 - INFO - Epoch [10/10], Batch [690/747], Loss: 0.2297
2025-04-04 02:23:33,995 - INFO - Epoch [10/10], Batch [700/747], Loss: 0.1974
2025-04-04 02:24:31,557 - INFO - Epoch [10/10], Batch [710/747], Loss: 0.4874
2025-04-04 02:25:29,135 - INFO - Epoch [10/10], Batch [720/747], Loss: 0.2956
2025-04-04 02:26:26,911 - INFO - Epoch [10/10], Batch [730/747], Loss: 0.4379
2025-04-04 02:27:24,500 - INFO - Epoch [10/10], Batch [740/747], Loss: 0.4040
2025-04-04 02:27:58,124 - INFO - Epoch 10/10 Train Loss: 0.3459, Train Accuracy: 84.11%
2025-04-04 02:33:04,239 - INFO - Epoch 10/10 Val Loss: 0.2814, Val Accuracy: 88.05%, AUC-ROC: 0.9446
2025-04-04 02:33:04,246 - INFO - New best model at epoch 10 with Val Accuracy: 88.05%
2025-04-04 02:33:04,247 - INFO - Total training time: 46234.66 seconds
2025-04-04 02:33:04,252 - INFO - Loaded best model with Val Accuracy: 88.05%
2025-04-04 02:35:38,241 - INFO - 
===== Final Test Results =====
2025-04-04 02:35:38,245 - INFO - Test Loss: 0.3016
2025-04-04 02:35:38,246 - INFO - Test Accuracy: 86.99%
2025-04-04 02:35:38,247 - INFO - Test AUC-ROC: 0.9361
2025-04-04 02:35:38,480 - INFO - Model saved to models/training_using_cpus_1_model.pth
2025-04-04 02:35:38,942 - INFO - Training plots saved as plots/training_using_cpus_1_results.png
2025-04-04 02:35:38,955 - INFO - Runtime parameters saved as metrics/training_using_cpus_1_params.json
2025-04-04 02:35:39,041 - INFO - Training completed.
2025-04-04 02:35:39,042 - INFO - Test Accuracy: 86.99%
2025-04-04 02:35:39,043 - INFO - Total Layers: 129
2025-04-04 02:35:39,044 - INFO - Total Parameters: 22,494,274
