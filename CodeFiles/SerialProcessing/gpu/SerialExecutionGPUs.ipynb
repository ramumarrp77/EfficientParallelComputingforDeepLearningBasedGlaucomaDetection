{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b011ea-331c-4edf-b310-a67e8d41413d",
   "metadata": {},
   "source": [
    "# CSYE 7105 - High Perfoamnce Machine Learning & AI - Project - Team 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0178d-11bf-40f1-a290-9b2f67733a09",
   "metadata": {},
   "source": [
    "## Analysis of Efficient Parallel Computing for Deep Learning-Based Glaucoma Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522f596-5f48-4c7c-a170-30118065407f",
   "metadata": {},
   "source": [
    "### Serial Execution using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d70aa-79e7-478b-860a-af211039e509",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc414e40-dda5-4fa5-9301-b3c2f2b1442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_fscore_support\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0923c-f289-43b1-9316-66a392e6cdc8",
   "metadata": {},
   "source": [
    "#### Defining the Glaucoma - Medical CNN model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ade6c-dd2f-4d5f-9920-f3ad28f7ef42",
   "metadata": {},
   "source": [
    "This MedicalCNN model, with 129 layers, uses a combination of convolutional, residual, multi-dilated, and fully connected blocks, tailored for glaucoma classification. Iâ€™ve included Residual Blocks to help with vanishing gradients, MultiDilated Blocks for enhanced feature extraction, and Squeeze-and-Excitation (SE) Blocks to fine-tune feature importance across channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c1b153-34ed-44b3-9cfd-01bdb9d87d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        image = torch.from_numpy(image).float()\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.permute(2, 0, 1)  # (H,W,C) -> (C,H,W)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, \n",
    "                              stride=stride, padding=padding, bias=False, dilation=dilation)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class MultiDilatedBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MultiDilatedBlock, self).__init__()\n",
    "        self.conv1 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv2 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv3 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=3, dilation=3)\n",
    "        self.conv4 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=4, dilation=4)\n",
    "        self.conv_fusion = ConvBlock(out_channels, out_channels, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "        x4 = self.conv4(x)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        return self.conv_fusion(x)\n",
    "\n",
    "class MedicalCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, base_filters=64):\n",
    "        super(MedicalCNN, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, base_filters, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(base_filters),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.stage1 = self._make_stage(base_filters, base_filters, blocks=3)\n",
    "        self.stage2 = self._make_stage(base_filters, base_filters*2, blocks=4, stride=2)\n",
    "        self.stage3_res = self._make_stage(base_filters*2, base_filters*4, blocks=6, stride=2)\n",
    "        self.stage3_md = MultiDilatedBlock(base_filters*4, base_filters*4)\n",
    "        self.stage4_res = self._make_stage(base_filters*4, base_filters*8, blocks=3, stride=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(base_filters*8, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_stage(self, in_channels, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3_res(x)\n",
    "        x = self.stage3_md(x)\n",
    "        x = self.stage4_res(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def count_layers(self):\n",
    "        stem_layers = 4\n",
    "        stage1_layers = 3 * 6\n",
    "        stage2_layers = 4 * 6\n",
    "        stage3_layers = 6 * 6 + 10\n",
    "        stage4_layers = 3 * 6 + 10\n",
    "        fc_layers = 9\n",
    "        total_layers = stem_layers + stage1_layers + stage2_layers + stage3_layers + stage4_layers + fc_layers\n",
    "        return total_layers\n",
    "\n",
    "def get_medical_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05, hue=0.05),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef906db0-9136-47fe-bcd3-aeb04aebd040",
   "metadata": {},
   "source": [
    "#### Defining the functions to train the model in 1GPU without DDP or other parallelization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f7c3c0-e3f5-4744-8421-1f5635c74a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level to INFO\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logs/training_using_gpus_1_logs.txt\"),\n",
    "        logging.StreamHandler()  # Also log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_and_evaluate(data_dir=\"../../preprocessed_glaucoma_data\"):\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    logging.info(\"Loading data...\")\n",
    "    X_train = np.load(os.path.join(data_dir, 'X_train.npy'))\n",
    "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
    "    X_val = np.load(os.path.join(data_dir, 'X_val.npy'))\n",
    "    y_val = np.load(os.path.join(data_dir, 'y_val.npy'))\n",
    "    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))\n",
    "    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "    \n",
    "    logging.info(f\"Data shapes: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}\")\n",
    "    \n",
    "    train_transform, val_transform = get_medical_transforms()\n",
    "    \n",
    "    train_dataset = GlaucomaDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = GlaucomaDataset(X_val, y_val, transform=val_transform)\n",
    "    test_dataset = GlaucomaDataset(X_test, y_test, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "    class_weights = torch.FloatTensor([1.0 / count for count in class_counts])\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    num_classes = 2\n",
    "    model = MedicalCNN(num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_layers = model.count_layers()\n",
    "    logging.info(f\"Model architecture: MedicalCNN\")\n",
    "    logging.info(f\"Total layers: {total_layers}\")\n",
    "    logging.info(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    initial_lr = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "    num_epochs = 10\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Lists to record metrics for plotting\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item() * data.size(0)\n",
    "            total_train_samples += data.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                logging.info(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_loss = epoch_train_loss / total_train_samples\n",
    "        train_accuracy = 100.0 * correct_train / total_train_samples\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        logging.info(f'Epoch {epoch+1}/{num_epochs} Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "        \n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val_samples = 0\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                epoch_val_loss += loss.item() * data.size(0)\n",
    "                total_val_samples += data.size(0)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct_val += (predicted == target).sum().item()\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_probabilities.extend(F.softmax(output, dim=1).cpu().numpy())\n",
    "        \n",
    "        val_loss = epoch_val_loss / total_val_samples\n",
    "        val_accuracy = 100.0 * correct_val / total_val_samples\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        if num_classes == 2:\n",
    "            all_targets = np.array(all_targets)\n",
    "            all_probabilities = np.array(all_probabilities)\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(all_targets, all_probabilities[:, 1])\n",
    "                logging.info(f'Epoch {epoch+1}/{num_epochs} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, AUC-ROC: {roc_auc:.4f}')\n",
    "            except Exception as e:\n",
    "                logging.info(f'Epoch {epoch+1}/{num_epochs} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        else:\n",
    "            logging.info(f'Epoch {epoch+1}/{num_epochs} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            logging.info(f\"New best model at epoch {epoch+1} with Val Accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"Total training time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Load best model for testing\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        logging.info(f\"Loaded best model with Val Accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test_samples = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            epoch_test_loss += loss.item() * data.size(0)\n",
    "            total_test_samples += data.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_test += (predicted == target).sum().item()\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_probabilities.extend(F.softmax(output, dim=1).cpu().numpy())\n",
    "    \n",
    "    test_loss = epoch_test_loss / total_test_samples\n",
    "    test_accuracy = 100.0 * correct_test / total_test_samples\n",
    "    \n",
    "    if num_classes == 2:\n",
    "        try:\n",
    "            test_auc = roc_auc_score(np.array(all_targets), np.array(all_probabilities)[:, 1])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculating Test AUC-ROC: {e}\")\n",
    "            test_auc = None\n",
    "    else:\n",
    "        test_auc = None\n",
    "    \n",
    "    logging.info(\"\\n===== Final Test Results =====\")\n",
    "    logging.info(f\"Test Loss: {test_loss:.4f}\")\n",
    "    logging.info(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    if test_auc is not None:\n",
    "        logging.info(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    model_filename = \"models/training_using_gpus_1_model.pth\"\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    logging.info(f\"Model saved to {model_filename}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    epochs_range = range(1, num_epochs+1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    plot_filename = \"plots/training_using_gpus_1_results.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    logging.info(f\"Training plots saved as {plot_filename}\")\n",
    "    \n",
    "    # Save runtime parameters and metrics as JSON\n",
    "    result = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_auc_roc\": test_auc,\n",
    "        \"computing_time\": total_time,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"initial_lr\": initial_lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"num_classes\": num_classes\n",
    "        },\n",
    "        \"total_parameters\": total_params,\n",
    "        \"total_layers\": total_layers\n",
    "    }\n",
    "    os.makedirs('metrics', exist_ok=True)\n",
    "    params_filename = \"metrics/training_using_gpus_1_params.json\"\n",
    "    with open(params_filename, 'w') as f:\n",
    "        json.dump(result, f, indent=4)\n",
    "    logging.info(f\"Runtime parameters saved as {params_filename}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f19ee-67dc-4785-b808-a9f421a84a08",
   "metadata": {},
   "source": [
    "#### Main function to handle and call all the functional flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3503d324-7fdb-46a1-99e9-89483e0ca1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 17:26:16,180 - INFO - Running on node: d1004\n",
      "2025-04-04 17:26:16,180 - INFO - Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single GPU...\n",
      "2025-04-04 17:26:16,186 - INFO - Loading data...\n",
      "2025-04-04 17:26:43,057 - INFO - Data shapes: Train (23898, 224, 224, 3), Val (5747, 224, 224, 3), Test (2874, 224, 224, 3)\n",
      "2025-04-04 17:26:43,114 - INFO - Using device: cuda\n",
      "2025-04-04 17:26:43,720 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-04 17:26:43,721 - INFO - Total layers: 129\n",
      "2025-04-04 17:26:43,721 - INFO - Total parameters: 22,494,274\n",
      "2025-04-04 17:26:44,930 - INFO - Epoch [1/10], Batch [0/747], Loss: 0.7060\n",
      "2025-04-04 17:26:45,573 - INFO - Epoch [1/10], Batch [10/747], Loss: 0.6762\n",
      "2025-04-04 17:26:46,203 - INFO - Epoch [1/10], Batch [20/747], Loss: 0.7399\n",
      "2025-04-04 17:26:46,833 - INFO - Epoch [1/10], Batch [30/747], Loss: 0.7481\n",
      "2025-04-04 17:26:47,460 - INFO - Epoch [1/10], Batch [40/747], Loss: 0.6709\n",
      "2025-04-04 17:26:48,090 - INFO - Epoch [1/10], Batch [50/747], Loss: 0.6425\n",
      "2025-04-04 17:26:48,719 - INFO - Epoch [1/10], Batch [60/747], Loss: 0.6451\n",
      "2025-04-04 17:26:49,349 - INFO - Epoch [1/10], Batch [70/747], Loss: 0.6676\n",
      "2025-04-04 17:26:49,978 - INFO - Epoch [1/10], Batch [80/747], Loss: 0.5724\n",
      "2025-04-04 17:26:50,609 - INFO - Epoch [1/10], Batch [90/747], Loss: 0.6042\n",
      "2025-04-04 17:26:51,238 - INFO - Epoch [1/10], Batch [100/747], Loss: 0.6673\n",
      "2025-04-04 17:26:51,867 - INFO - Epoch [1/10], Batch [110/747], Loss: 0.6507\n",
      "2025-04-04 17:26:52,497 - INFO - Epoch [1/10], Batch [120/747], Loss: 0.6771\n",
      "2025-04-04 17:26:53,126 - INFO - Epoch [1/10], Batch [130/747], Loss: 0.5774\n",
      "2025-04-04 17:26:53,755 - INFO - Epoch [1/10], Batch [140/747], Loss: 0.6925\n",
      "2025-04-04 17:26:54,384 - INFO - Epoch [1/10], Batch [150/747], Loss: 0.7941\n",
      "2025-04-04 17:26:55,012 - INFO - Epoch [1/10], Batch [160/747], Loss: 0.6486\n",
      "2025-04-04 17:26:55,642 - INFO - Epoch [1/10], Batch [170/747], Loss: 0.5632\n",
      "2025-04-04 17:26:56,272 - INFO - Epoch [1/10], Batch [180/747], Loss: 0.6257\n",
      "2025-04-04 17:26:56,901 - INFO - Epoch [1/10], Batch [190/747], Loss: 0.5918\n",
      "2025-04-04 17:26:57,530 - INFO - Epoch [1/10], Batch [200/747], Loss: 0.6498\n",
      "2025-04-04 17:26:58,158 - INFO - Epoch [1/10], Batch [210/747], Loss: 0.6398\n",
      "2025-04-04 17:26:58,788 - INFO - Epoch [1/10], Batch [220/747], Loss: 0.6722\n",
      "2025-04-04 17:26:59,417 - INFO - Epoch [1/10], Batch [230/747], Loss: 0.7163\n",
      "2025-04-04 17:27:00,045 - INFO - Epoch [1/10], Batch [240/747], Loss: 0.7323\n",
      "2025-04-04 17:27:00,675 - INFO - Epoch [1/10], Batch [250/747], Loss: 0.6578\n",
      "2025-04-04 17:27:01,304 - INFO - Epoch [1/10], Batch [260/747], Loss: 0.6664\n",
      "2025-04-04 17:27:01,944 - INFO - Epoch [1/10], Batch [270/747], Loss: 0.6716\n",
      "2025-04-04 17:27:02,612 - INFO - Epoch [1/10], Batch [280/747], Loss: 0.7521\n",
      "2025-04-04 17:27:03,241 - INFO - Epoch [1/10], Batch [290/747], Loss: 0.6132\n",
      "2025-04-04 17:27:03,870 - INFO - Epoch [1/10], Batch [300/747], Loss: 0.5217\n",
      "2025-04-04 17:27:04,499 - INFO - Epoch [1/10], Batch [310/747], Loss: 0.6916\n",
      "2025-04-04 17:27:05,128 - INFO - Epoch [1/10], Batch [320/747], Loss: 0.6201\n",
      "2025-04-04 17:27:05,756 - INFO - Epoch [1/10], Batch [330/747], Loss: 0.6029\n",
      "2025-04-04 17:27:06,384 - INFO - Epoch [1/10], Batch [340/747], Loss: 0.6288\n",
      "2025-04-04 17:27:07,015 - INFO - Epoch [1/10], Batch [350/747], Loss: 0.6296\n",
      "2025-04-04 17:27:07,644 - INFO - Epoch [1/10], Batch [360/747], Loss: 0.6514\n",
      "2025-04-04 17:27:08,273 - INFO - Epoch [1/10], Batch [370/747], Loss: 0.6760\n",
      "2025-04-04 17:27:08,902 - INFO - Epoch [1/10], Batch [380/747], Loss: 0.5734\n",
      "2025-04-04 17:27:09,531 - INFO - Epoch [1/10], Batch [390/747], Loss: 0.5895\n",
      "2025-04-04 17:27:10,159 - INFO - Epoch [1/10], Batch [400/747], Loss: 0.5934\n",
      "2025-04-04 17:27:10,787 - INFO - Epoch [1/10], Batch [410/747], Loss: 0.7140\n",
      "2025-04-04 17:27:11,417 - INFO - Epoch [1/10], Batch [420/747], Loss: 0.6718\n",
      "2025-04-04 17:27:12,046 - INFO - Epoch [1/10], Batch [430/747], Loss: 0.7312\n",
      "2025-04-04 17:27:12,676 - INFO - Epoch [1/10], Batch [440/747], Loss: 0.7356\n",
      "2025-04-04 17:27:13,304 - INFO - Epoch [1/10], Batch [450/747], Loss: 0.5939\n",
      "2025-04-04 17:27:13,933 - INFO - Epoch [1/10], Batch [460/747], Loss: 0.6910\n",
      "2025-04-04 17:27:14,564 - INFO - Epoch [1/10], Batch [470/747], Loss: 0.8246\n",
      "2025-04-04 17:27:15,192 - INFO - Epoch [1/10], Batch [480/747], Loss: 0.7292\n",
      "2025-04-04 17:27:15,821 - INFO - Epoch [1/10], Batch [490/747], Loss: 0.6103\n",
      "2025-04-04 17:27:16,450 - INFO - Epoch [1/10], Batch [500/747], Loss: 0.8626\n",
      "2025-04-04 17:27:17,079 - INFO - Epoch [1/10], Batch [510/747], Loss: 0.6151\n",
      "2025-04-04 17:27:17,709 - INFO - Epoch [1/10], Batch [520/747], Loss: 0.8032\n",
      "2025-04-04 17:27:18,340 - INFO - Epoch [1/10], Batch [530/747], Loss: 0.6685\n",
      "2025-04-04 17:27:18,968 - INFO - Epoch [1/10], Batch [540/747], Loss: 0.6669\n",
      "2025-04-04 17:27:19,598 - INFO - Epoch [1/10], Batch [550/747], Loss: 0.7102\n",
      "2025-04-04 17:27:20,227 - INFO - Epoch [1/10], Batch [560/747], Loss: 0.7054\n",
      "2025-04-04 17:27:20,855 - INFO - Epoch [1/10], Batch [570/747], Loss: 0.5790\n",
      "2025-04-04 17:27:21,484 - INFO - Epoch [1/10], Batch [580/747], Loss: 0.8206\n",
      "2025-04-04 17:27:22,114 - INFO - Epoch [1/10], Batch [590/747], Loss: 0.6242\n",
      "2025-04-04 17:27:22,743 - INFO - Epoch [1/10], Batch [600/747], Loss: 0.6703\n",
      "2025-04-04 17:27:23,374 - INFO - Epoch [1/10], Batch [610/747], Loss: 0.6693\n",
      "2025-04-04 17:27:24,003 - INFO - Epoch [1/10], Batch [620/747], Loss: 0.6300\n",
      "2025-04-04 17:27:24,633 - INFO - Epoch [1/10], Batch [630/747], Loss: 0.6488\n",
      "2025-04-04 17:27:25,262 - INFO - Epoch [1/10], Batch [640/747], Loss: 0.6767\n",
      "2025-04-04 17:27:25,891 - INFO - Epoch [1/10], Batch [650/747], Loss: 0.7537\n",
      "2025-04-04 17:27:26,519 - INFO - Epoch [1/10], Batch [660/747], Loss: 0.5889\n",
      "2025-04-04 17:27:27,149 - INFO - Epoch [1/10], Batch [670/747], Loss: 0.6136\n",
      "2025-04-04 17:27:27,778 - INFO - Epoch [1/10], Batch [680/747], Loss: 0.6173\n",
      "2025-04-04 17:27:28,407 - INFO - Epoch [1/10], Batch [690/747], Loss: 0.6496\n",
      "2025-04-04 17:27:29,036 - INFO - Epoch [1/10], Batch [700/747], Loss: 0.6680\n",
      "2025-04-04 17:27:29,665 - INFO - Epoch [1/10], Batch [710/747], Loss: 0.5958\n",
      "2025-04-04 17:27:30,293 - INFO - Epoch [1/10], Batch [720/747], Loss: 0.7212\n",
      "2025-04-04 17:27:30,922 - INFO - Epoch [1/10], Batch [730/747], Loss: 0.6394\n",
      "2025-04-04 17:27:31,552 - INFO - Epoch [1/10], Batch [740/747], Loss: 0.6195\n",
      "2025-04-04 17:27:31,955 - INFO - Epoch 1/10 Train Loss: 0.6564, Train Accuracy: 60.49%\n",
      "2025-04-04 17:27:35,688 - INFO - Epoch 1/10 Val Loss: 0.5789, Val Accuracy: 71.81%, AUC-ROC: 0.7365\n",
      "2025-04-04 17:27:35,691 - INFO - New best model at epoch 1 with Val Accuracy: 71.81%\n",
      "2025-04-04 17:27:36,165 - INFO - Epoch [2/10], Batch [0/747], Loss: 0.6200\n",
      "2025-04-04 17:27:36,796 - INFO - Epoch [2/10], Batch [10/747], Loss: 0.5701\n",
      "2025-04-04 17:27:37,427 - INFO - Epoch [2/10], Batch [20/747], Loss: 0.7462\n",
      "2025-04-04 17:27:38,058 - INFO - Epoch [2/10], Batch [30/747], Loss: 0.6496\n",
      "2025-04-04 17:27:38,688 - INFO - Epoch [2/10], Batch [40/747], Loss: 0.5964\n",
      "2025-04-04 17:27:39,318 - INFO - Epoch [2/10], Batch [50/747], Loss: 0.5435\n",
      "2025-04-04 17:27:39,949 - INFO - Epoch [2/10], Batch [60/747], Loss: 0.5325\n",
      "2025-04-04 17:27:40,580 - INFO - Epoch [2/10], Batch [70/747], Loss: 0.5735\n",
      "2025-04-04 17:27:41,209 - INFO - Epoch [2/10], Batch [80/747], Loss: 0.5560\n",
      "2025-04-04 17:27:41,840 - INFO - Epoch [2/10], Batch [90/747], Loss: 0.7285\n",
      "2025-04-04 17:27:42,470 - INFO - Epoch [2/10], Batch [100/747], Loss: 0.6377\n",
      "2025-04-04 17:27:43,100 - INFO - Epoch [2/10], Batch [110/747], Loss: 0.6602\n",
      "2025-04-04 17:27:43,731 - INFO - Epoch [2/10], Batch [120/747], Loss: 0.5613\n",
      "2025-04-04 17:27:44,361 - INFO - Epoch [2/10], Batch [130/747], Loss: 0.6282\n",
      "2025-04-04 17:27:44,991 - INFO - Epoch [2/10], Batch [140/747], Loss: 0.7047\n",
      "2025-04-04 17:27:45,621 - INFO - Epoch [2/10], Batch [150/747], Loss: 0.5969\n",
      "2025-04-04 17:27:46,251 - INFO - Epoch [2/10], Batch [160/747], Loss: 0.6182\n",
      "2025-04-04 17:27:46,881 - INFO - Epoch [2/10], Batch [170/747], Loss: 0.7419\n",
      "2025-04-04 17:27:47,512 - INFO - Epoch [2/10], Batch [180/747], Loss: 0.6049\n",
      "2025-04-04 17:27:48,141 - INFO - Epoch [2/10], Batch [190/747], Loss: 0.5126\n",
      "2025-04-04 17:27:48,773 - INFO - Epoch [2/10], Batch [200/747], Loss: 0.7419\n",
      "2025-04-04 17:27:49,403 - INFO - Epoch [2/10], Batch [210/747], Loss: 0.6843\n",
      "2025-04-04 17:27:50,033 - INFO - Epoch [2/10], Batch [220/747], Loss: 0.6438\n",
      "2025-04-04 17:27:50,662 - INFO - Epoch [2/10], Batch [230/747], Loss: 0.5507\n",
      "2025-04-04 17:27:51,293 - INFO - Epoch [2/10], Batch [240/747], Loss: 0.6761\n",
      "2025-04-04 17:27:51,925 - INFO - Epoch [2/10], Batch [250/747], Loss: 0.5984\n",
      "2025-04-04 17:27:52,555 - INFO - Epoch [2/10], Batch [260/747], Loss: 0.6136\n",
      "2025-04-04 17:27:53,185 - INFO - Epoch [2/10], Batch [270/747], Loss: 0.6162\n",
      "2025-04-04 17:27:53,816 - INFO - Epoch [2/10], Batch [280/747], Loss: 0.6096\n",
      "2025-04-04 17:27:54,448 - INFO - Epoch [2/10], Batch [290/747], Loss: 0.5202\n",
      "2025-04-04 17:27:55,078 - INFO - Epoch [2/10], Batch [300/747], Loss: 0.6578\n",
      "2025-04-04 17:27:55,708 - INFO - Epoch [2/10], Batch [310/747], Loss: 0.4904\n",
      "2025-04-04 17:27:56,339 - INFO - Epoch [2/10], Batch [320/747], Loss: 0.5042\n",
      "2025-04-04 17:27:56,969 - INFO - Epoch [2/10], Batch [330/747], Loss: 0.7443\n",
      "2025-04-04 17:27:57,600 - INFO - Epoch [2/10], Batch [340/747], Loss: 0.6425\n",
      "2025-04-04 17:27:58,232 - INFO - Epoch [2/10], Batch [350/747], Loss: 0.5827\n",
      "2025-04-04 17:27:58,863 - INFO - Epoch [2/10], Batch [360/747], Loss: 0.6976\n",
      "2025-04-04 17:27:59,494 - INFO - Epoch [2/10], Batch [370/747], Loss: 0.5392\n",
      "2025-04-04 17:28:00,125 - INFO - Epoch [2/10], Batch [380/747], Loss: 0.6237\n",
      "2025-04-04 17:28:00,755 - INFO - Epoch [2/10], Batch [390/747], Loss: 0.5131\n",
      "2025-04-04 17:28:01,387 - INFO - Epoch [2/10], Batch [400/747], Loss: 0.5678\n",
      "2025-04-04 17:28:02,016 - INFO - Epoch [2/10], Batch [410/747], Loss: 0.6779\n",
      "2025-04-04 17:28:02,646 - INFO - Epoch [2/10], Batch [420/747], Loss: 0.7438\n",
      "2025-04-04 17:28:03,275 - INFO - Epoch [2/10], Batch [430/747], Loss: 0.5240\n",
      "2025-04-04 17:28:03,905 - INFO - Epoch [2/10], Batch [440/747], Loss: 0.6227\n",
      "2025-04-04 17:28:04,535 - INFO - Epoch [2/10], Batch [450/747], Loss: 0.6075\n",
      "2025-04-04 17:28:05,166 - INFO - Epoch [2/10], Batch [460/747], Loss: 0.4230\n",
      "2025-04-04 17:28:05,797 - INFO - Epoch [2/10], Batch [470/747], Loss: 0.6746\n",
      "2025-04-04 17:28:06,427 - INFO - Epoch [2/10], Batch [480/747], Loss: 0.7482\n",
      "2025-04-04 17:28:07,058 - INFO - Epoch [2/10], Batch [490/747], Loss: 0.6168\n",
      "2025-04-04 17:28:07,688 - INFO - Epoch [2/10], Batch [500/747], Loss: 0.5529\n",
      "2025-04-04 17:28:08,319 - INFO - Epoch [2/10], Batch [510/747], Loss: 0.5678\n",
      "2025-04-04 17:28:08,950 - INFO - Epoch [2/10], Batch [520/747], Loss: 0.5322\n",
      "2025-04-04 17:28:09,580 - INFO - Epoch [2/10], Batch [530/747], Loss: 0.6180\n",
      "2025-04-04 17:28:10,212 - INFO - Epoch [2/10], Batch [540/747], Loss: 0.5846\n",
      "2025-04-04 17:28:10,842 - INFO - Epoch [2/10], Batch [550/747], Loss: 0.6215\n",
      "2025-04-04 17:28:11,474 - INFO - Epoch [2/10], Batch [560/747], Loss: 0.7978\n",
      "2025-04-04 17:28:12,105 - INFO - Epoch [2/10], Batch [570/747], Loss: 0.6316\n",
      "2025-04-04 17:28:12,736 - INFO - Epoch [2/10], Batch [580/747], Loss: 0.6627\n",
      "2025-04-04 17:28:13,366 - INFO - Epoch [2/10], Batch [590/747], Loss: 0.5517\n",
      "2025-04-04 17:28:13,997 - INFO - Epoch [2/10], Batch [600/747], Loss: 0.5612\n",
      "2025-04-04 17:28:14,629 - INFO - Epoch [2/10], Batch [610/747], Loss: 0.4061\n",
      "2025-04-04 17:28:15,260 - INFO - Epoch [2/10], Batch [620/747], Loss: 0.7108\n",
      "2025-04-04 17:28:15,889 - INFO - Epoch [2/10], Batch [630/747], Loss: 0.5649\n",
      "2025-04-04 17:28:16,520 - INFO - Epoch [2/10], Batch [640/747], Loss: 0.5352\n",
      "2025-04-04 17:28:17,150 - INFO - Epoch [2/10], Batch [650/747], Loss: 0.5398\n",
      "2025-04-04 17:28:17,780 - INFO - Epoch [2/10], Batch [660/747], Loss: 0.7270\n",
      "2025-04-04 17:28:18,410 - INFO - Epoch [2/10], Batch [670/747], Loss: 0.5722\n",
      "2025-04-04 17:28:19,041 - INFO - Epoch [2/10], Batch [680/747], Loss: 0.6567\n",
      "2025-04-04 17:28:19,671 - INFO - Epoch [2/10], Batch [690/747], Loss: 0.5576\n",
      "2025-04-04 17:28:20,302 - INFO - Epoch [2/10], Batch [700/747], Loss: 0.5882\n",
      "2025-04-04 17:28:20,931 - INFO - Epoch [2/10], Batch [710/747], Loss: 0.5732\n",
      "2025-04-04 17:28:21,561 - INFO - Epoch [2/10], Batch [720/747], Loss: 0.4886\n",
      "2025-04-04 17:28:22,191 - INFO - Epoch [2/10], Batch [730/747], Loss: 0.5633\n",
      "2025-04-04 17:28:22,821 - INFO - Epoch [2/10], Batch [740/747], Loss: 0.5948\n",
      "2025-04-04 17:28:23,233 - INFO - Epoch 2/10 Train Loss: 0.6178, Train Accuracy: 65.90%\n",
      "2025-04-04 17:28:26,935 - INFO - Epoch 2/10 Val Loss: 0.6058, Val Accuracy: 70.87%, AUC-ROC: 0.7593\n",
      "2025-04-04 17:28:27,411 - INFO - Epoch [3/10], Batch [0/747], Loss: 0.5155\n",
      "2025-04-04 17:28:28,041 - INFO - Epoch [3/10], Batch [10/747], Loss: 0.6646\n",
      "2025-04-04 17:28:28,672 - INFO - Epoch [3/10], Batch [20/747], Loss: 0.5879\n",
      "2025-04-04 17:28:29,301 - INFO - Epoch [3/10], Batch [30/747], Loss: 0.6520\n",
      "2025-04-04 17:28:29,931 - INFO - Epoch [3/10], Batch [40/747], Loss: 0.8039\n",
      "2025-04-04 17:28:30,561 - INFO - Epoch [3/10], Batch [50/747], Loss: 0.7999\n",
      "2025-04-04 17:28:31,193 - INFO - Epoch [3/10], Batch [60/747], Loss: 0.4937\n",
      "2025-04-04 17:28:31,822 - INFO - Epoch [3/10], Batch [70/747], Loss: 0.5657\n",
      "2025-04-04 17:28:32,452 - INFO - Epoch [3/10], Batch [80/747], Loss: 0.6320\n",
      "2025-04-04 17:28:33,082 - INFO - Epoch [3/10], Batch [90/747], Loss: 0.5598\n",
      "2025-04-04 17:28:33,712 - INFO - Epoch [3/10], Batch [100/747], Loss: 0.7997\n",
      "2025-04-04 17:28:34,342 - INFO - Epoch [3/10], Batch [110/747], Loss: 0.7436\n",
      "2025-04-04 17:28:34,974 - INFO - Epoch [3/10], Batch [120/747], Loss: 0.4297\n",
      "2025-04-04 17:28:35,605 - INFO - Epoch [3/10], Batch [130/747], Loss: 0.5961\n",
      "2025-04-04 17:28:36,235 - INFO - Epoch [3/10], Batch [140/747], Loss: 0.6167\n",
      "2025-04-04 17:28:36,867 - INFO - Epoch [3/10], Batch [150/747], Loss: 0.4091\n",
      "2025-04-04 17:28:37,499 - INFO - Epoch [3/10], Batch [160/747], Loss: 0.5190\n",
      "2025-04-04 17:28:38,128 - INFO - Epoch [3/10], Batch [170/747], Loss: 0.6049\n",
      "2025-04-04 17:28:38,758 - INFO - Epoch [3/10], Batch [180/747], Loss: 0.6380\n",
      "2025-04-04 17:28:39,388 - INFO - Epoch [3/10], Batch [190/747], Loss: 0.5150\n",
      "2025-04-04 17:28:40,019 - INFO - Epoch [3/10], Batch [200/747], Loss: 0.6003\n",
      "2025-04-04 17:28:40,648 - INFO - Epoch [3/10], Batch [210/747], Loss: 0.4893\n",
      "2025-04-04 17:28:41,278 - INFO - Epoch [3/10], Batch [220/747], Loss: 0.5127\n",
      "2025-04-04 17:28:41,910 - INFO - Epoch [3/10], Batch [230/747], Loss: 0.6463\n",
      "2025-04-04 17:28:42,539 - INFO - Epoch [3/10], Batch [240/747], Loss: 0.5662\n",
      "2025-04-04 17:28:43,169 - INFO - Epoch [3/10], Batch [250/747], Loss: 0.5642\n",
      "2025-04-04 17:28:43,800 - INFO - Epoch [3/10], Batch [260/747], Loss: 0.4430\n",
      "2025-04-04 17:28:44,429 - INFO - Epoch [3/10], Batch [270/747], Loss: 0.4896\n",
      "2025-04-04 17:28:45,059 - INFO - Epoch [3/10], Batch [280/747], Loss: 0.6019\n",
      "2025-04-04 17:28:45,689 - INFO - Epoch [3/10], Batch [290/747], Loss: 0.6771\n",
      "2025-04-04 17:28:46,319 - INFO - Epoch [3/10], Batch [300/747], Loss: 0.6687\n",
      "2025-04-04 17:28:46,949 - INFO - Epoch [3/10], Batch [310/747], Loss: 0.5743\n",
      "2025-04-04 17:28:47,579 - INFO - Epoch [3/10], Batch [320/747], Loss: 0.5184\n",
      "2025-04-04 17:28:48,209 - INFO - Epoch [3/10], Batch [330/747], Loss: 0.5984\n",
      "2025-04-04 17:28:48,838 - INFO - Epoch [3/10], Batch [340/747], Loss: 0.6519\n",
      "2025-04-04 17:28:49,468 - INFO - Epoch [3/10], Batch [350/747], Loss: 0.5384\n",
      "2025-04-04 17:28:50,098 - INFO - Epoch [3/10], Batch [360/747], Loss: 0.7295\n",
      "2025-04-04 17:28:50,728 - INFO - Epoch [3/10], Batch [370/747], Loss: 0.3891\n",
      "2025-04-04 17:28:51,358 - INFO - Epoch [3/10], Batch [380/747], Loss: 0.6681\n",
      "2025-04-04 17:28:51,988 - INFO - Epoch [3/10], Batch [390/747], Loss: 0.4285\n",
      "2025-04-04 17:28:52,618 - INFO - Epoch [3/10], Batch [400/747], Loss: 0.4980\n",
      "2025-04-04 17:28:53,248 - INFO - Epoch [3/10], Batch [410/747], Loss: 0.6386\n",
      "2025-04-04 17:28:53,878 - INFO - Epoch [3/10], Batch [420/747], Loss: 0.4243\n",
      "2025-04-04 17:28:54,508 - INFO - Epoch [3/10], Batch [430/747], Loss: 0.4590\n",
      "2025-04-04 17:28:55,138 - INFO - Epoch [3/10], Batch [440/747], Loss: 0.5713\n",
      "2025-04-04 17:28:55,769 - INFO - Epoch [3/10], Batch [450/747], Loss: 0.6144\n",
      "2025-04-04 17:28:56,399 - INFO - Epoch [3/10], Batch [460/747], Loss: 0.5324\n",
      "2025-04-04 17:28:57,030 - INFO - Epoch [3/10], Batch [470/747], Loss: 0.5160\n",
      "2025-04-04 17:28:57,660 - INFO - Epoch [3/10], Batch [480/747], Loss: 0.6304\n",
      "2025-04-04 17:28:58,290 - INFO - Epoch [3/10], Batch [490/747], Loss: 0.6687\n",
      "2025-04-04 17:28:58,920 - INFO - Epoch [3/10], Batch [500/747], Loss: 0.4914\n",
      "2025-04-04 17:28:59,550 - INFO - Epoch [3/10], Batch [510/747], Loss: 0.6527\n",
      "2025-04-04 17:29:00,181 - INFO - Epoch [3/10], Batch [520/747], Loss: 0.4255\n",
      "2025-04-04 17:29:00,811 - INFO - Epoch [3/10], Batch [530/747], Loss: 0.5867\n",
      "2025-04-04 17:29:01,440 - INFO - Epoch [3/10], Batch [540/747], Loss: 0.5766\n",
      "2025-04-04 17:29:02,072 - INFO - Epoch [3/10], Batch [550/747], Loss: 0.4543\n",
      "2025-04-04 17:29:02,701 - INFO - Epoch [3/10], Batch [560/747], Loss: 0.5733\n",
      "2025-04-04 17:29:03,330 - INFO - Epoch [3/10], Batch [570/747], Loss: 0.6090\n",
      "2025-04-04 17:29:03,962 - INFO - Epoch [3/10], Batch [580/747], Loss: 0.5123\n",
      "2025-04-04 17:29:04,592 - INFO - Epoch [3/10], Batch [590/747], Loss: 0.6876\n",
      "2025-04-04 17:29:05,222 - INFO - Epoch [3/10], Batch [600/747], Loss: 0.4908\n",
      "2025-04-04 17:29:05,852 - INFO - Epoch [3/10], Batch [610/747], Loss: 0.5659\n",
      "2025-04-04 17:29:06,482 - INFO - Epoch [3/10], Batch [620/747], Loss: 0.4380\n",
      "2025-04-04 17:29:07,112 - INFO - Epoch [3/10], Batch [630/747], Loss: 0.4352\n",
      "2025-04-04 17:29:07,742 - INFO - Epoch [3/10], Batch [640/747], Loss: 0.4034\n",
      "2025-04-04 17:29:08,372 - INFO - Epoch [3/10], Batch [650/747], Loss: 0.6636\n",
      "2025-04-04 17:29:09,002 - INFO - Epoch [3/10], Batch [660/747], Loss: 0.4546\n",
      "2025-04-04 17:29:09,633 - INFO - Epoch [3/10], Batch [670/747], Loss: 0.4936\n",
      "2025-04-04 17:29:10,262 - INFO - Epoch [3/10], Batch [680/747], Loss: 0.5885\n",
      "2025-04-04 17:29:10,892 - INFO - Epoch [3/10], Batch [690/747], Loss: 0.4527\n",
      "2025-04-04 17:29:11,522 - INFO - Epoch [3/10], Batch [700/747], Loss: 0.4906\n",
      "2025-04-04 17:29:12,152 - INFO - Epoch [3/10], Batch [710/747], Loss: 0.4177\n",
      "2025-04-04 17:29:12,781 - INFO - Epoch [3/10], Batch [720/747], Loss: 0.6620\n",
      "2025-04-04 17:29:13,412 - INFO - Epoch [3/10], Batch [730/747], Loss: 0.6671\n",
      "2025-04-04 17:29:14,043 - INFO - Epoch [3/10], Batch [740/747], Loss: 0.7200\n",
      "2025-04-04 17:29:14,450 - INFO - Epoch 3/10 Train Loss: 0.5613, Train Accuracy: 70.98%\n",
      "2025-04-04 17:29:18,164 - INFO - Epoch 3/10 Val Loss: 0.5125, Val Accuracy: 76.20%, AUC-ROC: 0.8520\n",
      "2025-04-04 17:29:18,168 - INFO - New best model at epoch 3 with Val Accuracy: 76.20%\n",
      "2025-04-04 17:29:18,662 - INFO - Epoch [4/10], Batch [0/747], Loss: 0.5965\n",
      "2025-04-04 17:29:19,293 - INFO - Epoch [4/10], Batch [10/747], Loss: 0.5724\n",
      "2025-04-04 17:29:19,922 - INFO - Epoch [4/10], Batch [20/747], Loss: 0.4221\n",
      "2025-04-04 17:29:20,552 - INFO - Epoch [4/10], Batch [30/747], Loss: 0.5469\n",
      "2025-04-04 17:29:21,182 - INFO - Epoch [4/10], Batch [40/747], Loss: 0.4923\n",
      "2025-04-04 17:29:21,812 - INFO - Epoch [4/10], Batch [50/747], Loss: 0.6392\n",
      "2025-04-04 17:29:22,442 - INFO - Epoch [4/10], Batch [60/747], Loss: 0.4479\n",
      "2025-04-04 17:29:23,071 - INFO - Epoch [4/10], Batch [70/747], Loss: 0.4417\n",
      "2025-04-04 17:29:23,702 - INFO - Epoch [4/10], Batch [80/747], Loss: 0.4910\n",
      "2025-04-04 17:29:24,332 - INFO - Epoch [4/10], Batch [90/747], Loss: 0.6170\n",
      "2025-04-04 17:29:24,961 - INFO - Epoch [4/10], Batch [100/747], Loss: 0.4668\n",
      "2025-04-04 17:29:25,591 - INFO - Epoch [4/10], Batch [110/747], Loss: 0.4872\n",
      "2025-04-04 17:29:26,221 - INFO - Epoch [4/10], Batch [120/747], Loss: 0.3874\n",
      "2025-04-04 17:29:26,851 - INFO - Epoch [4/10], Batch [130/747], Loss: 0.5485\n",
      "2025-04-04 17:29:27,480 - INFO - Epoch [4/10], Batch [140/747], Loss: 0.4907\n",
      "2025-04-04 17:29:28,110 - INFO - Epoch [4/10], Batch [150/747], Loss: 0.4670\n",
      "2025-04-04 17:29:28,740 - INFO - Epoch [4/10], Batch [160/747], Loss: 0.4099\n",
      "2025-04-04 17:29:29,370 - INFO - Epoch [4/10], Batch [170/747], Loss: 0.4264\n",
      "2025-04-04 17:29:30,000 - INFO - Epoch [4/10], Batch [180/747], Loss: 0.4109\n",
      "2025-04-04 17:29:30,629 - INFO - Epoch [4/10], Batch [190/747], Loss: 0.7164\n",
      "2025-04-04 17:29:31,260 - INFO - Epoch [4/10], Batch [200/747], Loss: 0.7047\n",
      "2025-04-04 17:29:31,891 - INFO - Epoch [4/10], Batch [210/747], Loss: 0.3785\n",
      "2025-04-04 17:29:32,521 - INFO - Epoch [4/10], Batch [220/747], Loss: 0.4961\n",
      "2025-04-04 17:29:33,152 - INFO - Epoch [4/10], Batch [230/747], Loss: 0.4339\n",
      "2025-04-04 17:29:33,782 - INFO - Epoch [4/10], Batch [240/747], Loss: 0.4353\n",
      "2025-04-04 17:29:34,413 - INFO - Epoch [4/10], Batch [250/747], Loss: 0.3764\n",
      "2025-04-04 17:29:35,042 - INFO - Epoch [4/10], Batch [260/747], Loss: 0.5672\n",
      "2025-04-04 17:29:35,672 - INFO - Epoch [4/10], Batch [270/747], Loss: 0.6864\n",
      "2025-04-04 17:29:36,301 - INFO - Epoch [4/10], Batch [280/747], Loss: 0.4662\n",
      "2025-04-04 17:29:36,932 - INFO - Epoch [4/10], Batch [290/747], Loss: 0.4676\n",
      "2025-04-04 17:29:37,562 - INFO - Epoch [4/10], Batch [300/747], Loss: 0.6180\n",
      "2025-04-04 17:29:38,192 - INFO - Epoch [4/10], Batch [310/747], Loss: 0.4535\n",
      "2025-04-04 17:29:38,822 - INFO - Epoch [4/10], Batch [320/747], Loss: 0.5825\n",
      "2025-04-04 17:29:39,452 - INFO - Epoch [4/10], Batch [330/747], Loss: 0.3816\n",
      "2025-04-04 17:29:40,083 - INFO - Epoch [4/10], Batch [340/747], Loss: 0.6293\n",
      "2025-04-04 17:29:40,714 - INFO - Epoch [4/10], Batch [350/747], Loss: 0.5000\n",
      "2025-04-04 17:29:41,344 - INFO - Epoch [4/10], Batch [360/747], Loss: 0.4631\n",
      "2025-04-04 17:29:41,974 - INFO - Epoch [4/10], Batch [370/747], Loss: 0.4434\n",
      "2025-04-04 17:29:42,604 - INFO - Epoch [4/10], Batch [380/747], Loss: 0.4846\n",
      "2025-04-04 17:29:43,235 - INFO - Epoch [4/10], Batch [390/747], Loss: 0.6205\n",
      "2025-04-04 17:29:43,865 - INFO - Epoch [4/10], Batch [400/747], Loss: 0.5267\n",
      "2025-04-04 17:29:44,495 - INFO - Epoch [4/10], Batch [410/747], Loss: 0.4995\n",
      "2025-04-04 17:29:45,125 - INFO - Epoch [4/10], Batch [420/747], Loss: 0.7268\n",
      "2025-04-04 17:29:45,755 - INFO - Epoch [4/10], Batch [430/747], Loss: 0.5845\n",
      "2025-04-04 17:29:46,385 - INFO - Epoch [4/10], Batch [440/747], Loss: 0.4525\n",
      "2025-04-04 17:29:47,015 - INFO - Epoch [4/10], Batch [450/747], Loss: 0.6399\n",
      "2025-04-04 17:29:47,644 - INFO - Epoch [4/10], Batch [460/747], Loss: 0.5424\n",
      "2025-04-04 17:29:48,274 - INFO - Epoch [4/10], Batch [470/747], Loss: 0.5010\n",
      "2025-04-04 17:29:48,905 - INFO - Epoch [4/10], Batch [480/747], Loss: 0.5060\n",
      "2025-04-04 17:29:49,535 - INFO - Epoch [4/10], Batch [490/747], Loss: 0.3094\n",
      "2025-04-04 17:29:50,165 - INFO - Epoch [4/10], Batch [500/747], Loss: 0.7445\n",
      "2025-04-04 17:29:50,795 - INFO - Epoch [4/10], Batch [510/747], Loss: 0.4860\n",
      "2025-04-04 17:29:51,425 - INFO - Epoch [4/10], Batch [520/747], Loss: 0.4569\n",
      "2025-04-04 17:29:52,055 - INFO - Epoch [4/10], Batch [530/747], Loss: 0.5912\n",
      "2025-04-04 17:29:52,684 - INFO - Epoch [4/10], Batch [540/747], Loss: 0.4380\n",
      "2025-04-04 17:29:53,315 - INFO - Epoch [4/10], Batch [550/747], Loss: 0.4170\n",
      "2025-04-04 17:29:53,944 - INFO - Epoch [4/10], Batch [560/747], Loss: 0.6537\n",
      "2025-04-04 17:29:54,574 - INFO - Epoch [4/10], Batch [570/747], Loss: 0.3536\n",
      "2025-04-04 17:29:55,204 - INFO - Epoch [4/10], Batch [580/747], Loss: 0.3412\n",
      "2025-04-04 17:29:55,834 - INFO - Epoch [4/10], Batch [590/747], Loss: 0.3707\n",
      "2025-04-04 17:29:56,465 - INFO - Epoch [4/10], Batch [600/747], Loss: 0.2827\n",
      "2025-04-04 17:29:57,094 - INFO - Epoch [4/10], Batch [610/747], Loss: 0.3267\n",
      "2025-04-04 17:29:57,724 - INFO - Epoch [4/10], Batch [620/747], Loss: 0.3754\n",
      "2025-04-04 17:29:58,354 - INFO - Epoch [4/10], Batch [630/747], Loss: 0.5928\n",
      "2025-04-04 17:29:58,983 - INFO - Epoch [4/10], Batch [640/747], Loss: 0.4620\n",
      "2025-04-04 17:29:59,613 - INFO - Epoch [4/10], Batch [650/747], Loss: 0.5080\n",
      "2025-04-04 17:30:00,244 - INFO - Epoch [4/10], Batch [660/747], Loss: 0.3963\n",
      "2025-04-04 17:30:00,875 - INFO - Epoch [4/10], Batch [670/747], Loss: 0.5754\n",
      "2025-04-04 17:30:01,505 - INFO - Epoch [4/10], Batch [680/747], Loss: 0.4533\n",
      "2025-04-04 17:30:02,134 - INFO - Epoch [4/10], Batch [690/747], Loss: 0.5033\n",
      "2025-04-04 17:30:02,764 - INFO - Epoch [4/10], Batch [700/747], Loss: 0.4807\n",
      "2025-04-04 17:30:03,393 - INFO - Epoch [4/10], Batch [710/747], Loss: 0.7182\n",
      "2025-04-04 17:30:04,023 - INFO - Epoch [4/10], Batch [720/747], Loss: 0.3246\n",
      "2025-04-04 17:30:04,652 - INFO - Epoch [4/10], Batch [730/747], Loss: 0.3435\n",
      "2025-04-04 17:30:05,283 - INFO - Epoch [4/10], Batch [740/747], Loss: 0.5200\n",
      "2025-04-04 17:30:05,691 - INFO - Epoch 4/10 Train Loss: 0.4873, Train Accuracy: 76.33%\n",
      "2025-04-04 17:30:09,406 - INFO - Epoch 4/10 Val Loss: 0.4093, Val Accuracy: 81.85%, AUC-ROC: 0.8873\n",
      "2025-04-04 17:30:09,409 - INFO - New best model at epoch 4 with Val Accuracy: 81.85%\n",
      "2025-04-04 17:30:09,903 - INFO - Epoch [5/10], Batch [0/747], Loss: 0.4769\n",
      "2025-04-04 17:30:10,533 - INFO - Epoch [5/10], Batch [10/747], Loss: 0.4854\n",
      "2025-04-04 17:30:11,163 - INFO - Epoch [5/10], Batch [20/747], Loss: 0.3157\n",
      "2025-04-04 17:30:11,793 - INFO - Epoch [5/10], Batch [30/747], Loss: 0.2914\n",
      "2025-04-04 17:30:12,423 - INFO - Epoch [5/10], Batch [40/747], Loss: 0.6138\n",
      "2025-04-04 17:30:13,053 - INFO - Epoch [5/10], Batch [50/747], Loss: 0.4790\n",
      "2025-04-04 17:30:13,683 - INFO - Epoch [5/10], Batch [60/747], Loss: 0.5634\n",
      "2025-04-04 17:30:14,313 - INFO - Epoch [5/10], Batch [70/747], Loss: 0.4417\n",
      "2025-04-04 17:30:14,943 - INFO - Epoch [5/10], Batch [80/747], Loss: 0.4766\n",
      "2025-04-04 17:30:15,574 - INFO - Epoch [5/10], Batch [90/747], Loss: 0.4318\n",
      "2025-04-04 17:30:16,204 - INFO - Epoch [5/10], Batch [100/747], Loss: 0.4461\n",
      "2025-04-04 17:30:16,834 - INFO - Epoch [5/10], Batch [110/747], Loss: 0.4361\n",
      "2025-04-04 17:30:17,464 - INFO - Epoch [5/10], Batch [120/747], Loss: 0.3814\n",
      "2025-04-04 17:30:18,094 - INFO - Epoch [5/10], Batch [130/747], Loss: 0.4133\n",
      "2025-04-04 17:30:18,725 - INFO - Epoch [5/10], Batch [140/747], Loss: 0.6657\n",
      "2025-04-04 17:30:19,355 - INFO - Epoch [5/10], Batch [150/747], Loss: 0.2780\n",
      "2025-04-04 17:30:19,987 - INFO - Epoch [5/10], Batch [160/747], Loss: 0.5909\n",
      "2025-04-04 17:30:20,618 - INFO - Epoch [5/10], Batch [170/747], Loss: 0.3661\n",
      "2025-04-04 17:30:21,248 - INFO - Epoch [5/10], Batch [180/747], Loss: 0.5486\n",
      "2025-04-04 17:30:21,878 - INFO - Epoch [5/10], Batch [190/747], Loss: 0.4977\n",
      "2025-04-04 17:30:22,509 - INFO - Epoch [5/10], Batch [200/747], Loss: 0.5212\n",
      "2025-04-04 17:30:23,138 - INFO - Epoch [5/10], Batch [210/747], Loss: 0.4189\n",
      "2025-04-04 17:30:23,767 - INFO - Epoch [5/10], Batch [220/747], Loss: 0.4553\n",
      "2025-04-04 17:30:24,397 - INFO - Epoch [5/10], Batch [230/747], Loss: 0.3519\n",
      "2025-04-04 17:30:25,027 - INFO - Epoch [5/10], Batch [240/747], Loss: 0.4507\n",
      "2025-04-04 17:30:25,658 - INFO - Epoch [5/10], Batch [250/747], Loss: 0.5833\n",
      "2025-04-04 17:30:26,290 - INFO - Epoch [5/10], Batch [260/747], Loss: 0.2748\n",
      "2025-04-04 17:30:26,920 - INFO - Epoch [5/10], Batch [270/747], Loss: 0.3609\n",
      "2025-04-04 17:30:27,552 - INFO - Epoch [5/10], Batch [280/747], Loss: 0.3760\n",
      "2025-04-04 17:30:28,182 - INFO - Epoch [5/10], Batch [290/747], Loss: 0.3644\n",
      "2025-04-04 17:30:28,812 - INFO - Epoch [5/10], Batch [300/747], Loss: 0.4319\n",
      "2025-04-04 17:30:29,442 - INFO - Epoch [5/10], Batch [310/747], Loss: 0.4852\n",
      "2025-04-04 17:30:30,072 - INFO - Epoch [5/10], Batch [320/747], Loss: 0.6985\n",
      "2025-04-04 17:30:30,702 - INFO - Epoch [5/10], Batch [330/747], Loss: 0.3558\n",
      "2025-04-04 17:30:31,333 - INFO - Epoch [5/10], Batch [340/747], Loss: 0.5282\n",
      "2025-04-04 17:30:31,963 - INFO - Epoch [5/10], Batch [350/747], Loss: 0.4134\n",
      "2025-04-04 17:30:32,593 - INFO - Epoch [5/10], Batch [360/747], Loss: 0.4013\n",
      "2025-04-04 17:30:33,224 - INFO - Epoch [5/10], Batch [370/747], Loss: 0.3355\n",
      "2025-04-04 17:30:33,854 - INFO - Epoch [5/10], Batch [380/747], Loss: 0.3900\n",
      "2025-04-04 17:30:34,484 - INFO - Epoch [5/10], Batch [390/747], Loss: 0.4251\n",
      "2025-04-04 17:30:35,115 - INFO - Epoch [5/10], Batch [400/747], Loss: 0.5296\n",
      "2025-04-04 17:30:35,745 - INFO - Epoch [5/10], Batch [410/747], Loss: 0.4584\n",
      "2025-04-04 17:30:36,376 - INFO - Epoch [5/10], Batch [420/747], Loss: 0.4194\n",
      "2025-04-04 17:30:37,006 - INFO - Epoch [5/10], Batch [430/747], Loss: 0.3425\n",
      "2025-04-04 17:30:37,637 - INFO - Epoch [5/10], Batch [440/747], Loss: 0.3678\n",
      "2025-04-04 17:30:38,267 - INFO - Epoch [5/10], Batch [450/747], Loss: 0.3263\n",
      "2025-04-04 17:30:38,896 - INFO - Epoch [5/10], Batch [460/747], Loss: 0.4732\n",
      "2025-04-04 17:30:39,527 - INFO - Epoch [5/10], Batch [470/747], Loss: 0.6138\n",
      "2025-04-04 17:30:40,158 - INFO - Epoch [5/10], Batch [480/747], Loss: 0.2456\n",
      "2025-04-04 17:30:40,789 - INFO - Epoch [5/10], Batch [490/747], Loss: 0.3252\n",
      "2025-04-04 17:30:41,419 - INFO - Epoch [5/10], Batch [500/747], Loss: 0.5063\n",
      "2025-04-04 17:30:42,050 - INFO - Epoch [5/10], Batch [510/747], Loss: 0.5715\n",
      "2025-04-04 17:30:42,681 - INFO - Epoch [5/10], Batch [520/747], Loss: 0.6838\n",
      "2025-04-04 17:30:43,311 - INFO - Epoch [5/10], Batch [530/747], Loss: 0.4408\n",
      "2025-04-04 17:30:43,940 - INFO - Epoch [5/10], Batch [540/747], Loss: 0.3261\n",
      "2025-04-04 17:30:44,570 - INFO - Epoch [5/10], Batch [550/747], Loss: 0.4428\n",
      "2025-04-04 17:30:45,200 - INFO - Epoch [5/10], Batch [560/747], Loss: 0.4415\n",
      "2025-04-04 17:30:45,829 - INFO - Epoch [5/10], Batch [570/747], Loss: 0.3611\n",
      "2025-04-04 17:30:46,458 - INFO - Epoch [5/10], Batch [580/747], Loss: 0.3785\n",
      "2025-04-04 17:30:47,088 - INFO - Epoch [5/10], Batch [590/747], Loss: 0.4172\n",
      "2025-04-04 17:30:47,718 - INFO - Epoch [5/10], Batch [600/747], Loss: 0.5633\n",
      "2025-04-04 17:30:48,348 - INFO - Epoch [5/10], Batch [610/747], Loss: 0.4205\n",
      "2025-04-04 17:30:48,978 - INFO - Epoch [5/10], Batch [620/747], Loss: 0.4014\n",
      "2025-04-04 17:30:49,608 - INFO - Epoch [5/10], Batch [630/747], Loss: 0.3278\n",
      "2025-04-04 17:30:50,238 - INFO - Epoch [5/10], Batch [640/747], Loss: 0.4704\n",
      "2025-04-04 17:30:50,868 - INFO - Epoch [5/10], Batch [650/747], Loss: 0.2923\n",
      "2025-04-04 17:30:51,498 - INFO - Epoch [5/10], Batch [660/747], Loss: 0.4062\n",
      "2025-04-04 17:30:52,128 - INFO - Epoch [5/10], Batch [670/747], Loss: 0.5082\n",
      "2025-04-04 17:30:52,759 - INFO - Epoch [5/10], Batch [680/747], Loss: 0.3540\n",
      "2025-04-04 17:30:53,390 - INFO - Epoch [5/10], Batch [690/747], Loss: 0.4682\n",
      "2025-04-04 17:30:54,021 - INFO - Epoch [5/10], Batch [700/747], Loss: 0.3750\n",
      "2025-04-04 17:30:54,651 - INFO - Epoch [5/10], Batch [710/747], Loss: 0.5851\n",
      "2025-04-04 17:30:55,281 - INFO - Epoch [5/10], Batch [720/747], Loss: 0.3576\n",
      "2025-04-04 17:30:55,913 - INFO - Epoch [5/10], Batch [730/747], Loss: 0.3017\n",
      "2025-04-04 17:30:56,544 - INFO - Epoch [5/10], Batch [740/747], Loss: 0.3272\n",
      "2025-04-04 17:30:56,951 - INFO - Epoch 5/10 Train Loss: 0.4343, Train Accuracy: 79.76%\n",
      "2025-04-04 17:31:00,664 - INFO - Epoch 5/10 Val Loss: 0.3858, Val Accuracy: 84.37%, AUC-ROC: 0.9062\n",
      "2025-04-04 17:31:00,667 - INFO - New best model at epoch 5 with Val Accuracy: 84.37%\n",
      "2025-04-04 17:31:01,160 - INFO - Epoch [6/10], Batch [0/747], Loss: 0.3876\n",
      "2025-04-04 17:31:01,790 - INFO - Epoch [6/10], Batch [10/747], Loss: 0.4216\n",
      "2025-04-04 17:31:02,420 - INFO - Epoch [6/10], Batch [20/747], Loss: 0.2760\n",
      "2025-04-04 17:31:03,051 - INFO - Epoch [6/10], Batch [30/747], Loss: 0.6726\n",
      "2025-04-04 17:31:03,682 - INFO - Epoch [6/10], Batch [40/747], Loss: 0.2862\n",
      "2025-04-04 17:31:04,312 - INFO - Epoch [6/10], Batch [50/747], Loss: 0.5116\n",
      "2025-04-04 17:31:04,942 - INFO - Epoch [6/10], Batch [60/747], Loss: 0.2343\n",
      "2025-04-04 17:31:05,571 - INFO - Epoch [6/10], Batch [70/747], Loss: 0.3696\n",
      "2025-04-04 17:31:06,202 - INFO - Epoch [6/10], Batch [80/747], Loss: 0.5696\n",
      "2025-04-04 17:31:06,831 - INFO - Epoch [6/10], Batch [90/747], Loss: 0.4072\n",
      "2025-04-04 17:31:07,463 - INFO - Epoch [6/10], Batch [100/747], Loss: 0.3839\n",
      "2025-04-04 17:31:08,093 - INFO - Epoch [6/10], Batch [110/747], Loss: 0.4044\n",
      "2025-04-04 17:31:08,724 - INFO - Epoch [6/10], Batch [120/747], Loss: 0.5560\n",
      "2025-04-04 17:31:09,354 - INFO - Epoch [6/10], Batch [130/747], Loss: 0.2985\n",
      "2025-04-04 17:31:09,985 - INFO - Epoch [6/10], Batch [140/747], Loss: 0.4173\n",
      "2025-04-04 17:31:10,616 - INFO - Epoch [6/10], Batch [150/747], Loss: 0.4779\n",
      "2025-04-04 17:31:11,247 - INFO - Epoch [6/10], Batch [160/747], Loss: 0.2915\n",
      "2025-04-04 17:31:11,878 - INFO - Epoch [6/10], Batch [170/747], Loss: 0.4459\n",
      "2025-04-04 17:31:12,508 - INFO - Epoch [6/10], Batch [180/747], Loss: 0.2023\n",
      "2025-04-04 17:31:13,139 - INFO - Epoch [6/10], Batch [190/747], Loss: 0.5155\n",
      "2025-04-04 17:31:13,769 - INFO - Epoch [6/10], Batch [200/747], Loss: 0.4623\n",
      "2025-04-04 17:31:14,398 - INFO - Epoch [6/10], Batch [210/747], Loss: 0.3559\n",
      "2025-04-04 17:31:15,029 - INFO - Epoch [6/10], Batch [220/747], Loss: 0.2918\n",
      "2025-04-04 17:31:15,660 - INFO - Epoch [6/10], Batch [230/747], Loss: 0.3831\n",
      "2025-04-04 17:31:16,291 - INFO - Epoch [6/10], Batch [240/747], Loss: 0.4353\n",
      "2025-04-04 17:31:16,921 - INFO - Epoch [6/10], Batch [250/747], Loss: 0.6269\n",
      "2025-04-04 17:31:17,551 - INFO - Epoch [6/10], Batch [260/747], Loss: 0.2644\n",
      "2025-04-04 17:31:18,183 - INFO - Epoch [6/10], Batch [270/747], Loss: 0.3949\n",
      "2025-04-04 17:31:18,814 - INFO - Epoch [6/10], Batch [280/747], Loss: 0.3495\n",
      "2025-04-04 17:31:19,444 - INFO - Epoch [6/10], Batch [290/747], Loss: 0.5010\n",
      "2025-04-04 17:31:20,074 - INFO - Epoch [6/10], Batch [300/747], Loss: 0.3793\n",
      "2025-04-04 17:31:20,704 - INFO - Epoch [6/10], Batch [310/747], Loss: 0.5464\n",
      "2025-04-04 17:31:21,334 - INFO - Epoch [6/10], Batch [320/747], Loss: 0.2876\n",
      "2025-04-04 17:31:21,964 - INFO - Epoch [6/10], Batch [330/747], Loss: 0.3988\n",
      "2025-04-04 17:31:22,595 - INFO - Epoch [6/10], Batch [340/747], Loss: 0.5612\n",
      "2025-04-04 17:31:23,225 - INFO - Epoch [6/10], Batch [350/747], Loss: 0.4615\n",
      "2025-04-04 17:31:23,855 - INFO - Epoch [6/10], Batch [360/747], Loss: 0.3683\n",
      "2025-04-04 17:31:24,485 - INFO - Epoch [6/10], Batch [370/747], Loss: 0.2800\n",
      "2025-04-04 17:31:25,115 - INFO - Epoch [6/10], Batch [380/747], Loss: 0.4600\n",
      "2025-04-04 17:31:25,746 - INFO - Epoch [6/10], Batch [390/747], Loss: 0.3631\n",
      "2025-04-04 17:31:26,377 - INFO - Epoch [6/10], Batch [400/747], Loss: 0.4906\n",
      "2025-04-04 17:31:27,007 - INFO - Epoch [6/10], Batch [410/747], Loss: 0.3994\n",
      "2025-04-04 17:31:27,638 - INFO - Epoch [6/10], Batch [420/747], Loss: 0.2954\n",
      "2025-04-04 17:31:28,268 - INFO - Epoch [6/10], Batch [430/747], Loss: 0.4251\n",
      "2025-04-04 17:31:28,897 - INFO - Epoch [6/10], Batch [440/747], Loss: 0.3511\n",
      "2025-04-04 17:31:29,527 - INFO - Epoch [6/10], Batch [450/747], Loss: 0.3250\n",
      "2025-04-04 17:31:30,157 - INFO - Epoch [6/10], Batch [460/747], Loss: 0.6235\n",
      "2025-04-04 17:31:30,787 - INFO - Epoch [6/10], Batch [470/747], Loss: 0.4831\n",
      "2025-04-04 17:31:31,416 - INFO - Epoch [6/10], Batch [480/747], Loss: 0.3407\n",
      "2025-04-04 17:31:32,046 - INFO - Epoch [6/10], Batch [490/747], Loss: 0.5008\n",
      "2025-04-04 17:31:32,676 - INFO - Epoch [6/10], Batch [500/747], Loss: 0.3819\n",
      "2025-04-04 17:31:33,306 - INFO - Epoch [6/10], Batch [510/747], Loss: 0.4911\n",
      "2025-04-04 17:31:33,937 - INFO - Epoch [6/10], Batch [520/747], Loss: 0.4534\n",
      "2025-04-04 17:31:34,566 - INFO - Epoch [6/10], Batch [530/747], Loss: 0.5403\n",
      "2025-04-04 17:31:35,197 - INFO - Epoch [6/10], Batch [540/747], Loss: 0.3891\n",
      "2025-04-04 17:31:35,828 - INFO - Epoch [6/10], Batch [550/747], Loss: 0.2485\n",
      "2025-04-04 17:31:36,458 - INFO - Epoch [6/10], Batch [560/747], Loss: 0.2297\n",
      "2025-04-04 17:31:37,088 - INFO - Epoch [6/10], Batch [570/747], Loss: 0.2588\n",
      "2025-04-04 17:31:37,718 - INFO - Epoch [6/10], Batch [580/747], Loss: 0.4190\n",
      "2025-04-04 17:31:38,349 - INFO - Epoch [6/10], Batch [590/747], Loss: 0.5464\n",
      "2025-04-04 17:31:38,979 - INFO - Epoch [6/10], Batch [600/747], Loss: 0.2592\n",
      "2025-04-04 17:31:39,610 - INFO - Epoch [6/10], Batch [610/747], Loss: 0.3269\n",
      "2025-04-04 17:31:40,241 - INFO - Epoch [6/10], Batch [620/747], Loss: 0.5138\n",
      "2025-04-04 17:31:40,871 - INFO - Epoch [6/10], Batch [630/747], Loss: 0.3141\n",
      "2025-04-04 17:31:41,501 - INFO - Epoch [6/10], Batch [640/747], Loss: 0.4319\n",
      "2025-04-04 17:31:42,132 - INFO - Epoch [6/10], Batch [650/747], Loss: 0.4128\n",
      "2025-04-04 17:31:42,762 - INFO - Epoch [6/10], Batch [660/747], Loss: 0.3238\n",
      "2025-04-04 17:31:43,393 - INFO - Epoch [6/10], Batch [670/747], Loss: 0.2470\n",
      "2025-04-04 17:31:44,024 - INFO - Epoch [6/10], Batch [680/747], Loss: 0.4029\n",
      "2025-04-04 17:31:44,655 - INFO - Epoch [6/10], Batch [690/747], Loss: 0.2704\n",
      "2025-04-04 17:31:45,285 - INFO - Epoch [6/10], Batch [700/747], Loss: 0.5640\n",
      "2025-04-04 17:31:45,915 - INFO - Epoch [6/10], Batch [710/747], Loss: 0.5211\n",
      "2025-04-04 17:31:46,545 - INFO - Epoch [6/10], Batch [720/747], Loss: 0.4322\n",
      "2025-04-04 17:31:47,177 - INFO - Epoch [6/10], Batch [730/747], Loss: 0.6698\n",
      "2025-04-04 17:31:47,808 - INFO - Epoch [6/10], Batch [740/747], Loss: 0.5667\n",
      "2025-04-04 17:31:48,213 - INFO - Epoch 6/10 Train Loss: 0.4060, Train Accuracy: 81.05%\n",
      "2025-04-04 17:31:51,923 - INFO - Epoch 6/10 Val Loss: 0.3374, Val Accuracy: 85.59%, AUC-ROC: 0.9212\n",
      "2025-04-04 17:31:51,927 - INFO - New best model at epoch 6 with Val Accuracy: 85.59%\n",
      "2025-04-04 17:31:52,422 - INFO - Epoch [7/10], Batch [0/747], Loss: 0.2530\n",
      "2025-04-04 17:31:53,053 - INFO - Epoch [7/10], Batch [10/747], Loss: 0.3063\n",
      "2025-04-04 17:31:53,684 - INFO - Epoch [7/10], Batch [20/747], Loss: 0.4242\n",
      "2025-04-04 17:31:54,314 - INFO - Epoch [7/10], Batch [30/747], Loss: 0.4055\n",
      "2025-04-04 17:31:54,944 - INFO - Epoch [7/10], Batch [40/747], Loss: 0.2855\n",
      "2025-04-04 17:31:55,575 - INFO - Epoch [7/10], Batch [50/747], Loss: 0.3079\n",
      "2025-04-04 17:31:56,207 - INFO - Epoch [7/10], Batch [60/747], Loss: 0.3739\n",
      "2025-04-04 17:31:56,840 - INFO - Epoch [7/10], Batch [70/747], Loss: 0.2373\n",
      "2025-04-04 17:31:57,472 - INFO - Epoch [7/10], Batch [80/747], Loss: 0.4449\n",
      "2025-04-04 17:31:58,103 - INFO - Epoch [7/10], Batch [90/747], Loss: 0.3352\n",
      "2025-04-04 17:31:58,734 - INFO - Epoch [7/10], Batch [100/747], Loss: 0.3091\n",
      "2025-04-04 17:31:59,365 - INFO - Epoch [7/10], Batch [110/747], Loss: 0.2719\n",
      "2025-04-04 17:31:59,997 - INFO - Epoch [7/10], Batch [120/747], Loss: 0.3900\n",
      "2025-04-04 17:32:00,629 - INFO - Epoch [7/10], Batch [130/747], Loss: 0.4448\n",
      "2025-04-04 17:32:01,262 - INFO - Epoch [7/10], Batch [140/747], Loss: 0.4315\n",
      "2025-04-04 17:32:01,893 - INFO - Epoch [7/10], Batch [150/747], Loss: 0.4758\n",
      "2025-04-04 17:32:02,524 - INFO - Epoch [7/10], Batch [160/747], Loss: 0.4309\n",
      "2025-04-04 17:32:03,155 - INFO - Epoch [7/10], Batch [170/747], Loss: 0.2740\n",
      "2025-04-04 17:32:03,785 - INFO - Epoch [7/10], Batch [180/747], Loss: 0.4297\n",
      "2025-04-04 17:32:04,416 - INFO - Epoch [7/10], Batch [190/747], Loss: 0.5227\n",
      "2025-04-04 17:32:05,047 - INFO - Epoch [7/10], Batch [200/747], Loss: 0.4902\n",
      "2025-04-04 17:32:05,679 - INFO - Epoch [7/10], Batch [210/747], Loss: 0.2858\n",
      "2025-04-04 17:32:06,310 - INFO - Epoch [7/10], Batch [220/747], Loss: 0.4510\n",
      "2025-04-04 17:32:06,941 - INFO - Epoch [7/10], Batch [230/747], Loss: 0.3159\n",
      "2025-04-04 17:32:07,572 - INFO - Epoch [7/10], Batch [240/747], Loss: 0.2653\n",
      "2025-04-04 17:32:08,203 - INFO - Epoch [7/10], Batch [250/747], Loss: 0.4732\n",
      "2025-04-04 17:32:08,834 - INFO - Epoch [7/10], Batch [260/747], Loss: 0.4709\n",
      "2025-04-04 17:32:09,465 - INFO - Epoch [7/10], Batch [270/747], Loss: 0.3195\n",
      "2025-04-04 17:32:10,098 - INFO - Epoch [7/10], Batch [280/747], Loss: 0.3460\n",
      "2025-04-04 17:32:10,729 - INFO - Epoch [7/10], Batch [290/747], Loss: 0.3215\n",
      "2025-04-04 17:32:11,359 - INFO - Epoch [7/10], Batch [300/747], Loss: 0.3094\n",
      "2025-04-04 17:32:11,992 - INFO - Epoch [7/10], Batch [310/747], Loss: 0.6705\n",
      "2025-04-04 17:32:12,621 - INFO - Epoch [7/10], Batch [320/747], Loss: 0.3154\n",
      "2025-04-04 17:32:13,254 - INFO - Epoch [7/10], Batch [330/747], Loss: 0.3076\n",
      "2025-04-04 17:32:13,885 - INFO - Epoch [7/10], Batch [340/747], Loss: 0.2671\n",
      "2025-04-04 17:32:14,516 - INFO - Epoch [7/10], Batch [350/747], Loss: 0.2924\n",
      "2025-04-04 17:32:15,148 - INFO - Epoch [7/10], Batch [360/747], Loss: 0.3023\n",
      "2025-04-04 17:32:15,779 - INFO - Epoch [7/10], Batch [370/747], Loss: 0.4734\n",
      "2025-04-04 17:32:16,411 - INFO - Epoch [7/10], Batch [380/747], Loss: 0.2786\n",
      "2025-04-04 17:32:17,042 - INFO - Epoch [7/10], Batch [390/747], Loss: 0.3445\n",
      "2025-04-04 17:32:17,674 - INFO - Epoch [7/10], Batch [400/747], Loss: 0.4151\n",
      "2025-04-04 17:32:18,304 - INFO - Epoch [7/10], Batch [410/747], Loss: 0.2715\n",
      "2025-04-04 17:32:18,936 - INFO - Epoch [7/10], Batch [420/747], Loss: 0.3479\n",
      "2025-04-04 17:32:19,568 - INFO - Epoch [7/10], Batch [430/747], Loss: 0.2533\n",
      "2025-04-04 17:32:20,199 - INFO - Epoch [7/10], Batch [440/747], Loss: 0.3923\n",
      "2025-04-04 17:32:20,828 - INFO - Epoch [7/10], Batch [450/747], Loss: 0.2728\n",
      "2025-04-04 17:32:21,458 - INFO - Epoch [7/10], Batch [460/747], Loss: 0.3670\n",
      "2025-04-04 17:32:22,090 - INFO - Epoch [7/10], Batch [470/747], Loss: 0.3838\n",
      "2025-04-04 17:32:22,721 - INFO - Epoch [7/10], Batch [480/747], Loss: 0.4125\n",
      "2025-04-04 17:32:23,352 - INFO - Epoch [7/10], Batch [490/747], Loss: 0.3848\n",
      "2025-04-04 17:32:23,982 - INFO - Epoch [7/10], Batch [500/747], Loss: 0.4874\n",
      "2025-04-04 17:32:24,614 - INFO - Epoch [7/10], Batch [510/747], Loss: 0.3173\n",
      "2025-04-04 17:32:25,245 - INFO - Epoch [7/10], Batch [520/747], Loss: 0.2273\n",
      "2025-04-04 17:32:25,878 - INFO - Epoch [7/10], Batch [530/747], Loss: 0.2213\n",
      "2025-04-04 17:32:26,511 - INFO - Epoch [7/10], Batch [540/747], Loss: 0.4913\n",
      "2025-04-04 17:32:27,143 - INFO - Epoch [7/10], Batch [550/747], Loss: 0.3651\n",
      "2025-04-04 17:32:27,774 - INFO - Epoch [7/10], Batch [560/747], Loss: 0.3802\n",
      "2025-04-04 17:32:28,405 - INFO - Epoch [7/10], Batch [570/747], Loss: 0.3000\n",
      "2025-04-04 17:32:29,035 - INFO - Epoch [7/10], Batch [580/747], Loss: 0.3880\n",
      "2025-04-04 17:32:29,665 - INFO - Epoch [7/10], Batch [590/747], Loss: 0.5180\n",
      "2025-04-04 17:32:30,296 - INFO - Epoch [7/10], Batch [600/747], Loss: 0.5169\n",
      "2025-04-04 17:32:30,927 - INFO - Epoch [7/10], Batch [610/747], Loss: 0.2794\n",
      "2025-04-04 17:32:31,558 - INFO - Epoch [7/10], Batch [620/747], Loss: 0.5248\n",
      "2025-04-04 17:32:32,188 - INFO - Epoch [7/10], Batch [630/747], Loss: 0.2846\n",
      "2025-04-04 17:32:32,821 - INFO - Epoch [7/10], Batch [640/747], Loss: 0.3613\n",
      "2025-04-04 17:32:33,453 - INFO - Epoch [7/10], Batch [650/747], Loss: 0.3921\n",
      "2025-04-04 17:32:34,083 - INFO - Epoch [7/10], Batch [660/747], Loss: 0.3624\n",
      "2025-04-04 17:32:34,714 - INFO - Epoch [7/10], Batch [670/747], Loss: 0.2296\n",
      "2025-04-04 17:32:35,345 - INFO - Epoch [7/10], Batch [680/747], Loss: 0.2775\n",
      "2025-04-04 17:32:35,978 - INFO - Epoch [7/10], Batch [690/747], Loss: 0.3102\n",
      "2025-04-04 17:32:36,610 - INFO - Epoch [7/10], Batch [700/747], Loss: 0.2974\n",
      "2025-04-04 17:32:37,243 - INFO - Epoch [7/10], Batch [710/747], Loss: 0.4102\n",
      "2025-04-04 17:32:37,874 - INFO - Epoch [7/10], Batch [720/747], Loss: 0.5145\n",
      "2025-04-04 17:32:38,506 - INFO - Epoch [7/10], Batch [730/747], Loss: 0.2735\n",
      "2025-04-04 17:32:39,139 - INFO - Epoch [7/10], Batch [740/747], Loss: 0.4533\n",
      "2025-04-04 17:32:39,559 - INFO - Epoch 7/10 Train Loss: 0.3720, Train Accuracy: 82.78%\n",
      "2025-04-04 17:32:43,304 - INFO - Epoch 7/10 Val Loss: 0.3110, Val Accuracy: 86.93%, AUC-ROC: 0.9333\n",
      "2025-04-04 17:32:43,307 - INFO - New best model at epoch 7 with Val Accuracy: 86.93%\n",
      "2025-04-04 17:32:43,772 - INFO - Epoch [8/10], Batch [0/747], Loss: 0.2312\n",
      "2025-04-04 17:32:44,403 - INFO - Epoch [8/10], Batch [10/747], Loss: 0.2523\n",
      "2025-04-04 17:32:45,033 - INFO - Epoch [8/10], Batch [20/747], Loss: 0.2350\n",
      "2025-04-04 17:32:45,663 - INFO - Epoch [8/10], Batch [30/747], Loss: 0.3340\n",
      "2025-04-04 17:32:46,294 - INFO - Epoch [8/10], Batch [40/747], Loss: 0.3449\n",
      "2025-04-04 17:32:46,925 - INFO - Epoch [8/10], Batch [50/747], Loss: 0.3612\n",
      "2025-04-04 17:32:47,556 - INFO - Epoch [8/10], Batch [60/747], Loss: 0.4914\n",
      "2025-04-04 17:32:48,186 - INFO - Epoch [8/10], Batch [70/747], Loss: 0.3684\n",
      "2025-04-04 17:32:48,817 - INFO - Epoch [8/10], Batch [80/747], Loss: 0.2663\n",
      "2025-04-04 17:32:49,447 - INFO - Epoch [8/10], Batch [90/747], Loss: 0.4078\n",
      "2025-04-04 17:32:50,077 - INFO - Epoch [8/10], Batch [100/747], Loss: 0.4269\n",
      "2025-04-04 17:32:50,708 - INFO - Epoch [8/10], Batch [110/747], Loss: 0.6483\n",
      "2025-04-04 17:32:51,338 - INFO - Epoch [8/10], Batch [120/747], Loss: 0.4552\n",
      "2025-04-04 17:32:51,969 - INFO - Epoch [8/10], Batch [130/747], Loss: 0.3410\n",
      "2025-04-04 17:32:52,598 - INFO - Epoch [8/10], Batch [140/747], Loss: 0.3282\n",
      "2025-04-04 17:32:53,229 - INFO - Epoch [8/10], Batch [150/747], Loss: 0.3102\n",
      "2025-04-04 17:32:53,860 - INFO - Epoch [8/10], Batch [160/747], Loss: 0.3955\n",
      "2025-04-04 17:32:54,491 - INFO - Epoch [8/10], Batch [170/747], Loss: 0.3516\n",
      "2025-04-04 17:32:55,122 - INFO - Epoch [8/10], Batch [180/747], Loss: 0.5281\n",
      "2025-04-04 17:32:55,752 - INFO - Epoch [8/10], Batch [190/747], Loss: 0.2768\n",
      "2025-04-04 17:32:56,382 - INFO - Epoch [8/10], Batch [200/747], Loss: 0.2762\n",
      "2025-04-04 17:32:57,013 - INFO - Epoch [8/10], Batch [210/747], Loss: 0.5016\n",
      "2025-04-04 17:32:57,645 - INFO - Epoch [8/10], Batch [220/747], Loss: 0.3129\n",
      "2025-04-04 17:32:58,276 - INFO - Epoch [8/10], Batch [230/747], Loss: 0.4705\n",
      "2025-04-04 17:32:58,905 - INFO - Epoch [8/10], Batch [240/747], Loss: 0.4075\n",
      "2025-04-04 17:32:59,536 - INFO - Epoch [8/10], Batch [250/747], Loss: 0.4100\n",
      "2025-04-04 17:33:00,168 - INFO - Epoch [8/10], Batch [260/747], Loss: 0.2515\n",
      "2025-04-04 17:33:00,798 - INFO - Epoch [8/10], Batch [270/747], Loss: 0.5454\n",
      "2025-04-04 17:33:01,429 - INFO - Epoch [8/10], Batch [280/747], Loss: 0.5747\n",
      "2025-04-04 17:33:02,060 - INFO - Epoch [8/10], Batch [290/747], Loss: 0.1942\n",
      "2025-04-04 17:33:02,689 - INFO - Epoch [8/10], Batch [300/747], Loss: 0.3534\n",
      "2025-04-04 17:33:03,319 - INFO - Epoch [8/10], Batch [310/747], Loss: 0.3353\n",
      "2025-04-04 17:33:03,950 - INFO - Epoch [8/10], Batch [320/747], Loss: 0.3963\n",
      "2025-04-04 17:33:04,581 - INFO - Epoch [8/10], Batch [330/747], Loss: 0.4199\n",
      "2025-04-04 17:33:05,210 - INFO - Epoch [8/10], Batch [340/747], Loss: 0.2608\n",
      "2025-04-04 17:33:05,840 - INFO - Epoch [8/10], Batch [350/747], Loss: 0.2889\n",
      "2025-04-04 17:33:06,472 - INFO - Epoch [8/10], Batch [360/747], Loss: 0.2408\n",
      "2025-04-04 17:33:07,103 - INFO - Epoch [8/10], Batch [370/747], Loss: 0.2830\n",
      "2025-04-04 17:33:07,734 - INFO - Epoch [8/10], Batch [380/747], Loss: 0.2946\n",
      "2025-04-04 17:33:08,364 - INFO - Epoch [8/10], Batch [390/747], Loss: 0.3561\n",
      "2025-04-04 17:33:08,994 - INFO - Epoch [8/10], Batch [400/747], Loss: 0.4880\n",
      "2025-04-04 17:33:09,624 - INFO - Epoch [8/10], Batch [410/747], Loss: 0.3777\n",
      "2025-04-04 17:33:10,253 - INFO - Epoch [8/10], Batch [420/747], Loss: 0.3642\n",
      "2025-04-04 17:33:10,884 - INFO - Epoch [8/10], Batch [430/747], Loss: 0.2921\n",
      "2025-04-04 17:33:11,514 - INFO - Epoch [8/10], Batch [440/747], Loss: 0.2611\n",
      "2025-04-04 17:33:12,145 - INFO - Epoch [8/10], Batch [450/747], Loss: 0.3444\n",
      "2025-04-04 17:33:12,775 - INFO - Epoch [8/10], Batch [460/747], Loss: 0.4256\n",
      "2025-04-04 17:33:13,405 - INFO - Epoch [8/10], Batch [470/747], Loss: 0.4040\n",
      "2025-04-04 17:33:14,034 - INFO - Epoch [8/10], Batch [480/747], Loss: 0.2334\n",
      "2025-04-04 17:33:14,665 - INFO - Epoch [8/10], Batch [490/747], Loss: 0.2872\n",
      "2025-04-04 17:33:15,296 - INFO - Epoch [8/10], Batch [500/747], Loss: 0.3099\n",
      "2025-04-04 17:33:15,927 - INFO - Epoch [8/10], Batch [510/747], Loss: 0.2756\n",
      "2025-04-04 17:33:16,558 - INFO - Epoch [8/10], Batch [520/747], Loss: 0.4287\n",
      "2025-04-04 17:33:17,189 - INFO - Epoch [8/10], Batch [530/747], Loss: 0.3308\n",
      "2025-04-04 17:33:17,820 - INFO - Epoch [8/10], Batch [540/747], Loss: 0.2987\n",
      "2025-04-04 17:33:18,450 - INFO - Epoch [8/10], Batch [550/747], Loss: 0.4366\n",
      "2025-04-04 17:33:19,080 - INFO - Epoch [8/10], Batch [560/747], Loss: 0.3224\n",
      "2025-04-04 17:33:19,710 - INFO - Epoch [8/10], Batch [570/747], Loss: 0.1615\n",
      "2025-04-04 17:33:20,340 - INFO - Epoch [8/10], Batch [580/747], Loss: 0.2494\n",
      "2025-04-04 17:33:20,969 - INFO - Epoch [8/10], Batch [590/747], Loss: 0.3338\n",
      "2025-04-04 17:33:21,600 - INFO - Epoch [8/10], Batch [600/747], Loss: 0.2886\n",
      "2025-04-04 17:33:22,230 - INFO - Epoch [8/10], Batch [610/747], Loss: 0.3004\n",
      "2025-04-04 17:33:22,860 - INFO - Epoch [8/10], Batch [620/747], Loss: 0.4024\n",
      "2025-04-04 17:33:23,489 - INFO - Epoch [8/10], Batch [630/747], Loss: 0.5070\n",
      "2025-04-04 17:33:24,120 - INFO - Epoch [8/10], Batch [640/747], Loss: 0.2854\n",
      "2025-04-04 17:33:24,750 - INFO - Epoch [8/10], Batch [650/747], Loss: 0.2873\n",
      "2025-04-04 17:33:25,381 - INFO - Epoch [8/10], Batch [660/747], Loss: 0.4956\n",
      "2025-04-04 17:33:26,012 - INFO - Epoch [8/10], Batch [670/747], Loss: 0.5760\n",
      "2025-04-04 17:33:26,643 - INFO - Epoch [8/10], Batch [680/747], Loss: 0.6263\n",
      "2025-04-04 17:33:27,274 - INFO - Epoch [8/10], Batch [690/747], Loss: 0.2280\n",
      "2025-04-04 17:33:27,905 - INFO - Epoch [8/10], Batch [700/747], Loss: 0.2021\n",
      "2025-04-04 17:33:28,535 - INFO - Epoch [8/10], Batch [710/747], Loss: 0.5055\n",
      "2025-04-04 17:33:29,165 - INFO - Epoch [8/10], Batch [720/747], Loss: 0.3972\n",
      "2025-04-04 17:33:29,795 - INFO - Epoch [8/10], Batch [730/747], Loss: 0.1770\n",
      "2025-04-04 17:33:30,426 - INFO - Epoch [8/10], Batch [740/747], Loss: 0.3708\n",
      "2025-04-04 17:33:30,831 - INFO - Epoch 8/10 Train Loss: 0.3575, Train Accuracy: 83.50%\n",
      "2025-04-04 17:33:34,554 - INFO - Epoch 8/10 Val Loss: 0.2854, Val Accuracy: 87.92%, AUC-ROC: 0.9423\n",
      "2025-04-04 17:33:34,558 - INFO - New best model at epoch 8 with Val Accuracy: 87.92%\n",
      "2025-04-04 17:33:35,041 - INFO - Epoch [9/10], Batch [0/747], Loss: 0.3800\n",
      "2025-04-04 17:33:35,671 - INFO - Epoch [9/10], Batch [10/747], Loss: 0.3359\n",
      "2025-04-04 17:33:36,301 - INFO - Epoch [9/10], Batch [20/747], Loss: 0.2795\n",
      "2025-04-04 17:33:36,932 - INFO - Epoch [9/10], Batch [30/747], Loss: 0.3166\n",
      "2025-04-04 17:33:37,562 - INFO - Epoch [9/10], Batch [40/747], Loss: 0.3366\n",
      "2025-04-04 17:33:38,192 - INFO - Epoch [9/10], Batch [50/747], Loss: 0.2378\n",
      "2025-04-04 17:33:38,823 - INFO - Epoch [9/10], Batch [60/747], Loss: 0.4348\n",
      "2025-04-04 17:33:39,454 - INFO - Epoch [9/10], Batch [70/747], Loss: 0.3897\n",
      "2025-04-04 17:33:40,083 - INFO - Epoch [9/10], Batch [80/747], Loss: 0.5025\n",
      "2025-04-04 17:33:40,713 - INFO - Epoch [9/10], Batch [90/747], Loss: 0.2101\n",
      "2025-04-04 17:33:41,343 - INFO - Epoch [9/10], Batch [100/747], Loss: 0.3514\n",
      "2025-04-04 17:33:41,973 - INFO - Epoch [9/10], Batch [110/747], Loss: 0.5482\n",
      "2025-04-04 17:33:42,602 - INFO - Epoch [9/10], Batch [120/747], Loss: 0.2871\n",
      "2025-04-04 17:33:43,231 - INFO - Epoch [9/10], Batch [130/747], Loss: 0.2567\n",
      "2025-04-04 17:33:43,861 - INFO - Epoch [9/10], Batch [140/747], Loss: 0.2612\n",
      "2025-04-04 17:33:44,491 - INFO - Epoch [9/10], Batch [150/747], Loss: 0.4169\n",
      "2025-04-04 17:33:45,121 - INFO - Epoch [9/10], Batch [160/747], Loss: 0.2940\n",
      "2025-04-04 17:33:45,751 - INFO - Epoch [9/10], Batch [170/747], Loss: 0.2841\n",
      "2025-04-04 17:33:46,381 - INFO - Epoch [9/10], Batch [180/747], Loss: 0.5060\n",
      "2025-04-04 17:33:47,011 - INFO - Epoch [9/10], Batch [190/747], Loss: 0.4602\n",
      "2025-04-04 17:33:47,641 - INFO - Epoch [9/10], Batch [200/747], Loss: 0.3763\n",
      "2025-04-04 17:33:48,272 - INFO - Epoch [9/10], Batch [210/747], Loss: 0.3508\n",
      "2025-04-04 17:33:48,903 - INFO - Epoch [9/10], Batch [220/747], Loss: 0.3586\n",
      "2025-04-04 17:33:49,532 - INFO - Epoch [9/10], Batch [230/747], Loss: 0.3804\n",
      "2025-04-04 17:33:50,162 - INFO - Epoch [9/10], Batch [240/747], Loss: 0.2797\n",
      "2025-04-04 17:33:50,791 - INFO - Epoch [9/10], Batch [250/747], Loss: 0.3982\n",
      "2025-04-04 17:33:51,421 - INFO - Epoch [9/10], Batch [260/747], Loss: 0.4020\n",
      "2025-04-04 17:33:52,052 - INFO - Epoch [9/10], Batch [270/747], Loss: 0.3621\n",
      "2025-04-04 17:33:52,681 - INFO - Epoch [9/10], Batch [280/747], Loss: 0.4117\n",
      "2025-04-04 17:33:53,311 - INFO - Epoch [9/10], Batch [290/747], Loss: 0.4812\n",
      "2025-04-04 17:33:53,941 - INFO - Epoch [9/10], Batch [300/747], Loss: 0.3244\n",
      "2025-04-04 17:33:54,570 - INFO - Epoch [9/10], Batch [310/747], Loss: 0.3922\n",
      "2025-04-04 17:33:55,199 - INFO - Epoch [9/10], Batch [320/747], Loss: 0.4266\n",
      "2025-04-04 17:33:55,829 - INFO - Epoch [9/10], Batch [330/747], Loss: 0.3345\n",
      "2025-04-04 17:33:56,459 - INFO - Epoch [9/10], Batch [340/747], Loss: 0.2987\n",
      "2025-04-04 17:33:57,089 - INFO - Epoch [9/10], Batch [350/747], Loss: 0.3223\n",
      "2025-04-04 17:33:57,720 - INFO - Epoch [9/10], Batch [360/747], Loss: 0.2211\n",
      "2025-04-04 17:33:58,350 - INFO - Epoch [9/10], Batch [370/747], Loss: 0.3020\n",
      "2025-04-04 17:33:58,980 - INFO - Epoch [9/10], Batch [380/747], Loss: 0.2764\n",
      "2025-04-04 17:33:59,610 - INFO - Epoch [9/10], Batch [390/747], Loss: 0.6747\n",
      "2025-04-04 17:34:00,239 - INFO - Epoch [9/10], Batch [400/747], Loss: 0.2429\n",
      "2025-04-04 17:34:00,869 - INFO - Epoch [9/10], Batch [410/747], Loss: 0.4011\n",
      "2025-04-04 17:34:01,499 - INFO - Epoch [9/10], Batch [420/747], Loss: 0.2203\n",
      "2025-04-04 17:34:02,129 - INFO - Epoch [9/10], Batch [430/747], Loss: 0.2302\n",
      "2025-04-04 17:34:02,758 - INFO - Epoch [9/10], Batch [440/747], Loss: 0.2103\n",
      "2025-04-04 17:34:03,389 - INFO - Epoch [9/10], Batch [450/747], Loss: 0.1664\n",
      "2025-04-04 17:34:04,018 - INFO - Epoch [9/10], Batch [460/747], Loss: 0.3939\n",
      "2025-04-04 17:34:04,648 - INFO - Epoch [9/10], Batch [470/747], Loss: 0.4459\n",
      "2025-04-04 17:34:05,279 - INFO - Epoch [9/10], Batch [480/747], Loss: 0.4123\n",
      "2025-04-04 17:34:05,908 - INFO - Epoch [9/10], Batch [490/747], Loss: 0.3293\n",
      "2025-04-04 17:34:06,538 - INFO - Epoch [9/10], Batch [500/747], Loss: 0.2804\n",
      "2025-04-04 17:34:07,167 - INFO - Epoch [9/10], Batch [510/747], Loss: 0.2819\n",
      "2025-04-04 17:34:07,797 - INFO - Epoch [9/10], Batch [520/747], Loss: 0.3546\n",
      "2025-04-04 17:34:08,428 - INFO - Epoch [9/10], Batch [530/747], Loss: 0.2066\n",
      "2025-04-04 17:34:09,058 - INFO - Epoch [9/10], Batch [540/747], Loss: 0.2920\n",
      "2025-04-04 17:34:09,690 - INFO - Epoch [9/10], Batch [550/747], Loss: 0.2794\n",
      "2025-04-04 17:34:10,320 - INFO - Epoch [9/10], Batch [560/747], Loss: 0.4791\n",
      "2025-04-04 17:34:10,950 - INFO - Epoch [9/10], Batch [570/747], Loss: 0.3700\n",
      "2025-04-04 17:34:11,579 - INFO - Epoch [9/10], Batch [580/747], Loss: 0.3952\n",
      "2025-04-04 17:34:12,209 - INFO - Epoch [9/10], Batch [590/747], Loss: 0.1925\n",
      "2025-04-04 17:34:12,840 - INFO - Epoch [9/10], Batch [600/747], Loss: 0.1421\n",
      "2025-04-04 17:34:13,470 - INFO - Epoch [9/10], Batch [610/747], Loss: 0.2933\n",
      "2025-04-04 17:34:14,100 - INFO - Epoch [9/10], Batch [620/747], Loss: 0.2423\n",
      "2025-04-04 17:34:14,729 - INFO - Epoch [9/10], Batch [630/747], Loss: 0.4613\n",
      "2025-04-04 17:34:15,359 - INFO - Epoch [9/10], Batch [640/747], Loss: 0.3798\n",
      "2025-04-04 17:34:15,988 - INFO - Epoch [9/10], Batch [650/747], Loss: 0.2864\n",
      "2025-04-04 17:34:16,618 - INFO - Epoch [9/10], Batch [660/747], Loss: 0.6162\n",
      "2025-04-04 17:34:17,248 - INFO - Epoch [9/10], Batch [670/747], Loss: 0.2295\n",
      "2025-04-04 17:34:17,877 - INFO - Epoch [9/10], Batch [680/747], Loss: 0.2731\n",
      "2025-04-04 17:34:18,509 - INFO - Epoch [9/10], Batch [690/747], Loss: 0.3869\n",
      "2025-04-04 17:34:19,140 - INFO - Epoch [9/10], Batch [700/747], Loss: 0.3854\n",
      "2025-04-04 17:34:19,770 - INFO - Epoch [9/10], Batch [710/747], Loss: 0.2286\n",
      "2025-04-04 17:34:20,401 - INFO - Epoch [9/10], Batch [720/747], Loss: 0.2152\n",
      "2025-04-04 17:34:21,031 - INFO - Epoch [9/10], Batch [730/747], Loss: 0.2044\n",
      "2025-04-04 17:34:21,661 - INFO - Epoch [9/10], Batch [740/747], Loss: 0.3158\n",
      "2025-04-04 17:34:22,074 - INFO - Epoch 9/10 Train Loss: 0.3421, Train Accuracy: 84.52%\n",
      "2025-04-04 17:34:25,791 - INFO - Epoch 9/10 Val Loss: 0.2796, Val Accuracy: 88.05%, AUC-ROC: 0.9455\n",
      "2025-04-04 17:34:25,794 - INFO - New best model at epoch 9 with Val Accuracy: 88.05%\n",
      "2025-04-04 17:34:26,290 - INFO - Epoch [10/10], Batch [0/747], Loss: 0.3159\n",
      "2025-04-04 17:34:26,921 - INFO - Epoch [10/10], Batch [10/747], Loss: 0.3237\n",
      "2025-04-04 17:34:27,552 - INFO - Epoch [10/10], Batch [20/747], Loss: 0.2211\n",
      "2025-04-04 17:34:28,183 - INFO - Epoch [10/10], Batch [30/747], Loss: 0.2268\n",
      "2025-04-04 17:34:28,814 - INFO - Epoch [10/10], Batch [40/747], Loss: 0.2644\n",
      "2025-04-04 17:34:29,444 - INFO - Epoch [10/10], Batch [50/747], Loss: 0.3779\n",
      "2025-04-04 17:34:30,075 - INFO - Epoch [10/10], Batch [60/747], Loss: 0.2503\n",
      "2025-04-04 17:34:30,705 - INFO - Epoch [10/10], Batch [70/747], Loss: 0.3373\n",
      "2025-04-04 17:34:31,334 - INFO - Epoch [10/10], Batch [80/747], Loss: 0.2966\n",
      "2025-04-04 17:34:31,964 - INFO - Epoch [10/10], Batch [90/747], Loss: 0.1682\n",
      "2025-04-04 17:34:32,594 - INFO - Epoch [10/10], Batch [100/747], Loss: 0.2036\n",
      "2025-04-04 17:34:33,224 - INFO - Epoch [10/10], Batch [110/747], Loss: 0.1801\n",
      "2025-04-04 17:34:33,855 - INFO - Epoch [10/10], Batch [120/747], Loss: 0.3990\n",
      "2025-04-04 17:34:34,486 - INFO - Epoch [10/10], Batch [130/747], Loss: 0.2326\n",
      "2025-04-04 17:34:35,116 - INFO - Epoch [10/10], Batch [140/747], Loss: 0.2990\n",
      "2025-04-04 17:34:35,746 - INFO - Epoch [10/10], Batch [150/747], Loss: 0.2239\n",
      "2025-04-04 17:34:36,376 - INFO - Epoch [10/10], Batch [160/747], Loss: 0.3524\n",
      "2025-04-04 17:34:37,007 - INFO - Epoch [10/10], Batch [170/747], Loss: 0.2986\n",
      "2025-04-04 17:34:37,636 - INFO - Epoch [10/10], Batch [180/747], Loss: 0.5324\n",
      "2025-04-04 17:34:38,266 - INFO - Epoch [10/10], Batch [190/747], Loss: 0.2322\n",
      "2025-04-04 17:34:38,897 - INFO - Epoch [10/10], Batch [200/747], Loss: 0.4662\n",
      "2025-04-04 17:34:39,528 - INFO - Epoch [10/10], Batch [210/747], Loss: 0.3486\n",
      "2025-04-04 17:34:40,159 - INFO - Epoch [10/10], Batch [220/747], Loss: 0.5344\n",
      "2025-04-04 17:34:40,790 - INFO - Epoch [10/10], Batch [230/747], Loss: 0.2501\n",
      "2025-04-04 17:34:41,420 - INFO - Epoch [10/10], Batch [240/747], Loss: 0.3085\n",
      "2025-04-04 17:34:42,050 - INFO - Epoch [10/10], Batch [250/747], Loss: 0.3041\n",
      "2025-04-04 17:34:42,679 - INFO - Epoch [10/10], Batch [260/747], Loss: 0.3042\n",
      "2025-04-04 17:34:43,309 - INFO - Epoch [10/10], Batch [270/747], Loss: 0.2946\n",
      "2025-04-04 17:34:43,939 - INFO - Epoch [10/10], Batch [280/747], Loss: 0.1889\n",
      "2025-04-04 17:34:44,571 - INFO - Epoch [10/10], Batch [290/747], Loss: 0.3133\n",
      "2025-04-04 17:34:45,202 - INFO - Epoch [10/10], Batch [300/747], Loss: 0.2687\n",
      "2025-04-04 17:34:45,832 - INFO - Epoch [10/10], Batch [310/747], Loss: 0.2368\n",
      "2025-04-04 17:34:46,461 - INFO - Epoch [10/10], Batch [320/747], Loss: 0.3663\n",
      "2025-04-04 17:34:47,091 - INFO - Epoch [10/10], Batch [330/747], Loss: 0.2723\n",
      "2025-04-04 17:34:47,721 - INFO - Epoch [10/10], Batch [340/747], Loss: 0.3015\n",
      "2025-04-04 17:34:48,351 - INFO - Epoch [10/10], Batch [350/747], Loss: 0.1767\n",
      "2025-04-04 17:34:48,984 - INFO - Epoch [10/10], Batch [360/747], Loss: 0.2447\n",
      "2025-04-04 17:34:49,614 - INFO - Epoch [10/10], Batch [370/747], Loss: 0.5244\n",
      "2025-04-04 17:34:50,244 - INFO - Epoch [10/10], Batch [380/747], Loss: 0.2620\n",
      "2025-04-04 17:34:50,877 - INFO - Epoch [10/10], Batch [390/747], Loss: 0.3024\n",
      "2025-04-04 17:34:51,507 - INFO - Epoch [10/10], Batch [400/747], Loss: 0.2424\n",
      "2025-04-04 17:34:52,137 - INFO - Epoch [10/10], Batch [410/747], Loss: 0.2944\n",
      "2025-04-04 17:34:52,767 - INFO - Epoch [10/10], Batch [420/747], Loss: 0.4503\n",
      "2025-04-04 17:34:53,396 - INFO - Epoch [10/10], Batch [430/747], Loss: 0.4168\n",
      "2025-04-04 17:34:54,026 - INFO - Epoch [10/10], Batch [440/747], Loss: 0.3498\n",
      "2025-04-04 17:34:54,656 - INFO - Epoch [10/10], Batch [450/747], Loss: 0.2396\n",
      "2025-04-04 17:34:55,286 - INFO - Epoch [10/10], Batch [460/747], Loss: 0.3535\n",
      "2025-04-04 17:34:55,915 - INFO - Epoch [10/10], Batch [470/747], Loss: 0.2855\n",
      "2025-04-04 17:34:56,546 - INFO - Epoch [10/10], Batch [480/747], Loss: 0.3828\n",
      "2025-04-04 17:34:57,176 - INFO - Epoch [10/10], Batch [490/747], Loss: 0.5376\n",
      "2025-04-04 17:34:57,806 - INFO - Epoch [10/10], Batch [500/747], Loss: 0.1939\n",
      "2025-04-04 17:34:58,437 - INFO - Epoch [10/10], Batch [510/747], Loss: 0.4615\n",
      "2025-04-04 17:34:59,067 - INFO - Epoch [10/10], Batch [520/747], Loss: 0.2506\n",
      "2025-04-04 17:34:59,700 - INFO - Epoch [10/10], Batch [530/747], Loss: 0.3822\n",
      "2025-04-04 17:35:00,329 - INFO - Epoch [10/10], Batch [540/747], Loss: 0.3021\n",
      "2025-04-04 17:35:00,961 - INFO - Epoch [10/10], Batch [550/747], Loss: 0.4976\n",
      "2025-04-04 17:35:01,592 - INFO - Epoch [10/10], Batch [560/747], Loss: 0.3117\n",
      "2025-04-04 17:35:02,222 - INFO - Epoch [10/10], Batch [570/747], Loss: 0.2225\n",
      "2025-04-04 17:35:02,853 - INFO - Epoch [10/10], Batch [580/747], Loss: 0.5336\n",
      "2025-04-04 17:35:03,483 - INFO - Epoch [10/10], Batch [590/747], Loss: 0.3845\n",
      "2025-04-04 17:35:04,113 - INFO - Epoch [10/10], Batch [600/747], Loss: 0.3668\n",
      "2025-04-04 17:35:04,743 - INFO - Epoch [10/10], Batch [610/747], Loss: 0.5414\n",
      "2025-04-04 17:35:05,373 - INFO - Epoch [10/10], Batch [620/747], Loss: 0.4018\n",
      "2025-04-04 17:35:06,003 - INFO - Epoch [10/10], Batch [630/747], Loss: 0.2333\n",
      "2025-04-04 17:35:06,634 - INFO - Epoch [10/10], Batch [640/747], Loss: 0.4081\n",
      "2025-04-04 17:35:07,264 - INFO - Epoch [10/10], Batch [650/747], Loss: 0.2167\n",
      "2025-04-04 17:35:07,894 - INFO - Epoch [10/10], Batch [660/747], Loss: 0.2555\n",
      "2025-04-04 17:35:08,524 - INFO - Epoch [10/10], Batch [670/747], Loss: 0.3775\n",
      "2025-04-04 17:35:09,155 - INFO - Epoch [10/10], Batch [680/747], Loss: 0.2879\n",
      "2025-04-04 17:35:09,786 - INFO - Epoch [10/10], Batch [690/747], Loss: 0.3838\n",
      "2025-04-04 17:35:10,417 - INFO - Epoch [10/10], Batch [700/747], Loss: 0.1964\n",
      "2025-04-04 17:35:11,049 - INFO - Epoch [10/10], Batch [710/747], Loss: 0.4796\n",
      "2025-04-04 17:35:11,679 - INFO - Epoch [10/10], Batch [720/747], Loss: 0.2928\n",
      "2025-04-04 17:35:12,310 - INFO - Epoch [10/10], Batch [730/747], Loss: 0.3422\n",
      "2025-04-04 17:35:12,940 - INFO - Epoch [10/10], Batch [740/747], Loss: 0.3656\n",
      "2025-04-04 17:35:13,350 - INFO - Epoch 10/10 Train Loss: 0.3333, Train Accuracy: 84.75%\n",
      "2025-04-04 17:35:17,067 - INFO - Epoch 10/10 Val Loss: 0.2729, Val Accuracy: 88.25%, AUC-ROC: 0.9477\n",
      "2025-04-04 17:35:17,071 - INFO - New best model at epoch 10 with Val Accuracy: 88.25%\n",
      "2025-04-04 17:35:17,071 - INFO - Total training time: 513.35 seconds\n",
      "2025-04-04 17:35:17,076 - INFO - Loaded best model with Val Accuracy: 88.25%\n",
      "2025-04-04 17:35:19,093 - INFO - \n",
      "===== Final Test Results =====\n",
      "2025-04-04 17:35:19,094 - INFO - Test Loss: 0.2887\n",
      "2025-04-04 17:35:19,094 - INFO - Test Accuracy: 88.27%\n",
      "2025-04-04 17:35:19,094 - INFO - Test AUC-ROC: 0.9429\n",
      "2025-04-04 17:35:19,307 - INFO - Model saved to models/training_using_gpus_1_model.pth\n",
      "2025-04-04 17:35:19,989 - INFO - Training plots saved as plots/training_using_gpus_1_results.png\n",
      "2025-04-04 17:35:20,002 - INFO - Runtime parameters saved as metrics/training_using_gpus_1_params.json\n",
      "2025-04-04 17:35:20,082 - INFO - Training completed.\n",
      "2025-04-04 17:35:20,083 - INFO - Test Accuracy: 88.27%\n",
      "2025-04-04 17:35:20,083 - INFO - Total Layers: 129\n",
      "2025-04-04 17:35:20,083 - INFO - Total Parameters: 22,494,274\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    hostname = socket.gethostname()\n",
    "    logging.info(f\"Running on node: {hostname}\")\n",
    "    data_dir = \"../../preprocessed_glaucoma_data\" \n",
    "    logging.info(\"Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single GPU...\")\n",
    "    results = train_and_evaluate(data_dir)\n",
    "    logging.info(\"Training completed.\")\n",
    "    logging.info(f\"Test Accuracy: {results['test_accuracy']:.2f}%\")\n",
    "    logging.info(f\"Total Layers: {results['total_layers']}\")\n",
    "    logging.info(f\"Total Parameters: {results['total_parameters']:,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d8891-6231-49c6-bf15-557744a4eedd",
   "metadata": {},
   "source": [
    "When training on the V100-SXM2 GPU, the process was much faster compared to a CPU. The GPUâ€™s ability to handle parallel processing really made a difference here, unlike CPUs that perform tasks sequentially, the GPU was able to run many operations at the same time (like matrix multiplications and convolutions), which significantly sped up training.\n",
    "\n",
    "On a CPU, the model has taken about 12.6 hours to train. However, with the V100-SXM2 GPU, I saw a notable reduction in training time. This showcases how much faster deep learning tasks can be with GPUs, especially for models with a large number of layers like mine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
