2025-04-04 20:21:00,144 - INFO - Running on node: d1004
2025-04-04 20:21:00,145 - INFO - Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single GPU...
2025-04-04 20:21:00,169 - INFO - Loading data...
2025-04-04 20:22:55,307 - INFO - Data shapes: Train (23898, 224, 224, 3), Val (5747, 224, 224, 3), Test (2874, 224, 224, 3)
2025-04-04 20:22:55,346 - INFO - Using device: cuda
2025-04-04 20:22:55,936 - INFO - Model architecture: MedicalCNN
2025-04-04 20:22:55,938 - INFO - Total layers: 129
2025-04-04 20:22:55,938 - INFO - Total parameters: 22,494,274
2025-04-04 20:22:57,484 - INFO - Epoch [1/10], Batch [0/747], Loss: 0.7058
2025-04-04 20:22:57,887 - INFO - Epoch [1/10], Batch [10/747], Loss: 0.6738
2025-04-04 20:22:58,290 - INFO - Epoch [1/10], Batch [20/747], Loss: 0.6454
2025-04-04 20:22:58,726 - INFO - Epoch [1/10], Batch [30/747], Loss: 0.6368
2025-04-04 20:22:59,193 - INFO - Epoch [1/10], Batch [40/747], Loss: 0.7188
2025-04-04 20:22:59,686 - INFO - Epoch [1/10], Batch [50/747], Loss: 0.6776
2025-04-04 20:23:00,154 - INFO - Epoch [1/10], Batch [60/747], Loss: 0.6732
2025-04-04 20:23:00,644 - INFO - Epoch [1/10], Batch [70/747], Loss: 0.6494
2025-04-04 20:23:01,114 - INFO - Epoch [1/10], Batch [80/747], Loss: 0.6342
2025-04-04 20:23:01,600 - INFO - Epoch [1/10], Batch [90/747], Loss: 0.5757
2025-04-04 20:23:02,076 - INFO - Epoch [1/10], Batch [100/747], Loss: 0.7729
2025-04-04 20:23:02,558 - INFO - Epoch [1/10], Batch [110/747], Loss: 0.7481
2025-04-04 20:23:03,032 - INFO - Epoch [1/10], Batch [120/747], Loss: 0.6578
2025-04-04 20:23:03,514 - INFO - Epoch [1/10], Batch [130/747], Loss: 0.5830
2025-04-04 20:23:03,986 - INFO - Epoch [1/10], Batch [140/747], Loss: 0.5944
2025-04-04 20:23:04,468 - INFO - Epoch [1/10], Batch [150/747], Loss: 0.6324
2025-04-04 20:23:04,945 - INFO - Epoch [1/10], Batch [160/747], Loss: 0.6124
2025-04-04 20:23:05,423 - INFO - Epoch [1/10], Batch [170/747], Loss: 0.5893
2025-04-04 20:23:05,904 - INFO - Epoch [1/10], Batch [180/747], Loss: 0.6013
2025-04-04 20:23:06,380 - INFO - Epoch [1/10], Batch [190/747], Loss: 0.5779
2025-04-04 20:23:06,859 - INFO - Epoch [1/10], Batch [200/747], Loss: 0.6588
2025-04-04 20:23:07,339 - INFO - Epoch [1/10], Batch [210/747], Loss: 0.6089
2025-04-04 20:23:07,821 - INFO - Epoch [1/10], Batch [220/747], Loss: 0.6957
2025-04-04 20:23:08,298 - INFO - Epoch [1/10], Batch [230/747], Loss: 0.6701
2025-04-04 20:23:08,797 - INFO - Epoch [1/10], Batch [240/747], Loss: 0.7334
2025-04-04 20:23:09,274 - INFO - Epoch [1/10], Batch [250/747], Loss: 0.5886
2025-04-04 20:23:09,757 - INFO - Epoch [1/10], Batch [260/747], Loss: 0.6144
2025-04-04 20:23:10,231 - INFO - Epoch [1/10], Batch [270/747], Loss: 0.6378
2025-04-04 20:23:10,710 - INFO - Epoch [1/10], Batch [280/747], Loss: 0.6858
2025-04-04 20:23:11,186 - INFO - Epoch [1/10], Batch [290/747], Loss: 0.5790
2025-04-04 20:23:11,674 - INFO - Epoch [1/10], Batch [300/747], Loss: 0.5856
2025-04-04 20:23:12,143 - INFO - Epoch [1/10], Batch [310/747], Loss: 0.6527
2025-04-04 20:23:12,630 - INFO - Epoch [1/10], Batch [320/747], Loss: 0.6119
2025-04-04 20:23:13,104 - INFO - Epoch [1/10], Batch [330/747], Loss: 0.5655
2025-04-04 20:23:13,590 - INFO - Epoch [1/10], Batch [340/747], Loss: 0.6442
2025-04-04 20:23:14,058 - INFO - Epoch [1/10], Batch [350/747], Loss: 0.5664
2025-04-04 20:23:14,555 - INFO - Epoch [1/10], Batch [360/747], Loss: 0.5319
2025-04-04 20:23:15,020 - INFO - Epoch [1/10], Batch [370/747], Loss: 0.5914
2025-04-04 20:23:15,518 - INFO - Epoch [1/10], Batch [380/747], Loss: 0.5571
2025-04-04 20:23:15,981 - INFO - Epoch [1/10], Batch [390/747], Loss: 0.6473
2025-04-04 20:23:16,478 - INFO - Epoch [1/10], Batch [400/747], Loss: 0.4864
2025-04-04 20:23:16,943 - INFO - Epoch [1/10], Batch [410/747], Loss: 0.7911
2025-04-04 20:23:17,441 - INFO - Epoch [1/10], Batch [420/747], Loss: 0.5942
2025-04-04 20:23:17,905 - INFO - Epoch [1/10], Batch [430/747], Loss: 0.6440
2025-04-04 20:23:18,398 - INFO - Epoch [1/10], Batch [440/747], Loss: 0.6740
2025-04-04 20:23:18,866 - INFO - Epoch [1/10], Batch [450/747], Loss: 0.7172
2025-04-04 20:23:19,361 - INFO - Epoch [1/10], Batch [460/747], Loss: 0.6591
2025-04-04 20:23:19,825 - INFO - Epoch [1/10], Batch [470/747], Loss: 0.8040
2025-04-04 20:23:20,322 - INFO - Epoch [1/10], Batch [480/747], Loss: 0.7419
2025-04-04 20:23:20,783 - INFO - Epoch [1/10], Batch [490/747], Loss: 0.5413
2025-04-04 20:23:21,274 - INFO - Epoch [1/10], Batch [500/747], Loss: 0.6686
2025-04-04 20:23:21,740 - INFO - Epoch [1/10], Batch [510/747], Loss: 0.5663
2025-04-04 20:23:22,234 - INFO - Epoch [1/10], Batch [520/747], Loss: 0.5947
2025-04-04 20:23:22,699 - INFO - Epoch [1/10], Batch [530/747], Loss: 0.5123
2025-04-04 20:23:23,195 - INFO - Epoch [1/10], Batch [540/747], Loss: 0.6727
2025-04-04 20:23:23,660 - INFO - Epoch [1/10], Batch [550/747], Loss: 0.6438
2025-04-04 20:23:24,155 - INFO - Epoch [1/10], Batch [560/747], Loss: 0.5629
2025-04-04 20:23:24,618 - INFO - Epoch [1/10], Batch [570/747], Loss: 0.5952
2025-04-04 20:23:25,110 - INFO - Epoch [1/10], Batch [580/747], Loss: 0.5244
2025-04-04 20:23:25,577 - INFO - Epoch [1/10], Batch [590/747], Loss: 0.4771
2025-04-04 20:23:26,073 - INFO - Epoch [1/10], Batch [600/747], Loss: 0.5377
2025-04-04 20:23:26,537 - INFO - Epoch [1/10], Batch [610/747], Loss: 0.5386
2025-04-04 20:23:27,038 - INFO - Epoch [1/10], Batch [620/747], Loss: 0.6881
2025-04-04 20:23:27,500 - INFO - Epoch [1/10], Batch [630/747], Loss: 0.5684
2025-04-04 20:23:28,004 - INFO - Epoch [1/10], Batch [640/747], Loss: 0.5237
2025-04-04 20:23:28,470 - INFO - Epoch [1/10], Batch [650/747], Loss: 0.5679
2025-04-04 20:23:28,971 - INFO - Epoch [1/10], Batch [660/747], Loss: 0.5217
2025-04-04 20:23:29,434 - INFO - Epoch [1/10], Batch [670/747], Loss: 0.5968
2025-04-04 20:23:29,934 - INFO - Epoch [1/10], Batch [680/747], Loss: 0.5865
2025-04-04 20:23:30,399 - INFO - Epoch [1/10], Batch [690/747], Loss: 0.6011
2025-04-04 20:23:30,889 - INFO - Epoch [1/10], Batch [700/747], Loss: 0.6162
2025-04-04 20:23:31,353 - INFO - Epoch [1/10], Batch [710/747], Loss: 0.6244
2025-04-04 20:23:31,848 - INFO - Epoch [1/10], Batch [720/747], Loss: 0.6067
2025-04-04 20:23:32,314 - INFO - Epoch [1/10], Batch [730/747], Loss: 0.5220
2025-04-04 20:23:32,806 - INFO - Epoch [1/10], Batch [740/747], Loss: 0.5510
2025-04-04 20:23:33,252 - INFO - Epoch 1/10 Train Loss: 0.6211, Train Accuracy: 64.61%
2025-04-04 20:23:37,029 - INFO - Epoch 1/10 Val Loss: 0.5677, Val Accuracy: 73.33%, AUC-ROC: 0.8068
2025-04-04 20:23:37,032 - INFO - New best model at epoch 1 with Val Accuracy: 73.33%
2025-04-04 20:23:37,486 - INFO - Epoch [2/10], Batch [0/747], Loss: 0.5722
2025-04-04 20:23:37,965 - INFO - Epoch [2/10], Batch [10/747], Loss: 0.6808
2025-04-04 20:23:38,515 - INFO - Epoch [2/10], Batch [20/747], Loss: 0.4809
2025-04-04 20:23:39,016 - INFO - Epoch [2/10], Batch [30/747], Loss: 0.5173
2025-04-04 20:23:39,566 - INFO - Epoch [2/10], Batch [40/747], Loss: 0.5596
2025-04-04 20:23:40,069 - INFO - Epoch [2/10], Batch [50/747], Loss: 0.5769
2025-04-04 20:23:40,624 - INFO - Epoch [2/10], Batch [60/747], Loss: 0.3571
2025-04-04 20:23:41,123 - INFO - Epoch [2/10], Batch [70/747], Loss: 0.5828
2025-04-04 20:23:41,677 - INFO - Epoch [2/10], Batch [80/747], Loss: 0.4224
2025-04-04 20:23:42,174 - INFO - Epoch [2/10], Batch [90/747], Loss: 0.5871
2025-04-04 20:23:42,728 - INFO - Epoch [2/10], Batch [100/747], Loss: 0.6827
2025-04-04 20:23:43,227 - INFO - Epoch [2/10], Batch [110/747], Loss: 0.5459
2025-04-04 20:23:43,786 - INFO - Epoch [2/10], Batch [120/747], Loss: 0.4224
2025-04-04 20:23:44,288 - INFO - Epoch [2/10], Batch [130/747], Loss: 0.4761
2025-04-04 20:23:44,838 - INFO - Epoch [2/10], Batch [140/747], Loss: 0.6669
2025-04-04 20:23:45,338 - INFO - Epoch [2/10], Batch [150/747], Loss: 0.5006
2025-04-04 20:23:45,893 - INFO - Epoch [2/10], Batch [160/747], Loss: 0.5563
2025-04-04 20:23:46,392 - INFO - Epoch [2/10], Batch [170/747], Loss: 0.7454
2025-04-04 20:23:46,950 - INFO - Epoch [2/10], Batch [180/747], Loss: 0.6958
2025-04-04 20:23:47,453 - INFO - Epoch [2/10], Batch [190/747], Loss: 0.4431
2025-04-04 20:23:48,010 - INFO - Epoch [2/10], Batch [200/747], Loss: 0.6406
2025-04-04 20:23:48,517 - INFO - Epoch [2/10], Batch [210/747], Loss: 0.6644
2025-04-04 20:23:49,071 - INFO - Epoch [2/10], Batch [220/747], Loss: 0.5771
2025-04-04 20:23:49,573 - INFO - Epoch [2/10], Batch [230/747], Loss: 0.5820
2025-04-04 20:23:50,126 - INFO - Epoch [2/10], Batch [240/747], Loss: 0.5949
2025-04-04 20:23:50,627 - INFO - Epoch [2/10], Batch [250/747], Loss: 0.5946
2025-04-04 20:23:51,181 - INFO - Epoch [2/10], Batch [260/747], Loss: 0.4278
2025-04-04 20:23:51,683 - INFO - Epoch [2/10], Batch [270/747], Loss: 0.4664
2025-04-04 20:23:52,232 - INFO - Epoch [2/10], Batch [280/747], Loss: 0.5681
2025-04-04 20:23:52,735 - INFO - Epoch [2/10], Batch [290/747], Loss: 0.4865
2025-04-04 20:23:53,284 - INFO - Epoch [2/10], Batch [300/747], Loss: 0.6310
2025-04-04 20:23:53,784 - INFO - Epoch [2/10], Batch [310/747], Loss: 0.4215
2025-04-04 20:23:54,333 - INFO - Epoch [2/10], Batch [320/747], Loss: 0.3806
2025-04-04 20:23:54,835 - INFO - Epoch [2/10], Batch [330/747], Loss: 0.4597
2025-04-04 20:23:55,387 - INFO - Epoch [2/10], Batch [340/747], Loss: 0.5183
2025-04-04 20:23:55,888 - INFO - Epoch [2/10], Batch [350/747], Loss: 0.5220
2025-04-04 20:23:56,442 - INFO - Epoch [2/10], Batch [360/747], Loss: 0.4955
2025-04-04 20:23:56,944 - INFO - Epoch [2/10], Batch [370/747], Loss: 0.4871
2025-04-04 20:23:57,496 - INFO - Epoch [2/10], Batch [380/747], Loss: 0.4764
2025-04-04 20:23:57,995 - INFO - Epoch [2/10], Batch [390/747], Loss: 0.3943
2025-04-04 20:23:58,545 - INFO - Epoch [2/10], Batch [400/747], Loss: 0.6053
2025-04-04 20:23:59,048 - INFO - Epoch [2/10], Batch [410/747], Loss: 0.5980
2025-04-04 20:23:59,597 - INFO - Epoch [2/10], Batch [420/747], Loss: 0.6314
2025-04-04 20:24:00,098 - INFO - Epoch [2/10], Batch [430/747], Loss: 0.5148
2025-04-04 20:24:00,653 - INFO - Epoch [2/10], Batch [440/747], Loss: 0.4974
2025-04-04 20:24:01,153 - INFO - Epoch [2/10], Batch [450/747], Loss: 0.3831
2025-04-04 20:24:01,703 - INFO - Epoch [2/10], Batch [460/747], Loss: 0.3644
2025-04-04 20:24:02,202 - INFO - Epoch [2/10], Batch [470/747], Loss: 0.4742
2025-04-04 20:24:02,754 - INFO - Epoch [2/10], Batch [480/747], Loss: 0.4661
2025-04-04 20:24:03,256 - INFO - Epoch [2/10], Batch [490/747], Loss: 0.4312
2025-04-04 20:24:03,810 - INFO - Epoch [2/10], Batch [500/747], Loss: 0.4802
2025-04-04 20:24:04,310 - INFO - Epoch [2/10], Batch [510/747], Loss: 0.4377
2025-04-04 20:24:04,865 - INFO - Epoch [2/10], Batch [520/747], Loss: 0.4482
2025-04-04 20:24:05,367 - INFO - Epoch [2/10], Batch [530/747], Loss: 0.4063
2025-04-04 20:24:05,924 - INFO - Epoch [2/10], Batch [540/747], Loss: 0.6174
2025-04-04 20:24:06,427 - INFO - Epoch [2/10], Batch [550/747], Loss: 0.4624
2025-04-04 20:24:06,980 - INFO - Epoch [2/10], Batch [560/747], Loss: 0.6218
2025-04-04 20:24:07,485 - INFO - Epoch [2/10], Batch [570/747], Loss: 0.5381
2025-04-04 20:24:08,044 - INFO - Epoch [2/10], Batch [580/747], Loss: 0.4922
2025-04-04 20:24:08,551 - INFO - Epoch [2/10], Batch [590/747], Loss: 0.5191
2025-04-04 20:24:09,109 - INFO - Epoch [2/10], Batch [600/747], Loss: 0.4549
2025-04-04 20:24:09,613 - INFO - Epoch [2/10], Batch [610/747], Loss: 0.3683
2025-04-04 20:24:10,164 - INFO - Epoch [2/10], Batch [620/747], Loss: 0.4774
2025-04-04 20:24:10,666 - INFO - Epoch [2/10], Batch [630/747], Loss: 0.4298
2025-04-04 20:24:11,220 - INFO - Epoch [2/10], Batch [640/747], Loss: 0.4646
2025-04-04 20:24:11,722 - INFO - Epoch [2/10], Batch [650/747], Loss: 0.5982
2025-04-04 20:24:12,277 - INFO - Epoch [2/10], Batch [660/747], Loss: 0.4785
2025-04-04 20:24:12,783 - INFO - Epoch [2/10], Batch [670/747], Loss: 0.4239
2025-04-04 20:24:13,335 - INFO - Epoch [2/10], Batch [680/747], Loss: 0.5629
2025-04-04 20:24:13,835 - INFO - Epoch [2/10], Batch [690/747], Loss: 0.5082
2025-04-04 20:24:14,385 - INFO - Epoch [2/10], Batch [700/747], Loss: 0.4211
2025-04-04 20:24:14,888 - INFO - Epoch [2/10], Batch [710/747], Loss: 0.3659
2025-04-04 20:24:15,440 - INFO - Epoch [2/10], Batch [720/747], Loss: 0.4178
2025-04-04 20:24:15,942 - INFO - Epoch [2/10], Batch [730/747], Loss: 0.3162
2025-04-04 20:24:16,490 - INFO - Epoch [2/10], Batch [740/747], Loss: 0.7366
2025-04-04 20:24:16,810 - INFO - Epoch 2/10 Train Loss: 0.5158, Train Accuracy: 74.73%
2025-04-04 20:24:20,497 - INFO - Epoch 2/10 Val Loss: 0.4782, Val Accuracy: 77.88%, AUC-ROC: 0.8627
2025-04-04 20:24:20,500 - INFO - New best model at epoch 2 with Val Accuracy: 77.88%
2025-04-04 20:24:20,947 - INFO - Epoch [3/10], Batch [0/747], Loss: 0.5497
2025-04-04 20:24:21,429 - INFO - Epoch [3/10], Batch [10/747], Loss: 0.5059
2025-04-04 20:24:21,959 - INFO - Epoch [3/10], Batch [20/747], Loss: 0.4485
2025-04-04 20:24:22,449 - INFO - Epoch [3/10], Batch [30/747], Loss: 0.4059
2025-04-04 20:24:22,976 - INFO - Epoch [3/10], Batch [40/747], Loss: 0.3937
2025-04-04 20:24:23,462 - INFO - Epoch [3/10], Batch [50/747], Loss: 0.6578
2025-04-04 20:24:23,991 - INFO - Epoch [3/10], Batch [60/747], Loss: 0.5508
2025-04-04 20:24:24,478 - INFO - Epoch [3/10], Batch [70/747], Loss: 0.4682
2025-04-04 20:24:25,009 - INFO - Epoch [3/10], Batch [80/747], Loss: 0.5121
2025-04-04 20:24:25,499 - INFO - Epoch [3/10], Batch [90/747], Loss: 0.3641
2025-04-04 20:24:26,025 - INFO - Epoch [3/10], Batch [100/747], Loss: 0.3909
2025-04-04 20:24:26,512 - INFO - Epoch [3/10], Batch [110/747], Loss: 0.3237
2025-04-04 20:24:27,043 - INFO - Epoch [3/10], Batch [120/747], Loss: 0.2935
2025-04-04 20:24:27,530 - INFO - Epoch [3/10], Batch [130/747], Loss: 0.4758
2025-04-04 20:24:28,062 - INFO - Epoch [3/10], Batch [140/747], Loss: 0.5344
2025-04-04 20:24:28,550 - INFO - Epoch [3/10], Batch [150/747], Loss: 0.5088
2025-04-04 20:24:29,075 - INFO - Epoch [3/10], Batch [160/747], Loss: 0.5789
2025-04-04 20:24:29,563 - INFO - Epoch [3/10], Batch [170/747], Loss: 0.4917
2025-04-04 20:24:30,105 - INFO - Epoch [3/10], Batch [180/747], Loss: 0.4268
2025-04-04 20:24:30,594 - INFO - Epoch [3/10], Batch [190/747], Loss: 0.4362
2025-04-04 20:24:31,124 - INFO - Epoch [3/10], Batch [200/747], Loss: 0.5092
2025-04-04 20:24:31,610 - INFO - Epoch [3/10], Batch [210/747], Loss: 0.4428
2025-04-04 20:24:32,142 - INFO - Epoch [3/10], Batch [220/747], Loss: 0.4004
2025-04-04 20:24:32,628 - INFO - Epoch [3/10], Batch [230/747], Loss: 0.5247
2025-04-04 20:24:33,158 - INFO - Epoch [3/10], Batch [240/747], Loss: 0.4412
2025-04-04 20:24:33,648 - INFO - Epoch [3/10], Batch [250/747], Loss: 0.5590
2025-04-04 20:24:34,181 - INFO - Epoch [3/10], Batch [260/747], Loss: 0.4643
2025-04-04 20:24:34,668 - INFO - Epoch [3/10], Batch [270/747], Loss: 0.4455
2025-04-04 20:24:35,194 - INFO - Epoch [3/10], Batch [280/747], Loss: 0.3324
2025-04-04 20:24:35,684 - INFO - Epoch [3/10], Batch [290/747], Loss: 0.5762
2025-04-04 20:24:36,216 - INFO - Epoch [3/10], Batch [300/747], Loss: 0.5908
2025-04-04 20:24:36,703 - INFO - Epoch [3/10], Batch [310/747], Loss: 0.5631
2025-04-04 20:24:37,239 - INFO - Epoch [3/10], Batch [320/747], Loss: 0.4360
2025-04-04 20:24:37,728 - INFO - Epoch [3/10], Batch [330/747], Loss: 0.4179
2025-04-04 20:24:38,257 - INFO - Epoch [3/10], Batch [340/747], Loss: 0.4532
2025-04-04 20:24:38,743 - INFO - Epoch [3/10], Batch [350/747], Loss: 0.7057
2025-04-04 20:24:39,275 - INFO - Epoch [3/10], Batch [360/747], Loss: 0.5117
2025-04-04 20:24:39,765 - INFO - Epoch [3/10], Batch [370/747], Loss: 0.3967
2025-04-04 20:24:40,297 - INFO - Epoch [3/10], Batch [380/747], Loss: 0.4944
2025-04-04 20:24:40,787 - INFO - Epoch [3/10], Batch [390/747], Loss: 0.3690
2025-04-04 20:24:41,320 - INFO - Epoch [3/10], Batch [400/747], Loss: 0.4383
2025-04-04 20:24:41,808 - INFO - Epoch [3/10], Batch [410/747], Loss: 0.5825
2025-04-04 20:24:42,342 - INFO - Epoch [3/10], Batch [420/747], Loss: 0.4084
2025-04-04 20:24:42,831 - INFO - Epoch [3/10], Batch [430/747], Loss: 0.4748
2025-04-04 20:24:43,363 - INFO - Epoch [3/10], Batch [440/747], Loss: 0.5202
2025-04-04 20:24:43,847 - INFO - Epoch [3/10], Batch [450/747], Loss: 0.5350
2025-04-04 20:24:44,379 - INFO - Epoch [3/10], Batch [460/747], Loss: 0.3930
2025-04-04 20:24:44,865 - INFO - Epoch [3/10], Batch [470/747], Loss: 0.4572
2025-04-04 20:24:45,391 - INFO - Epoch [3/10], Batch [480/747], Loss: 0.5743
2025-04-04 20:24:45,880 - INFO - Epoch [3/10], Batch [490/747], Loss: 0.3996
2025-04-04 20:24:46,411 - INFO - Epoch [3/10], Batch [500/747], Loss: 0.5019
2025-04-04 20:24:46,899 - INFO - Epoch [3/10], Batch [510/747], Loss: 0.6498
2025-04-04 20:24:47,432 - INFO - Epoch [3/10], Batch [520/747], Loss: 0.4059
2025-04-04 20:24:47,919 - INFO - Epoch [3/10], Batch [530/747], Loss: 0.5006
2025-04-04 20:24:48,452 - INFO - Epoch [3/10], Batch [540/747], Loss: 0.4409
2025-04-04 20:24:48,939 - INFO - Epoch [3/10], Batch [550/747], Loss: 0.4434
2025-04-04 20:24:49,465 - INFO - Epoch [3/10], Batch [560/747], Loss: 0.5041
2025-04-04 20:24:49,958 - INFO - Epoch [3/10], Batch [570/747], Loss: 0.5176
2025-04-04 20:24:50,500 - INFO - Epoch [3/10], Batch [580/747], Loss: 0.3629
2025-04-04 20:24:50,986 - INFO - Epoch [3/10], Batch [590/747], Loss: 0.5732
2025-04-04 20:24:51,516 - INFO - Epoch [3/10], Batch [600/747], Loss: 0.3766
2025-04-04 20:24:52,005 - INFO - Epoch [3/10], Batch [610/747], Loss: 0.6073
2025-04-04 20:24:52,538 - INFO - Epoch [3/10], Batch [620/747], Loss: 0.4317
2025-04-04 20:24:53,026 - INFO - Epoch [3/10], Batch [630/747], Loss: 0.3086
2025-04-04 20:24:53,559 - INFO - Epoch [3/10], Batch [640/747], Loss: 0.2766
2025-04-04 20:24:54,046 - INFO - Epoch [3/10], Batch [650/747], Loss: 0.5512
2025-04-04 20:24:54,578 - INFO - Epoch [3/10], Batch [660/747], Loss: 0.4745
2025-04-04 20:24:55,062 - INFO - Epoch [3/10], Batch [670/747], Loss: 0.3949
2025-04-04 20:24:55,592 - INFO - Epoch [3/10], Batch [680/747], Loss: 0.5587
2025-04-04 20:24:56,080 - INFO - Epoch [3/10], Batch [690/747], Loss: 0.3356
2025-04-04 20:24:56,607 - INFO - Epoch [3/10], Batch [700/747], Loss: 0.3356
2025-04-04 20:24:57,092 - INFO - Epoch [3/10], Batch [710/747], Loss: 0.3714
2025-04-04 20:24:57,617 - INFO - Epoch [3/10], Batch [720/747], Loss: 0.4551
2025-04-04 20:24:58,104 - INFO - Epoch [3/10], Batch [730/747], Loss: 0.6120
2025-04-04 20:24:58,634 - INFO - Epoch [3/10], Batch [740/747], Loss: 0.6321
2025-04-04 20:24:58,944 - INFO - Epoch 3/10 Train Loss: 0.4667, Train Accuracy: 77.38%
2025-04-04 20:25:02,652 - INFO - Epoch 3/10 Val Loss: 0.4239, Val Accuracy: 81.75%, AUC-ROC: 0.8835
2025-04-04 20:25:02,655 - INFO - New best model at epoch 3 with Val Accuracy: 81.75%
2025-04-04 20:25:03,116 - INFO - Epoch [4/10], Batch [0/747], Loss: 0.5574
2025-04-04 20:25:03,568 - INFO - Epoch [4/10], Batch [10/747], Loss: 0.5044
2025-04-04 20:25:04,062 - INFO - Epoch [4/10], Batch [20/747], Loss: 0.3650
2025-04-04 20:25:04,582 - INFO - Epoch [4/10], Batch [30/747], Loss: 0.5230
2025-04-04 20:25:05,076 - INFO - Epoch [4/10], Batch [40/747], Loss: 0.4977
2025-04-04 20:25:05,620 - INFO - Epoch [4/10], Batch [50/747], Loss: 0.5960
2025-04-04 20:25:06,119 - INFO - Epoch [4/10], Batch [60/747], Loss: 0.4372
2025-04-04 20:25:06,665 - INFO - Epoch [4/10], Batch [70/747], Loss: 0.3425
2025-04-04 20:25:07,177 - INFO - Epoch [4/10], Batch [80/747], Loss: 0.5788
2025-04-04 20:25:07,712 - INFO - Epoch [4/10], Batch [90/747], Loss: 0.4796
2025-04-04 20:25:08,242 - INFO - Epoch [4/10], Batch [100/747], Loss: 0.3922
2025-04-04 20:25:08,754 - INFO - Epoch [4/10], Batch [110/747], Loss: 0.5114
2025-04-04 20:25:09,309 - INFO - Epoch [4/10], Batch [120/747], Loss: 0.3452
2025-04-04 20:25:09,819 - INFO - Epoch [4/10], Batch [130/747], Loss: 0.5288
2025-04-04 20:25:10,377 - INFO - Epoch [4/10], Batch [140/747], Loss: 0.4030
2025-04-04 20:25:10,886 - INFO - Epoch [4/10], Batch [150/747], Loss: 0.4360
2025-04-04 20:25:11,451 - INFO - Epoch [4/10], Batch [160/747], Loss: 0.3694
2025-04-04 20:25:11,957 - INFO - Epoch [4/10], Batch [170/747], Loss: 0.4376
2025-04-04 20:25:12,514 - INFO - Epoch [4/10], Batch [180/747], Loss: 0.4015
2025-04-04 20:25:13,018 - INFO - Epoch [4/10], Batch [190/747], Loss: 0.4516
2025-04-04 20:25:13,583 - INFO - Epoch [4/10], Batch [200/747], Loss: 0.6341
2025-04-04 20:25:14,089 - INFO - Epoch [4/10], Batch [210/747], Loss: 0.4023
2025-04-04 20:25:14,641 - INFO - Epoch [4/10], Batch [220/747], Loss: 0.4862
2025-04-04 20:25:15,148 - INFO - Epoch [4/10], Batch [230/747], Loss: 0.3998
2025-04-04 20:25:15,707 - INFO - Epoch [4/10], Batch [240/747], Loss: 0.4225
2025-04-04 20:25:16,214 - INFO - Epoch [4/10], Batch [250/747], Loss: 0.3412
2025-04-04 20:25:16,772 - INFO - Epoch [4/10], Batch [260/747], Loss: 0.5271
2025-04-04 20:25:17,281 - INFO - Epoch [4/10], Batch [270/747], Loss: 0.5419
2025-04-04 20:25:17,841 - INFO - Epoch [4/10], Batch [280/747], Loss: 0.3319
2025-04-04 20:25:18,351 - INFO - Epoch [4/10], Batch [290/747], Loss: 0.4394
2025-04-04 20:25:18,907 - INFO - Epoch [4/10], Batch [300/747], Loss: 0.4867
2025-04-04 20:25:19,413 - INFO - Epoch [4/10], Batch [310/747], Loss: 0.4132
2025-04-04 20:25:19,966 - INFO - Epoch [4/10], Batch [320/747], Loss: 0.4560
2025-04-04 20:25:20,472 - INFO - Epoch [4/10], Batch [330/747], Loss: 0.3186
2025-04-04 20:25:21,029 - INFO - Epoch [4/10], Batch [340/747], Loss: 0.4009
2025-04-04 20:25:21,531 - INFO - Epoch [4/10], Batch [350/747], Loss: 0.4009
2025-04-04 20:25:22,086 - INFO - Epoch [4/10], Batch [360/747], Loss: 0.4856
2025-04-04 20:25:22,591 - INFO - Epoch [4/10], Batch [370/747], Loss: 0.4723
2025-04-04 20:25:23,148 - INFO - Epoch [4/10], Batch [380/747], Loss: 0.3801
2025-04-04 20:25:23,653 - INFO - Epoch [4/10], Batch [390/747], Loss: 0.4890
2025-04-04 20:25:24,208 - INFO - Epoch [4/10], Batch [400/747], Loss: 0.5417
2025-04-04 20:25:24,711 - INFO - Epoch [4/10], Batch [410/747], Loss: 0.5086
2025-04-04 20:25:25,268 - INFO - Epoch [4/10], Batch [420/747], Loss: 0.5618
2025-04-04 20:25:25,776 - INFO - Epoch [4/10], Batch [430/747], Loss: 0.4290
2025-04-04 20:25:26,334 - INFO - Epoch [4/10], Batch [440/747], Loss: 0.4031
2025-04-04 20:25:26,841 - INFO - Epoch [4/10], Batch [450/747], Loss: 0.4639
2025-04-04 20:25:27,396 - INFO - Epoch [4/10], Batch [460/747], Loss: 0.5229
2025-04-04 20:25:27,900 - INFO - Epoch [4/10], Batch [470/747], Loss: 0.4582
2025-04-04 20:25:28,460 - INFO - Epoch [4/10], Batch [480/747], Loss: 0.4756
2025-04-04 20:25:28,967 - INFO - Epoch [4/10], Batch [490/747], Loss: 0.3736
2025-04-04 20:25:29,527 - INFO - Epoch [4/10], Batch [500/747], Loss: 0.6470
2025-04-04 20:25:30,033 - INFO - Epoch [4/10], Batch [510/747], Loss: 0.5384
2025-04-04 20:25:30,594 - INFO - Epoch [4/10], Batch [520/747], Loss: 0.3821
2025-04-04 20:25:31,109 - INFO - Epoch [4/10], Batch [530/747], Loss: 0.4950
2025-04-04 20:25:31,672 - INFO - Epoch [4/10], Batch [540/747], Loss: 0.4492
2025-04-04 20:25:32,178 - INFO - Epoch [4/10], Batch [550/747], Loss: 0.4153
2025-04-04 20:25:32,735 - INFO - Epoch [4/10], Batch [560/747], Loss: 0.5300
2025-04-04 20:25:33,239 - INFO - Epoch [4/10], Batch [570/747], Loss: 0.3440
2025-04-04 20:25:33,798 - INFO - Epoch [4/10], Batch [580/747], Loss: 0.2967
2025-04-04 20:25:34,305 - INFO - Epoch [4/10], Batch [590/747], Loss: 0.2994
2025-04-04 20:25:34,865 - INFO - Epoch [4/10], Batch [600/747], Loss: 0.2672
2025-04-04 20:25:35,370 - INFO - Epoch [4/10], Batch [610/747], Loss: 0.2593
2025-04-04 20:25:35,930 - INFO - Epoch [4/10], Batch [620/747], Loss: 0.4707
2025-04-04 20:25:36,436 - INFO - Epoch [4/10], Batch [630/747], Loss: 0.5924
2025-04-04 20:25:36,995 - INFO - Epoch [4/10], Batch [640/747], Loss: 0.3977
2025-04-04 20:25:37,504 - INFO - Epoch [4/10], Batch [650/747], Loss: 0.4378
2025-04-04 20:25:38,065 - INFO - Epoch [4/10], Batch [660/747], Loss: 0.2734
2025-04-04 20:25:38,570 - INFO - Epoch [4/10], Batch [670/747], Loss: 0.5520
2025-04-04 20:25:39,129 - INFO - Epoch [4/10], Batch [680/747], Loss: 0.3601
2025-04-04 20:25:39,635 - INFO - Epoch [4/10], Batch [690/747], Loss: 0.4355
2025-04-04 20:25:40,196 - INFO - Epoch [4/10], Batch [700/747], Loss: 0.6326
2025-04-04 20:25:40,705 - INFO - Epoch [4/10], Batch [710/747], Loss: 0.5723
2025-04-04 20:25:41,264 - INFO - Epoch [4/10], Batch [720/747], Loss: 0.3621
2025-04-04 20:25:41,769 - INFO - Epoch [4/10], Batch [730/747], Loss: 0.3592
2025-04-04 20:25:42,328 - INFO - Epoch [4/10], Batch [740/747], Loss: 0.3967
2025-04-04 20:25:42,653 - INFO - Epoch 4/10 Train Loss: 0.4322, Train Accuracy: 79.73%
2025-04-04 20:25:46,347 - INFO - Epoch 4/10 Val Loss: 0.3634, Val Accuracy: 84.91%, AUC-ROC: 0.9137
2025-04-04 20:25:46,351 - INFO - New best model at epoch 4 with Val Accuracy: 84.91%
2025-04-04 20:25:46,816 - INFO - Epoch [5/10], Batch [0/747], Loss: 0.4008
2025-04-04 20:25:47,292 - INFO - Epoch [5/10], Batch [10/747], Loss: 0.3997
2025-04-04 20:25:47,847 - INFO - Epoch [5/10], Batch [20/747], Loss: 0.2269
2025-04-04 20:25:48,352 - INFO - Epoch [5/10], Batch [30/747], Loss: 0.2554
2025-04-04 20:25:48,905 - INFO - Epoch [5/10], Batch [40/747], Loss: 0.3840
2025-04-04 20:25:49,409 - INFO - Epoch [5/10], Batch [50/747], Loss: 0.3820
2025-04-04 20:25:49,959 - INFO - Epoch [5/10], Batch [60/747], Loss: 0.3883
2025-04-04 20:25:50,463 - INFO - Epoch [5/10], Batch [70/747], Loss: 0.4432
2025-04-04 20:25:51,016 - INFO - Epoch [5/10], Batch [80/747], Loss: 0.3779
2025-04-04 20:25:51,522 - INFO - Epoch [5/10], Batch [90/747], Loss: 0.5320
2025-04-04 20:25:52,074 - INFO - Epoch [5/10], Batch [100/747], Loss: 0.4205
2025-04-04 20:25:52,576 - INFO - Epoch [5/10], Batch [110/747], Loss: 0.3502
2025-04-04 20:25:53,143 - INFO - Epoch [5/10], Batch [120/747], Loss: 0.3597
2025-04-04 20:25:53,644 - INFO - Epoch [5/10], Batch [130/747], Loss: 0.3913
2025-04-04 20:25:54,191 - INFO - Epoch [5/10], Batch [140/747], Loss: 0.5063
2025-04-04 20:25:54,698 - INFO - Epoch [5/10], Batch [150/747], Loss: 0.2301
2025-04-04 20:25:55,253 - INFO - Epoch [5/10], Batch [160/747], Loss: 0.5412
2025-04-04 20:25:55,757 - INFO - Epoch [5/10], Batch [170/747], Loss: 0.3296
2025-04-04 20:25:56,309 - INFO - Epoch [5/10], Batch [180/747], Loss: 0.4996
2025-04-04 20:25:56,814 - INFO - Epoch [5/10], Batch [190/747], Loss: 0.3886
2025-04-04 20:25:57,371 - INFO - Epoch [5/10], Batch [200/747], Loss: 0.4960
2025-04-04 20:25:57,876 - INFO - Epoch [5/10], Batch [210/747], Loss: 0.2769
2025-04-04 20:25:58,433 - INFO - Epoch [5/10], Batch [220/747], Loss: 0.4797
2025-04-04 20:25:58,937 - INFO - Epoch [5/10], Batch [230/747], Loss: 0.4316
2025-04-04 20:25:59,493 - INFO - Epoch [5/10], Batch [240/747], Loss: 0.4548
2025-04-04 20:25:59,996 - INFO - Epoch [5/10], Batch [250/747], Loss: 0.4954
2025-04-04 20:26:00,552 - INFO - Epoch [5/10], Batch [260/747], Loss: 0.2189
2025-04-04 20:26:01,055 - INFO - Epoch [5/10], Batch [270/747], Loss: 0.3763
2025-04-04 20:26:01,610 - INFO - Epoch [5/10], Batch [280/747], Loss: 0.4289
2025-04-04 20:26:02,116 - INFO - Epoch [5/10], Batch [290/747], Loss: 0.3749
2025-04-04 20:26:02,674 - INFO - Epoch [5/10], Batch [300/747], Loss: 0.4354
2025-04-04 20:26:03,174 - INFO - Epoch [5/10], Batch [310/747], Loss: 0.4671
2025-04-04 20:26:03,729 - INFO - Epoch [5/10], Batch [320/747], Loss: 0.5560
2025-04-04 20:26:04,236 - INFO - Epoch [5/10], Batch [330/747], Loss: 0.2312
2025-04-04 20:26:04,790 - INFO - Epoch [5/10], Batch [340/747], Loss: 0.4412
2025-04-04 20:26:05,294 - INFO - Epoch [5/10], Batch [350/747], Loss: 0.4248
2025-04-04 20:26:05,855 - INFO - Epoch [5/10], Batch [360/747], Loss: 0.3492
2025-04-04 20:26:06,360 - INFO - Epoch [5/10], Batch [370/747], Loss: 0.3048
2025-04-04 20:26:06,914 - INFO - Epoch [5/10], Batch [380/747], Loss: 0.4477
2025-04-04 20:26:07,418 - INFO - Epoch [5/10], Batch [390/747], Loss: 0.3889
2025-04-04 20:26:07,975 - INFO - Epoch [5/10], Batch [400/747], Loss: 0.4550
2025-04-04 20:26:08,481 - INFO - Epoch [5/10], Batch [410/747], Loss: 0.3665
2025-04-04 20:26:09,034 - INFO - Epoch [5/10], Batch [420/747], Loss: 0.4638
2025-04-04 20:26:09,539 - INFO - Epoch [5/10], Batch [430/747], Loss: 0.2976
2025-04-04 20:26:10,094 - INFO - Epoch [5/10], Batch [440/747], Loss: 0.3147
2025-04-04 20:26:10,596 - INFO - Epoch [5/10], Batch [450/747], Loss: 0.2546
2025-04-04 20:26:11,152 - INFO - Epoch [5/10], Batch [460/747], Loss: 0.4490
2025-04-04 20:26:11,657 - INFO - Epoch [5/10], Batch [470/747], Loss: 0.4979
2025-04-04 20:26:12,209 - INFO - Epoch [5/10], Batch [480/747], Loss: 0.2262
2025-04-04 20:26:12,713 - INFO - Epoch [5/10], Batch [490/747], Loss: 0.2002
2025-04-04 20:26:13,280 - INFO - Epoch [5/10], Batch [500/747], Loss: 0.5448
2025-04-04 20:26:13,784 - INFO - Epoch [5/10], Batch [510/747], Loss: 0.4461
2025-04-04 20:26:14,338 - INFO - Epoch [5/10], Batch [520/747], Loss: 0.6570
2025-04-04 20:26:14,836 - INFO - Epoch [5/10], Batch [530/747], Loss: 0.4409
2025-04-04 20:26:15,391 - INFO - Epoch [5/10], Batch [540/747], Loss: 0.3447
2025-04-04 20:26:15,894 - INFO - Epoch [5/10], Batch [550/747], Loss: 0.3895
2025-04-04 20:26:16,445 - INFO - Epoch [5/10], Batch [560/747], Loss: 0.6413
2025-04-04 20:26:16,953 - INFO - Epoch [5/10], Batch [570/747], Loss: 0.3626
2025-04-04 20:26:17,510 - INFO - Epoch [5/10], Batch [580/747], Loss: 0.3278
2025-04-04 20:26:18,012 - INFO - Epoch [5/10], Batch [590/747], Loss: 0.3811
2025-04-04 20:26:18,569 - INFO - Epoch [5/10], Batch [600/747], Loss: 0.4156
2025-04-04 20:26:19,072 - INFO - Epoch [5/10], Batch [610/747], Loss: 0.2513
2025-04-04 20:26:19,630 - INFO - Epoch [5/10], Batch [620/747], Loss: 0.3373
2025-04-04 20:26:20,134 - INFO - Epoch [5/10], Batch [630/747], Loss: 0.2706
2025-04-04 20:26:20,690 - INFO - Epoch [5/10], Batch [640/747], Loss: 0.3573
2025-04-04 20:26:21,191 - INFO - Epoch [5/10], Batch [650/747], Loss: 0.4043
2025-04-04 20:26:21,742 - INFO - Epoch [5/10], Batch [660/747], Loss: 0.4094
2025-04-04 20:26:22,245 - INFO - Epoch [5/10], Batch [670/747], Loss: 0.4527
2025-04-04 20:26:22,800 - INFO - Epoch [5/10], Batch [680/747], Loss: 0.4096
2025-04-04 20:26:23,307 - INFO - Epoch [5/10], Batch [690/747], Loss: 0.3591
2025-04-04 20:26:23,861 - INFO - Epoch [5/10], Batch [700/747], Loss: 0.3838
2025-04-04 20:26:24,363 - INFO - Epoch [5/10], Batch [710/747], Loss: 0.4711
2025-04-04 20:26:24,920 - INFO - Epoch [5/10], Batch [720/747], Loss: 0.3097
2025-04-04 20:26:25,424 - INFO - Epoch [5/10], Batch [730/747], Loss: 0.2999
2025-04-04 20:26:25,981 - INFO - Epoch [5/10], Batch [740/747], Loss: 0.2830
2025-04-04 20:26:26,293 - INFO - Epoch 5/10 Train Loss: 0.4049, Train Accuracy: 81.27%
2025-04-04 20:26:29,995 - INFO - Epoch 5/10 Val Loss: 0.3471, Val Accuracy: 85.91%, AUC-ROC: 0.9267
2025-04-04 20:26:29,999 - INFO - New best model at epoch 5 with Val Accuracy: 85.91%
2025-04-04 20:26:30,452 - INFO - Epoch [6/10], Batch [0/747], Loss: 0.4333
2025-04-04 20:26:30,917 - INFO - Epoch [6/10], Batch [10/747], Loss: 0.3659
2025-04-04 20:26:31,442 - INFO - Epoch [6/10], Batch [20/747], Loss: 0.2712
2025-04-04 20:26:31,944 - INFO - Epoch [6/10], Batch [30/747], Loss: 0.5729
2025-04-04 20:26:32,459 - INFO - Epoch [6/10], Batch [40/747], Loss: 0.2703
2025-04-04 20:26:33,003 - INFO - Epoch [6/10], Batch [50/747], Loss: 0.5380
2025-04-04 20:26:33,506 - INFO - Epoch [6/10], Batch [60/747], Loss: 0.2917
2025-04-04 20:26:34,059 - INFO - Epoch [6/10], Batch [70/747], Loss: 0.3035
2025-04-04 20:26:34,562 - INFO - Epoch [6/10], Batch [80/747], Loss: 0.5146
2025-04-04 20:26:35,124 - INFO - Epoch [6/10], Batch [90/747], Loss: 0.3871
2025-04-04 20:26:35,637 - INFO - Epoch [6/10], Batch [100/747], Loss: 0.3016
2025-04-04 20:26:36,189 - INFO - Epoch [6/10], Batch [110/747], Loss: 0.4038
2025-04-04 20:26:36,686 - INFO - Epoch [6/10], Batch [120/747], Loss: 0.4472
2025-04-04 20:26:37,241 - INFO - Epoch [6/10], Batch [130/747], Loss: 0.3007
2025-04-04 20:26:37,745 - INFO - Epoch [6/10], Batch [140/747], Loss: 0.3395
2025-04-04 20:26:38,295 - INFO - Epoch [6/10], Batch [150/747], Loss: 0.3777
2025-04-04 20:26:38,796 - INFO - Epoch [6/10], Batch [160/747], Loss: 0.2855
2025-04-04 20:26:39,355 - INFO - Epoch [6/10], Batch [170/747], Loss: 0.4113
2025-04-04 20:26:39,856 - INFO - Epoch [6/10], Batch [180/747], Loss: 0.1883
2025-04-04 20:26:40,409 - INFO - Epoch [6/10], Batch [190/747], Loss: 0.4135
2025-04-04 20:26:40,915 - INFO - Epoch [6/10], Batch [200/747], Loss: 0.4569
2025-04-04 20:26:41,469 - INFO - Epoch [6/10], Batch [210/747], Loss: 0.3470
2025-04-04 20:26:41,972 - INFO - Epoch [6/10], Batch [220/747], Loss: 0.3296
2025-04-04 20:26:42,527 - INFO - Epoch [6/10], Batch [230/747], Loss: 0.3074
2025-04-04 20:26:43,032 - INFO - Epoch [6/10], Batch [240/747], Loss: 0.3394
2025-04-04 20:26:43,583 - INFO - Epoch [6/10], Batch [250/747], Loss: 0.6129
2025-04-04 20:26:44,087 - INFO - Epoch [6/10], Batch [260/747], Loss: 0.2550
2025-04-04 20:26:44,644 - INFO - Epoch [6/10], Batch [270/747], Loss: 0.3984
2025-04-04 20:26:45,145 - INFO - Epoch [6/10], Batch [280/747], Loss: 0.3844
2025-04-04 20:26:45,700 - INFO - Epoch [6/10], Batch [290/747], Loss: 0.5019
2025-04-04 20:26:46,205 - INFO - Epoch [6/10], Batch [300/747], Loss: 0.3042
2025-04-04 20:26:46,756 - INFO - Epoch [6/10], Batch [310/747], Loss: 0.3748
2025-04-04 20:26:47,262 - INFO - Epoch [6/10], Batch [320/747], Loss: 0.2535
2025-04-04 20:26:47,820 - INFO - Epoch [6/10], Batch [330/747], Loss: 0.3099
2025-04-04 20:26:48,323 - INFO - Epoch [6/10], Batch [340/747], Loss: 0.4473
2025-04-04 20:26:48,880 - INFO - Epoch [6/10], Batch [350/747], Loss: 0.5816
2025-04-04 20:26:49,379 - INFO - Epoch [6/10], Batch [360/747], Loss: 0.2435
2025-04-04 20:26:49,932 - INFO - Epoch [6/10], Batch [370/747], Loss: 0.3108
2025-04-04 20:26:50,436 - INFO - Epoch [6/10], Batch [380/747], Loss: 0.3906
2025-04-04 20:26:50,988 - INFO - Epoch [6/10], Batch [390/747], Loss: 0.3867
2025-04-04 20:26:51,491 - INFO - Epoch [6/10], Batch [400/747], Loss: 0.4771
2025-04-04 20:26:52,049 - INFO - Epoch [6/10], Batch [410/747], Loss: 0.3405
2025-04-04 20:26:52,551 - INFO - Epoch [6/10], Batch [420/747], Loss: 0.2465
2025-04-04 20:26:53,105 - INFO - Epoch [6/10], Batch [430/747], Loss: 0.3677
2025-04-04 20:26:53,608 - INFO - Epoch [6/10], Batch [440/747], Loss: 0.3563
2025-04-04 20:26:54,164 - INFO - Epoch [6/10], Batch [450/747], Loss: 0.4297
2025-04-04 20:26:54,667 - INFO - Epoch [6/10], Batch [460/747], Loss: 0.6718
2025-04-04 20:26:55,229 - INFO - Epoch [6/10], Batch [470/747], Loss: 0.5260
2025-04-04 20:26:55,744 - INFO - Epoch [6/10], Batch [480/747], Loss: 0.3508
2025-04-04 20:26:56,300 - INFO - Epoch [6/10], Batch [490/747], Loss: 0.4756
2025-04-04 20:26:56,806 - INFO - Epoch [6/10], Batch [500/747], Loss: 0.3104
2025-04-04 20:26:57,362 - INFO - Epoch [6/10], Batch [510/747], Loss: 0.4083
2025-04-04 20:26:57,866 - INFO - Epoch [6/10], Batch [520/747], Loss: 0.4248
2025-04-04 20:26:58,423 - INFO - Epoch [6/10], Batch [530/747], Loss: 0.4671
2025-04-04 20:26:58,924 - INFO - Epoch [6/10], Batch [540/747], Loss: 0.4408
2025-04-04 20:26:59,480 - INFO - Epoch [6/10], Batch [550/747], Loss: 0.2184
2025-04-04 20:26:59,984 - INFO - Epoch [6/10], Batch [560/747], Loss: 0.2239
2025-04-04 20:27:00,540 - INFO - Epoch [6/10], Batch [570/747], Loss: 0.2434
2025-04-04 20:27:01,047 - INFO - Epoch [6/10], Batch [580/747], Loss: 0.3047
2025-04-04 20:27:01,604 - INFO - Epoch [6/10], Batch [590/747], Loss: 0.5154
2025-04-04 20:27:02,106 - INFO - Epoch [6/10], Batch [600/747], Loss: 0.2015
2025-04-04 20:27:02,664 - INFO - Epoch [6/10], Batch [610/747], Loss: 0.3568
2025-04-04 20:27:03,168 - INFO - Epoch [6/10], Batch [620/747], Loss: 0.4239
2025-04-04 20:27:03,720 - INFO - Epoch [6/10], Batch [630/747], Loss: 0.3234
2025-04-04 20:27:04,225 - INFO - Epoch [6/10], Batch [640/747], Loss: 0.3761
2025-04-04 20:27:04,780 - INFO - Epoch [6/10], Batch [650/747], Loss: 0.5040
2025-04-04 20:27:05,284 - INFO - Epoch [6/10], Batch [660/747], Loss: 0.2444
2025-04-04 20:27:05,837 - INFO - Epoch [6/10], Batch [670/747], Loss: 0.2689
2025-04-04 20:27:06,339 - INFO - Epoch [6/10], Batch [680/747], Loss: 0.3075
2025-04-04 20:27:06,895 - INFO - Epoch [6/10], Batch [690/747], Loss: 0.3146
2025-04-04 20:27:07,403 - INFO - Epoch [6/10], Batch [700/747], Loss: 0.5241
2025-04-04 20:27:07,961 - INFO - Epoch [6/10], Batch [710/747], Loss: 0.4455
2025-04-04 20:27:08,467 - INFO - Epoch [6/10], Batch [720/747], Loss: 0.4804
2025-04-04 20:27:09,021 - INFO - Epoch [6/10], Batch [730/747], Loss: 0.6006
2025-04-04 20:27:09,525 - INFO - Epoch [6/10], Batch [740/747], Loss: 0.4209
2025-04-04 20:27:09,908 - INFO - Epoch 6/10 Train Loss: 0.3829, Train Accuracy: 82.45%
2025-04-04 20:27:13,599 - INFO - Epoch 6/10 Val Loss: 0.3154, Val Accuracy: 86.72%, AUC-ROC: 0.9328
2025-04-04 20:27:13,602 - INFO - New best model at epoch 6 with Val Accuracy: 86.72%
2025-04-04 20:27:14,056 - INFO - Epoch [7/10], Batch [0/747], Loss: 0.2017
2025-04-04 20:27:14,536 - INFO - Epoch [7/10], Batch [10/747], Loss: 0.2695
2025-04-04 20:27:15,099 - INFO - Epoch [7/10], Batch [20/747], Loss: 0.4475
2025-04-04 20:27:15,609 - INFO - Epoch [7/10], Batch [30/747], Loss: 0.4422
2025-04-04 20:27:16,167 - INFO - Epoch [7/10], Batch [40/747], Loss: 0.3093
2025-04-04 20:27:16,676 - INFO - Epoch [7/10], Batch [50/747], Loss: 0.3591
2025-04-04 20:27:17,242 - INFO - Epoch [7/10], Batch [60/747], Loss: 0.4728
2025-04-04 20:27:17,753 - INFO - Epoch [7/10], Batch [70/747], Loss: 0.3310
2025-04-04 20:27:18,314 - INFO - Epoch [7/10], Batch [80/747], Loss: 0.3258
2025-04-04 20:27:18,819 - INFO - Epoch [7/10], Batch [90/747], Loss: 0.3668
2025-04-04 20:27:19,380 - INFO - Epoch [7/10], Batch [100/747], Loss: 0.3428
2025-04-04 20:27:19,885 - INFO - Epoch [7/10], Batch [110/747], Loss: 0.2184
2025-04-04 20:27:20,446 - INFO - Epoch [7/10], Batch [120/747], Loss: 0.3390
2025-04-04 20:27:20,953 - INFO - Epoch [7/10], Batch [130/747], Loss: 0.5359
2025-04-04 20:27:21,510 - INFO - Epoch [7/10], Batch [140/747], Loss: 0.3667
2025-04-04 20:27:22,015 - INFO - Epoch [7/10], Batch [150/747], Loss: 0.4196
2025-04-04 20:27:22,573 - INFO - Epoch [7/10], Batch [160/747], Loss: 0.3681
2025-04-04 20:27:23,076 - INFO - Epoch [7/10], Batch [170/747], Loss: 0.2612
2025-04-04 20:27:23,638 - INFO - Epoch [7/10], Batch [180/747], Loss: 0.3804
2025-04-04 20:27:24,140 - INFO - Epoch [7/10], Batch [190/747], Loss: 0.5296
2025-04-04 20:27:24,699 - INFO - Epoch [7/10], Batch [200/747], Loss: 0.4030
2025-04-04 20:27:25,204 - INFO - Epoch [7/10], Batch [210/747], Loss: 0.3167
2025-04-04 20:27:25,764 - INFO - Epoch [7/10], Batch [220/747], Loss: 0.4413
2025-04-04 20:27:26,269 - INFO - Epoch [7/10], Batch [230/747], Loss: 0.3278
2025-04-04 20:27:26,825 - INFO - Epoch [7/10], Batch [240/747], Loss: 0.3049
2025-04-04 20:27:27,331 - INFO - Epoch [7/10], Batch [250/747], Loss: 0.3866
2025-04-04 20:27:27,889 - INFO - Epoch [7/10], Batch [260/747], Loss: 0.4158
2025-04-04 20:27:28,394 - INFO - Epoch [7/10], Batch [270/747], Loss: 0.3584
2025-04-04 20:27:28,952 - INFO - Epoch [7/10], Batch [280/747], Loss: 0.3804
2025-04-04 20:27:29,460 - INFO - Epoch [7/10], Batch [290/747], Loss: 0.2886
2025-04-04 20:27:30,021 - INFO - Epoch [7/10], Batch [300/747], Loss: 0.3015
2025-04-04 20:27:30,528 - INFO - Epoch [7/10], Batch [310/747], Loss: 0.5341
2025-04-04 20:27:31,086 - INFO - Epoch [7/10], Batch [320/747], Loss: 0.2931
2025-04-04 20:27:31,594 - INFO - Epoch [7/10], Batch [330/747], Loss: 0.2131
2025-04-04 20:27:32,155 - INFO - Epoch [7/10], Batch [340/747], Loss: 0.2227
2025-04-04 20:27:32,660 - INFO - Epoch [7/10], Batch [350/747], Loss: 0.2631
2025-04-04 20:27:33,217 - INFO - Epoch [7/10], Batch [360/747], Loss: 0.3038
2025-04-04 20:27:33,723 - INFO - Epoch [7/10], Batch [370/747], Loss: 0.4498
2025-04-04 20:27:34,283 - INFO - Epoch [7/10], Batch [380/747], Loss: 0.2253
2025-04-04 20:27:34,791 - INFO - Epoch [7/10], Batch [390/747], Loss: 0.3775
2025-04-04 20:27:35,353 - INFO - Epoch [7/10], Batch [400/747], Loss: 0.4131
2025-04-04 20:27:35,858 - INFO - Epoch [7/10], Batch [410/747], Loss: 0.3444
2025-04-04 20:27:36,423 - INFO - Epoch [7/10], Batch [420/747], Loss: 0.3791
2025-04-04 20:27:36,930 - INFO - Epoch [7/10], Batch [430/747], Loss: 0.2244
2025-04-04 20:27:37,495 - INFO - Epoch [7/10], Batch [440/747], Loss: 0.4168
2025-04-04 20:27:38,011 - INFO - Epoch [7/10], Batch [450/747], Loss: 0.3149
2025-04-04 20:27:38,572 - INFO - Epoch [7/10], Batch [460/747], Loss: 0.3022
2025-04-04 20:27:39,078 - INFO - Epoch [7/10], Batch [470/747], Loss: 0.2348
2025-04-04 20:27:39,638 - INFO - Epoch [7/10], Batch [480/747], Loss: 0.3692
2025-04-04 20:27:40,138 - INFO - Epoch [7/10], Batch [490/747], Loss: 0.3372
2025-04-04 20:27:40,699 - INFO - Epoch [7/10], Batch [500/747], Loss: 0.4543
2025-04-04 20:27:41,206 - INFO - Epoch [7/10], Batch [510/747], Loss: 0.3601
2025-04-04 20:27:41,766 - INFO - Epoch [7/10], Batch [520/747], Loss: 0.2536
2025-04-04 20:27:42,271 - INFO - Epoch [7/10], Batch [530/747], Loss: 0.2109
2025-04-04 20:27:42,827 - INFO - Epoch [7/10], Batch [540/747], Loss: 0.4852
2025-04-04 20:27:43,335 - INFO - Epoch [7/10], Batch [550/747], Loss: 0.3726
2025-04-04 20:27:43,894 - INFO - Epoch [7/10], Batch [560/747], Loss: 0.4169
2025-04-04 20:27:44,400 - INFO - Epoch [7/10], Batch [570/747], Loss: 0.3740
2025-04-04 20:27:44,959 - INFO - Epoch [7/10], Batch [580/747], Loss: 0.3641
2025-04-04 20:27:45,464 - INFO - Epoch [7/10], Batch [590/747], Loss: 0.3200
2025-04-04 20:27:46,023 - INFO - Epoch [7/10], Batch [600/747], Loss: 0.4485
2025-04-04 20:27:46,530 - INFO - Epoch [7/10], Batch [610/747], Loss: 0.2854
2025-04-04 20:27:47,090 - INFO - Epoch [7/10], Batch [620/747], Loss: 0.4568
2025-04-04 20:27:47,597 - INFO - Epoch [7/10], Batch [630/747], Loss: 0.2793
2025-04-04 20:27:48,155 - INFO - Epoch [7/10], Batch [640/747], Loss: 0.3450
2025-04-04 20:27:48,662 - INFO - Epoch [7/10], Batch [650/747], Loss: 0.3314
2025-04-04 20:27:49,224 - INFO - Epoch [7/10], Batch [660/747], Loss: 0.3326
2025-04-04 20:27:49,730 - INFO - Epoch [7/10], Batch [670/747], Loss: 0.1997
2025-04-04 20:27:50,284 - INFO - Epoch [7/10], Batch [680/747], Loss: 0.2670
2025-04-04 20:27:50,787 - INFO - Epoch [7/10], Batch [690/747], Loss: 0.3164
2025-04-04 20:27:51,347 - INFO - Epoch [7/10], Batch [700/747], Loss: 0.2766
2025-04-04 20:27:51,853 - INFO - Epoch [7/10], Batch [710/747], Loss: 0.4472
2025-04-04 20:27:52,412 - INFO - Epoch [7/10], Batch [720/747], Loss: 0.4908
2025-04-04 20:27:52,918 - INFO - Epoch [7/10], Batch [730/747], Loss: 0.2743
2025-04-04 20:27:53,477 - INFO - Epoch [7/10], Batch [740/747], Loss: 0.4464
2025-04-04 20:27:53,798 - INFO - Epoch 7/10 Train Loss: 0.3595, Train Accuracy: 83.50%
2025-04-04 20:27:57,514 - INFO - Epoch 7/10 Val Loss: 0.3015, Val Accuracy: 87.14%, AUC-ROC: 0.9377
2025-04-04 20:27:57,518 - INFO - New best model at epoch 7 with Val Accuracy: 87.14%
2025-04-04 20:27:57,980 - INFO - Epoch [8/10], Batch [0/747], Loss: 0.2360
2025-04-04 20:27:58,471 - INFO - Epoch [8/10], Batch [10/747], Loss: 0.2365
2025-04-04 20:27:59,035 - INFO - Epoch [8/10], Batch [20/747], Loss: 0.2660
2025-04-04 20:27:59,546 - INFO - Epoch [8/10], Batch [30/747], Loss: 0.3620
2025-04-04 20:28:00,113 - INFO - Epoch [8/10], Batch [40/747], Loss: 0.3313
2025-04-04 20:28:00,620 - INFO - Epoch [8/10], Batch [50/747], Loss: 0.3744
2025-04-04 20:28:01,182 - INFO - Epoch [8/10], Batch [60/747], Loss: 0.3146
2025-04-04 20:28:01,685 - INFO - Epoch [8/10], Batch [70/747], Loss: 0.3837
2025-04-04 20:28:02,246 - INFO - Epoch [8/10], Batch [80/747], Loss: 0.2434
2025-04-04 20:28:02,747 - INFO - Epoch [8/10], Batch [90/747], Loss: 0.4622
2025-04-04 20:28:03,311 - INFO - Epoch [8/10], Batch [100/747], Loss: 0.4905
2025-04-04 20:28:03,819 - INFO - Epoch [8/10], Batch [110/747], Loss: 0.6022
2025-04-04 20:28:04,379 - INFO - Epoch [8/10], Batch [120/747], Loss: 0.3513
2025-04-04 20:28:04,886 - INFO - Epoch [8/10], Batch [130/747], Loss: 0.3677
2025-04-04 20:28:05,446 - INFO - Epoch [8/10], Batch [140/747], Loss: 0.2797
2025-04-04 20:28:05,951 - INFO - Epoch [8/10], Batch [150/747], Loss: 0.2600
2025-04-04 20:28:06,515 - INFO - Epoch [8/10], Batch [160/747], Loss: 0.3861
2025-04-04 20:28:07,020 - INFO - Epoch [8/10], Batch [170/747], Loss: 0.4599
2025-04-04 20:28:07,578 - INFO - Epoch [8/10], Batch [180/747], Loss: 0.4339
2025-04-04 20:28:08,084 - INFO - Epoch [8/10], Batch [190/747], Loss: 0.2944
2025-04-04 20:28:08,644 - INFO - Epoch [8/10], Batch [200/747], Loss: 0.2507
2025-04-04 20:28:09,152 - INFO - Epoch [8/10], Batch [210/747], Loss: 0.4679
2025-04-04 20:28:09,707 - INFO - Epoch [8/10], Batch [220/747], Loss: 0.3419
2025-04-04 20:28:10,213 - INFO - Epoch [8/10], Batch [230/747], Loss: 0.4275
2025-04-04 20:28:10,780 - INFO - Epoch [8/10], Batch [240/747], Loss: 0.4708
2025-04-04 20:28:11,284 - INFO - Epoch [8/10], Batch [250/747], Loss: 0.3971
2025-04-04 20:28:11,842 - INFO - Epoch [8/10], Batch [260/747], Loss: 0.2551
2025-04-04 20:28:12,348 - INFO - Epoch [8/10], Batch [270/747], Loss: 0.5165
2025-04-04 20:28:12,909 - INFO - Epoch [8/10], Batch [280/747], Loss: 0.3624
2025-04-04 20:28:13,412 - INFO - Epoch [8/10], Batch [290/747], Loss: 0.2398
2025-04-04 20:28:13,975 - INFO - Epoch [8/10], Batch [300/747], Loss: 0.3692
2025-04-04 20:28:14,479 - INFO - Epoch [8/10], Batch [310/747], Loss: 0.3842
2025-04-04 20:28:15,043 - INFO - Epoch [8/10], Batch [320/747], Loss: 0.3617
2025-04-04 20:28:15,548 - INFO - Epoch [8/10], Batch [330/747], Loss: 0.4196
2025-04-04 20:28:16,109 - INFO - Epoch [8/10], Batch [340/747], Loss: 0.2586
2025-04-04 20:28:16,617 - INFO - Epoch [8/10], Batch [350/747], Loss: 0.2614
2025-04-04 20:28:17,177 - INFO - Epoch [8/10], Batch [360/747], Loss: 0.2821
2025-04-04 20:28:17,687 - INFO - Epoch [8/10], Batch [370/747], Loss: 0.3119
2025-04-04 20:28:18,250 - INFO - Epoch [8/10], Batch [380/747], Loss: 0.2465
2025-04-04 20:28:18,759 - INFO - Epoch [8/10], Batch [390/747], Loss: 0.3880
2025-04-04 20:28:19,315 - INFO - Epoch [8/10], Batch [400/747], Loss: 0.5619
2025-04-04 20:28:19,827 - INFO - Epoch [8/10], Batch [410/747], Loss: 0.4022
2025-04-04 20:28:20,393 - INFO - Epoch [8/10], Batch [420/747], Loss: 0.3238
2025-04-04 20:28:20,898 - INFO - Epoch [8/10], Batch [430/747], Loss: 0.2648
2025-04-04 20:28:21,459 - INFO - Epoch [8/10], Batch [440/747], Loss: 0.2148
2025-04-04 20:28:21,963 - INFO - Epoch [8/10], Batch [450/747], Loss: 0.2647
2025-04-04 20:28:22,523 - INFO - Epoch [8/10], Batch [460/747], Loss: 0.3305
2025-04-04 20:28:23,031 - INFO - Epoch [8/10], Batch [470/747], Loss: 0.4578
2025-04-04 20:28:23,590 - INFO - Epoch [8/10], Batch [480/747], Loss: 0.2126
2025-04-04 20:28:24,099 - INFO - Epoch [8/10], Batch [490/747], Loss: 0.2883
2025-04-04 20:28:24,660 - INFO - Epoch [8/10], Batch [500/747], Loss: 0.3188
2025-04-04 20:28:25,165 - INFO - Epoch [8/10], Batch [510/747], Loss: 0.2235
2025-04-04 20:28:25,726 - INFO - Epoch [8/10], Batch [520/747], Loss: 0.4310
2025-04-04 20:28:26,235 - INFO - Epoch [8/10], Batch [530/747], Loss: 0.3573
2025-04-04 20:28:26,795 - INFO - Epoch [8/10], Batch [540/747], Loss: 0.2586
2025-04-04 20:28:27,297 - INFO - Epoch [8/10], Batch [550/747], Loss: 0.4238
2025-04-04 20:28:27,860 - INFO - Epoch [8/10], Batch [560/747], Loss: 0.2665
2025-04-04 20:28:28,367 - INFO - Epoch [8/10], Batch [570/747], Loss: 0.1359
2025-04-04 20:28:28,929 - INFO - Epoch [8/10], Batch [580/747], Loss: 0.2808
2025-04-04 20:28:29,437 - INFO - Epoch [8/10], Batch [590/747], Loss: 0.2976
2025-04-04 20:28:29,995 - INFO - Epoch [8/10], Batch [600/747], Loss: 0.3114
2025-04-04 20:28:30,499 - INFO - Epoch [8/10], Batch [610/747], Loss: 0.2378
2025-04-04 20:28:31,057 - INFO - Epoch [8/10], Batch [620/747], Loss: 0.3725
2025-04-04 20:28:31,559 - INFO - Epoch [8/10], Batch [630/747], Loss: 0.4516
2025-04-04 20:28:32,120 - INFO - Epoch [8/10], Batch [640/747], Loss: 0.2055
2025-04-04 20:28:32,623 - INFO - Epoch [8/10], Batch [650/747], Loss: 0.2678
2025-04-04 20:28:33,187 - INFO - Epoch [8/10], Batch [660/747], Loss: 0.4641
2025-04-04 20:28:33,697 - INFO - Epoch [8/10], Batch [670/747], Loss: 0.5486
2025-04-04 20:28:34,259 - INFO - Epoch [8/10], Batch [680/747], Loss: 0.7045
2025-04-04 20:28:34,768 - INFO - Epoch [8/10], Batch [690/747], Loss: 0.2867
2025-04-04 20:28:35,332 - INFO - Epoch [8/10], Batch [700/747], Loss: 0.2206
2025-04-04 20:28:35,837 - INFO - Epoch [8/10], Batch [710/747], Loss: 0.4458
2025-04-04 20:28:36,393 - INFO - Epoch [8/10], Batch [720/747], Loss: 0.4190
2025-04-04 20:28:36,898 - INFO - Epoch [8/10], Batch [730/747], Loss: 0.2538
2025-04-04 20:28:37,461 - INFO - Epoch [8/10], Batch [740/747], Loss: 0.2723
2025-04-04 20:28:37,785 - INFO - Epoch 8/10 Train Loss: 0.3438, Train Accuracy: 84.20%
2025-04-04 20:28:41,498 - INFO - Epoch 8/10 Val Loss: 0.2771, Val Accuracy: 88.17%, AUC-ROC: 0.9465
2025-04-04 20:28:41,501 - INFO - New best model at epoch 8 with Val Accuracy: 88.17%
2025-04-04 20:28:41,952 - INFO - Epoch [9/10], Batch [0/747], Loss: 0.3899
2025-04-04 20:28:42,402 - INFO - Epoch [9/10], Batch [10/747], Loss: 0.3191
2025-04-04 20:28:42,893 - INFO - Epoch [9/10], Batch [20/747], Loss: 0.2427
2025-04-04 20:28:43,394 - INFO - Epoch [9/10], Batch [30/747], Loss: 0.3100
2025-04-04 20:28:43,882 - INFO - Epoch [9/10], Batch [40/747], Loss: 0.2766
2025-04-04 20:28:44,407 - INFO - Epoch [9/10], Batch [50/747], Loss: 0.2596
2025-04-04 20:28:44,891 - INFO - Epoch [9/10], Batch [60/747], Loss: 0.3075
2025-04-04 20:28:45,421 - INFO - Epoch [9/10], Batch [70/747], Loss: 0.3825
2025-04-04 20:28:45,905 - INFO - Epoch [9/10], Batch [80/747], Loss: 0.3933
2025-04-04 20:28:46,433 - INFO - Epoch [9/10], Batch [90/747], Loss: 0.2680
2025-04-04 20:28:46,923 - INFO - Epoch [9/10], Batch [100/747], Loss: 0.3583
2025-04-04 20:28:47,453 - INFO - Epoch [9/10], Batch [110/747], Loss: 0.5598
2025-04-04 20:28:47,934 - INFO - Epoch [9/10], Batch [120/747], Loss: 0.2774
2025-04-04 20:28:48,468 - INFO - Epoch [9/10], Batch [130/747], Loss: 0.2451
2025-04-04 20:28:48,957 - INFO - Epoch [9/10], Batch [140/747], Loss: 0.2733
2025-04-04 20:28:49,486 - INFO - Epoch [9/10], Batch [150/747], Loss: 0.3022
2025-04-04 20:28:49,971 - INFO - Epoch [9/10], Batch [160/747], Loss: 0.2111
2025-04-04 20:28:50,504 - INFO - Epoch [9/10], Batch [170/747], Loss: 0.2741
2025-04-04 20:28:50,989 - INFO - Epoch [9/10], Batch [180/747], Loss: 0.4486
2025-04-04 20:28:51,520 - INFO - Epoch [9/10], Batch [190/747], Loss: 0.4406
2025-04-04 20:28:52,009 - INFO - Epoch [9/10], Batch [200/747], Loss: 0.3153
2025-04-04 20:28:52,543 - INFO - Epoch [9/10], Batch [210/747], Loss: 0.2784
2025-04-04 20:28:53,029 - INFO - Epoch [9/10], Batch [220/747], Loss: 0.5169
2025-04-04 20:28:53,560 - INFO - Epoch [9/10], Batch [230/747], Loss: 0.3440
2025-04-04 20:28:54,054 - INFO - Epoch [9/10], Batch [240/747], Loss: 0.2226
2025-04-04 20:28:54,579 - INFO - Epoch [9/10], Batch [250/747], Loss: 0.3389
2025-04-04 20:28:55,064 - INFO - Epoch [9/10], Batch [260/747], Loss: 0.5323
2025-04-04 20:28:55,593 - INFO - Epoch [9/10], Batch [270/747], Loss: 0.3291
2025-04-04 20:28:56,083 - INFO - Epoch [9/10], Batch [280/747], Loss: 0.3230
2025-04-04 20:28:56,621 - INFO - Epoch [9/10], Batch [290/747], Loss: 0.4056
2025-04-04 20:28:57,106 - INFO - Epoch [9/10], Batch [300/747], Loss: 0.3579
2025-04-04 20:28:57,637 - INFO - Epoch [9/10], Batch [310/747], Loss: 0.3818
2025-04-04 20:28:58,122 - INFO - Epoch [9/10], Batch [320/747], Loss: 0.3161
2025-04-04 20:28:58,653 - INFO - Epoch [9/10], Batch [330/747], Loss: 0.2810
2025-04-04 20:28:59,137 - INFO - Epoch [9/10], Batch [340/747], Loss: 0.2816
2025-04-04 20:28:59,668 - INFO - Epoch [9/10], Batch [350/747], Loss: 0.4242
2025-04-04 20:29:00,156 - INFO - Epoch [9/10], Batch [360/747], Loss: 0.2176
2025-04-04 20:29:00,687 - INFO - Epoch [9/10], Batch [370/747], Loss: 0.3126
2025-04-04 20:29:01,173 - INFO - Epoch [9/10], Batch [380/747], Loss: 0.2629
2025-04-04 20:29:01,710 - INFO - Epoch [9/10], Batch [390/747], Loss: 0.6879
2025-04-04 20:29:02,194 - INFO - Epoch [9/10], Batch [400/747], Loss: 0.2158
2025-04-04 20:29:02,728 - INFO - Epoch [9/10], Batch [410/747], Loss: 0.3473
2025-04-04 20:29:03,205 - INFO - Epoch [9/10], Batch [420/747], Loss: 0.2684
2025-04-04 20:29:03,736 - INFO - Epoch [9/10], Batch [430/747], Loss: 0.2272
2025-04-04 20:29:04,223 - INFO - Epoch [9/10], Batch [440/747], Loss: 0.1771
2025-04-04 20:29:04,762 - INFO - Epoch [9/10], Batch [450/747], Loss: 0.1731
2025-04-04 20:29:05,250 - INFO - Epoch [9/10], Batch [460/747], Loss: 0.4478
2025-04-04 20:29:05,779 - INFO - Epoch [9/10], Batch [470/747], Loss: 0.3591
2025-04-04 20:29:06,265 - INFO - Epoch [9/10], Batch [480/747], Loss: 0.2531
2025-04-04 20:29:06,795 - INFO - Epoch [9/10], Batch [490/747], Loss: 0.3285
2025-04-04 20:29:07,283 - INFO - Epoch [9/10], Batch [500/747], Loss: 0.2883
2025-04-04 20:29:07,815 - INFO - Epoch [9/10], Batch [510/747], Loss: 0.2858
2025-04-04 20:29:08,302 - INFO - Epoch [9/10], Batch [520/747], Loss: 0.3953
2025-04-04 20:29:08,835 - INFO - Epoch [9/10], Batch [530/747], Loss: 0.2213
2025-04-04 20:29:09,321 - INFO - Epoch [9/10], Batch [540/747], Loss: 0.2711
2025-04-04 20:29:09,856 - INFO - Epoch [9/10], Batch [550/747], Loss: 0.2562
2025-04-04 20:29:10,343 - INFO - Epoch [9/10], Batch [560/747], Loss: 0.4631
2025-04-04 20:29:10,876 - INFO - Epoch [9/10], Batch [570/747], Loss: 0.3441
2025-04-04 20:29:11,366 - INFO - Epoch [9/10], Batch [580/747], Loss: 0.3463
2025-04-04 20:29:11,908 - INFO - Epoch [9/10], Batch [590/747], Loss: 0.2024
2025-04-04 20:29:12,395 - INFO - Epoch [9/10], Batch [600/747], Loss: 0.1406
2025-04-04 20:29:12,919 - INFO - Epoch [9/10], Batch [610/747], Loss: 0.2731
2025-04-04 20:29:13,407 - INFO - Epoch [9/10], Batch [620/747], Loss: 0.2629
2025-04-04 20:29:13,943 - INFO - Epoch [9/10], Batch [630/747], Loss: 0.4125
2025-04-04 20:29:14,433 - INFO - Epoch [9/10], Batch [640/747], Loss: 0.2722
2025-04-04 20:29:14,966 - INFO - Epoch [9/10], Batch [650/747], Loss: 0.2565
2025-04-04 20:29:15,455 - INFO - Epoch [9/10], Batch [660/747], Loss: 0.5531
2025-04-04 20:29:15,990 - INFO - Epoch [9/10], Batch [670/747], Loss: 0.3035
2025-04-04 20:29:16,479 - INFO - Epoch [9/10], Batch [680/747], Loss: 0.3070
2025-04-04 20:29:17,010 - INFO - Epoch [9/10], Batch [690/747], Loss: 0.3385
2025-04-04 20:29:17,497 - INFO - Epoch [9/10], Batch [700/747], Loss: 0.3409
2025-04-04 20:29:18,024 - INFO - Epoch [9/10], Batch [710/747], Loss: 0.2978
2025-04-04 20:29:18,507 - INFO - Epoch [9/10], Batch [720/747], Loss: 0.2683
2025-04-04 20:29:19,035 - INFO - Epoch [9/10], Batch [730/747], Loss: 0.2111
2025-04-04 20:29:19,526 - INFO - Epoch [9/10], Batch [740/747], Loss: 0.2198
2025-04-04 20:29:19,889 - INFO - Epoch 9/10 Train Loss: 0.3301, Train Accuracy: 85.26%
2025-04-04 20:29:23,579 - INFO - Epoch 9/10 Val Loss: 0.2701, Val Accuracy: 88.65%, AUC-ROC: 0.9508
2025-04-04 20:29:23,583 - INFO - New best model at epoch 9 with Val Accuracy: 88.65%
2025-04-04 20:29:24,032 - INFO - Epoch [10/10], Batch [0/747], Loss: 0.3187
2025-04-04 20:29:24,482 - INFO - Epoch [10/10], Batch [10/747], Loss: 0.3419
2025-04-04 20:29:24,975 - INFO - Epoch [10/10], Batch [20/747], Loss: 0.2214
2025-04-04 20:29:25,517 - INFO - Epoch [10/10], Batch [30/747], Loss: 0.2536
2025-04-04 20:29:26,015 - INFO - Epoch [10/10], Batch [40/747], Loss: 0.2103
2025-04-04 20:29:26,559 - INFO - Epoch [10/10], Batch [50/747], Loss: 0.4089
2025-04-04 20:29:27,055 - INFO - Epoch [10/10], Batch [60/747], Loss: 0.2991
2025-04-04 20:29:27,606 - INFO - Epoch [10/10], Batch [70/747], Loss: 0.3728
2025-04-04 20:29:28,108 - INFO - Epoch [10/10], Batch [80/747], Loss: 0.2552
2025-04-04 20:29:28,651 - INFO - Epoch [10/10], Batch [90/747], Loss: 0.1820
2025-04-04 20:29:29,149 - INFO - Epoch [10/10], Batch [100/747], Loss: 0.1737
2025-04-04 20:29:29,700 - INFO - Epoch [10/10], Batch [110/747], Loss: 0.1583
2025-04-04 20:29:30,197 - INFO - Epoch [10/10], Batch [120/747], Loss: 0.2776
2025-04-04 20:29:30,745 - INFO - Epoch [10/10], Batch [130/747], Loss: 0.2622
2025-04-04 20:29:31,241 - INFO - Epoch [10/10], Batch [140/747], Loss: 0.3733
2025-04-04 20:29:31,789 - INFO - Epoch [10/10], Batch [150/747], Loss: 0.1905
2025-04-04 20:29:32,287 - INFO - Epoch [10/10], Batch [160/747], Loss: 0.3485
2025-04-04 20:29:32,840 - INFO - Epoch [10/10], Batch [170/747], Loss: 0.2261
2025-04-04 20:29:33,338 - INFO - Epoch [10/10], Batch [180/747], Loss: 0.4798
2025-04-04 20:29:33,886 - INFO - Epoch [10/10], Batch [190/747], Loss: 0.2544
2025-04-04 20:29:34,380 - INFO - Epoch [10/10], Batch [200/747], Loss: 0.3650
2025-04-04 20:29:34,928 - INFO - Epoch [10/10], Batch [210/747], Loss: 0.3728
2025-04-04 20:29:35,429 - INFO - Epoch [10/10], Batch [220/747], Loss: 0.4611
2025-04-04 20:29:35,985 - INFO - Epoch [10/10], Batch [230/747], Loss: 0.2691
2025-04-04 20:29:36,489 - INFO - Epoch [10/10], Batch [240/747], Loss: 0.3062
2025-04-04 20:29:37,034 - INFO - Epoch [10/10], Batch [250/747], Loss: 0.2741
2025-04-04 20:29:37,528 - INFO - Epoch [10/10], Batch [260/747], Loss: 0.3194
2025-04-04 20:29:38,074 - INFO - Epoch [10/10], Batch [270/747], Loss: 0.3352
2025-04-04 20:29:38,569 - INFO - Epoch [10/10], Batch [280/747], Loss: 0.1945
2025-04-04 20:29:39,119 - INFO - Epoch [10/10], Batch [290/747], Loss: 0.2695
2025-04-04 20:29:39,617 - INFO - Epoch [10/10], Batch [300/747], Loss: 0.2380
2025-04-04 20:29:40,157 - INFO - Epoch [10/10], Batch [310/747], Loss: 0.1669
2025-04-04 20:29:40,653 - INFO - Epoch [10/10], Batch [320/747], Loss: 0.2932
2025-04-04 20:29:41,203 - INFO - Epoch [10/10], Batch [330/747], Loss: 0.2704
2025-04-04 20:29:41,699 - INFO - Epoch [10/10], Batch [340/747], Loss: 0.3180
2025-04-04 20:29:42,250 - INFO - Epoch [10/10], Batch [350/747], Loss: 0.1492
2025-04-04 20:29:42,751 - INFO - Epoch [10/10], Batch [360/747], Loss: 0.2046
2025-04-04 20:29:43,298 - INFO - Epoch [10/10], Batch [370/747], Loss: 0.4203
2025-04-04 20:29:43,797 - INFO - Epoch [10/10], Batch [380/747], Loss: 0.2304
2025-04-04 20:29:44,346 - INFO - Epoch [10/10], Batch [390/747], Loss: 0.2772
2025-04-04 20:29:44,844 - INFO - Epoch [10/10], Batch [400/747], Loss: 0.2119
2025-04-04 20:29:45,391 - INFO - Epoch [10/10], Batch [410/747], Loss: 0.3533
2025-04-04 20:29:45,889 - INFO - Epoch [10/10], Batch [420/747], Loss: 0.3783
2025-04-04 20:29:46,434 - INFO - Epoch [10/10], Batch [430/747], Loss: 0.3181
2025-04-04 20:29:46,934 - INFO - Epoch [10/10], Batch [440/747], Loss: 0.2262
2025-04-04 20:29:47,481 - INFO - Epoch [10/10], Batch [450/747], Loss: 0.2419
2025-04-04 20:29:47,978 - INFO - Epoch [10/10], Batch [460/747], Loss: 0.3871
2025-04-04 20:29:48,527 - INFO - Epoch [10/10], Batch [470/747], Loss: 0.2808
2025-04-04 20:29:49,028 - INFO - Epoch [10/10], Batch [480/747], Loss: 0.2656
2025-04-04 20:29:49,575 - INFO - Epoch [10/10], Batch [490/747], Loss: 0.3498
2025-04-04 20:29:50,074 - INFO - Epoch [10/10], Batch [500/747], Loss: 0.2378
2025-04-04 20:29:50,622 - INFO - Epoch [10/10], Batch [510/747], Loss: 0.5415
2025-04-04 20:29:51,128 - INFO - Epoch [10/10], Batch [520/747], Loss: 0.2393
2025-04-04 20:29:51,684 - INFO - Epoch [10/10], Batch [530/747], Loss: 0.2599
2025-04-04 20:29:52,186 - INFO - Epoch [10/10], Batch [540/747], Loss: 0.2626
2025-04-04 20:29:52,732 - INFO - Epoch [10/10], Batch [550/747], Loss: 0.3749
2025-04-04 20:29:53,231 - INFO - Epoch [10/10], Batch [560/747], Loss: 0.3175
2025-04-04 20:29:53,776 - INFO - Epoch [10/10], Batch [570/747], Loss: 0.2176
2025-04-04 20:29:54,274 - INFO - Epoch [10/10], Batch [580/747], Loss: 0.3732
2025-04-04 20:29:54,825 - INFO - Epoch [10/10], Batch [590/747], Loss: 0.4274
2025-04-04 20:29:55,322 - INFO - Epoch [10/10], Batch [600/747], Loss: 0.4409
2025-04-04 20:29:55,867 - INFO - Epoch [10/10], Batch [610/747], Loss: 0.4428
2025-04-04 20:29:56,366 - INFO - Epoch [10/10], Batch [620/747], Loss: 0.3472
2025-04-04 20:29:56,910 - INFO - Epoch [10/10], Batch [630/747], Loss: 0.2041
2025-04-04 20:29:57,405 - INFO - Epoch [10/10], Batch [640/747], Loss: 0.4338
2025-04-04 20:29:57,945 - INFO - Epoch [10/10], Batch [650/747], Loss: 0.2953
2025-04-04 20:29:58,447 - INFO - Epoch [10/10], Batch [660/747], Loss: 0.2306
2025-04-04 20:29:58,995 - INFO - Epoch [10/10], Batch [670/747], Loss: 0.4066
2025-04-04 20:29:59,495 - INFO - Epoch [10/10], Batch [680/747], Loss: 0.2871
2025-04-04 20:30:00,046 - INFO - Epoch [10/10], Batch [690/747], Loss: 0.3319
2025-04-04 20:30:00,545 - INFO - Epoch [10/10], Batch [700/747], Loss: 0.1636
2025-04-04 20:30:01,094 - INFO - Epoch [10/10], Batch [710/747], Loss: 0.4614
2025-04-04 20:30:01,591 - INFO - Epoch [10/10], Batch [720/747], Loss: 0.2988
2025-04-04 20:30:02,138 - INFO - Epoch [10/10], Batch [730/747], Loss: 0.3032
2025-04-04 20:30:02,635 - INFO - Epoch [10/10], Batch [740/747], Loss: 0.3627
2025-04-04 20:30:03,028 - INFO - Epoch 10/10 Train Loss: 0.3190, Train Accuracy: 85.30%
2025-04-04 20:30:06,750 - INFO - Epoch 10/10 Val Loss: 0.2609, Val Accuracy: 89.07%, AUC-ROC: 0.9532
2025-04-04 20:30:06,753 - INFO - New best model at epoch 10 with Val Accuracy: 89.07%
2025-04-04 20:30:06,753 - INFO - Total training time: 430.81 seconds
2025-04-04 20:30:06,758 - INFO - Loaded best model with Val Accuracy: 89.07%
2025-04-04 20:30:08,771 - INFO - 
===== Final Test Results =====
2025-04-04 20:30:08,772 - INFO - Test Loss: 0.2741
2025-04-04 20:30:08,773 - INFO - Test Accuracy: 88.45%
2025-04-04 20:30:08,773 - INFO - Test AUC-ROC: 0.9497
2025-04-04 20:30:09,116 - INFO - Model saved to models/training_using_gpus_1_amp_model.pth
2025-04-04 20:30:10,312 - INFO - Training plots saved as plots/training_using_gpus_1_amp_results.png
2025-04-04 20:30:10,349 - INFO - Runtime parameters saved as metrics/training_using_gpus_1_amp_params.json
2025-04-04 20:30:10,425 - INFO - Training completed.
2025-04-04 20:30:10,425 - INFO - Test Accuracy: 88.45%
2025-04-04 20:30:10,425 - INFO - Total Layers: 129
2025-04-04 20:30:10,426 - INFO - Total Parameters: 22,494,274
