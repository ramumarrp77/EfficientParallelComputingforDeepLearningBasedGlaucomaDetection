2025-04-04 17:23:25,248 - INFO - Running on node: d1004
2025-04-04 17:23:25,249 - INFO - Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single GPU...
2025-04-04 17:23:25,259 - INFO - Loading data...
2025-04-04 17:24:21,202 - INFO - Data shapes: Train (23898, 224, 224, 3), Val (5747, 224, 224, 3), Test (2874, 224, 224, 3)
2025-04-04 17:24:21,255 - INFO - Using device: cuda
2025-04-04 17:24:21,856 - INFO - Model architecture: MedicalCNN
2025-04-04 17:24:21,857 - INFO - Total layers: 129
2025-04-04 17:24:21,857 - INFO - Total parameters: 22,494,274
2025-04-04 17:24:23,091 - INFO - Epoch [1/10], Batch [0/747], Loss: 0.7060
2025-04-04 17:24:23,734 - INFO - Epoch [1/10], Batch [10/747], Loss: 0.6782
2025-04-04 17:24:24,363 - INFO - Epoch [1/10], Batch [20/747], Loss: 0.6572
2025-04-04 17:24:24,990 - INFO - Epoch [1/10], Batch [30/747], Loss: 0.6733
2025-04-04 17:24:25,618 - INFO - Epoch [1/10], Batch [40/747], Loss: 0.7845
2025-04-04 17:24:26,246 - INFO - Epoch [1/10], Batch [50/747], Loss: 0.6250
2025-04-04 17:24:26,875 - INFO - Epoch [1/10], Batch [60/747], Loss: 0.6811
2025-04-04 17:24:27,504 - INFO - Epoch [1/10], Batch [70/747], Loss: 0.6571
2025-04-04 17:24:28,131 - INFO - Epoch [1/10], Batch [80/747], Loss: 0.6283
2025-04-04 17:24:28,760 - INFO - Epoch [1/10], Batch [90/747], Loss: 0.5926
2025-04-04 17:24:29,388 - INFO - Epoch [1/10], Batch [100/747], Loss: 0.8469
2025-04-04 17:24:30,017 - INFO - Epoch [1/10], Batch [110/747], Loss: 0.6862
2025-04-04 17:24:30,644 - INFO - Epoch [1/10], Batch [120/747], Loss: 0.7407
2025-04-04 17:24:31,272 - INFO - Epoch [1/10], Batch [130/747], Loss: 0.6015
2025-04-04 17:24:31,900 - INFO - Epoch [1/10], Batch [140/747], Loss: 0.6741
2025-04-04 17:24:32,527 - INFO - Epoch [1/10], Batch [150/747], Loss: 0.8103
2025-04-04 17:24:33,156 - INFO - Epoch [1/10], Batch [160/747], Loss: 0.6812
2025-04-04 17:24:33,784 - INFO - Epoch [1/10], Batch [170/747], Loss: 0.6571
2025-04-04 17:24:34,413 - INFO - Epoch [1/10], Batch [180/747], Loss: 0.6287
2025-04-04 17:24:35,041 - INFO - Epoch [1/10], Batch [190/747], Loss: 0.5913
2025-04-04 17:24:35,670 - INFO - Epoch [1/10], Batch [200/747], Loss: 0.7040
2025-04-04 17:24:36,299 - INFO - Epoch [1/10], Batch [210/747], Loss: 0.7034
2025-04-04 17:24:36,927 - INFO - Epoch [1/10], Batch [220/747], Loss: 0.7919
2025-04-04 17:24:37,554 - INFO - Epoch [1/10], Batch [230/747], Loss: 0.7865
2025-04-04 17:24:38,182 - INFO - Epoch [1/10], Batch [240/747], Loss: 0.7659
2025-04-04 17:24:38,810 - INFO - Epoch [1/10], Batch [250/747], Loss: 0.6654
2025-04-04 17:24:39,439 - INFO - Epoch [1/10], Batch [260/747], Loss: 0.5969
2025-04-04 17:24:40,068 - INFO - Epoch [1/10], Batch [270/747], Loss: 0.6587
2025-04-04 17:24:40,696 - INFO - Epoch [1/10], Batch [280/747], Loss: 0.6060
2025-04-04 17:24:41,325 - INFO - Epoch [1/10], Batch [290/747], Loss: 0.5818
2025-04-04 17:24:41,953 - INFO - Epoch [1/10], Batch [300/747], Loss: 0.5297
2025-04-04 17:24:42,582 - INFO - Epoch [1/10], Batch [310/747], Loss: 0.6114
2025-04-04 17:24:43,211 - INFO - Epoch [1/10], Batch [320/747], Loss: 0.6118
2025-04-04 17:24:43,841 - INFO - Epoch [1/10], Batch [330/747], Loss: 0.5466
2025-04-04 17:24:44,469 - INFO - Epoch [1/10], Batch [340/747], Loss: 0.6284
2025-04-04 17:24:45,098 - INFO - Epoch [1/10], Batch [350/747], Loss: 0.6079
2025-04-04 17:24:45,726 - INFO - Epoch [1/10], Batch [360/747], Loss: 0.6654
2025-04-04 17:24:46,355 - INFO - Epoch [1/10], Batch [370/747], Loss: 0.5950
2025-04-04 17:24:46,983 - INFO - Epoch [1/10], Batch [380/747], Loss: 0.6385
2025-04-04 17:24:47,613 - INFO - Epoch [1/10], Batch [390/747], Loss: 0.6000
2025-04-04 17:24:48,241 - INFO - Epoch [1/10], Batch [400/747], Loss: 0.5650
2025-04-04 17:24:48,872 - INFO - Epoch [1/10], Batch [410/747], Loss: 0.6269
2025-04-04 17:24:49,500 - INFO - Epoch [1/10], Batch [420/747], Loss: 0.6334
2025-04-04 17:24:50,129 - INFO - Epoch [1/10], Batch [430/747], Loss: 0.6473
2025-04-04 17:24:50,758 - INFO - Epoch [1/10], Batch [440/747], Loss: 0.6871
2025-04-04 17:24:51,386 - INFO - Epoch [1/10], Batch [450/747], Loss: 0.6466
2025-04-04 17:24:52,015 - INFO - Epoch [1/10], Batch [460/747], Loss: 0.7484
2025-04-04 17:24:52,644 - INFO - Epoch [1/10], Batch [470/747], Loss: 0.9463
2025-04-04 17:24:53,272 - INFO - Epoch [1/10], Batch [480/747], Loss: 0.7318
2025-04-04 17:24:53,900 - INFO - Epoch [1/10], Batch [490/747], Loss: 0.5511
2025-04-04 17:24:54,529 - INFO - Epoch [1/10], Batch [500/747], Loss: 0.7787
2025-04-04 17:24:55,158 - INFO - Epoch [1/10], Batch [510/747], Loss: 0.5949
2025-04-04 17:24:55,787 - INFO - Epoch [1/10], Batch [520/747], Loss: 0.6607
2025-04-04 17:24:56,416 - INFO - Epoch [1/10], Batch [530/747], Loss: 0.6510
2025-04-04 17:24:57,045 - INFO - Epoch [1/10], Batch [540/747], Loss: 0.6355
2025-04-04 17:24:57,673 - INFO - Epoch [1/10], Batch [550/747], Loss: 0.6544
2025-04-04 17:24:58,303 - INFO - Epoch [1/10], Batch [560/747], Loss: 0.6834
2025-04-04 17:24:58,932 - INFO - Epoch [1/10], Batch [570/747], Loss: 0.5472
2025-04-04 17:24:59,561 - INFO - Epoch [1/10], Batch [580/747], Loss: 0.6686
2025-04-04 17:25:00,189 - INFO - Epoch [1/10], Batch [590/747], Loss: 0.6011
2025-04-04 17:25:00,818 - INFO - Epoch [1/10], Batch [600/747], Loss: 0.6840
2025-04-04 17:25:01,445 - INFO - Epoch [1/10], Batch [610/747], Loss: 0.6280
2025-04-04 17:25:02,075 - INFO - Epoch [1/10], Batch [620/747], Loss: 0.6315
2025-04-04 17:25:02,705 - INFO - Epoch [1/10], Batch [630/747], Loss: 0.6859
2025-04-04 17:25:03,334 - INFO - Epoch [1/10], Batch [640/747], Loss: 0.5918
2025-04-04 17:25:03,963 - INFO - Epoch [1/10], Batch [650/747], Loss: 0.6537
2025-04-04 17:25:04,592 - INFO - Epoch [1/10], Batch [660/747], Loss: 0.5763
2025-04-04 17:25:05,220 - INFO - Epoch [1/10], Batch [670/747], Loss: 0.5168
2025-04-04 17:25:05,851 - INFO - Epoch [1/10], Batch [680/747], Loss: 0.6578
2025-04-04 17:25:06,480 - INFO - Epoch [1/10], Batch [690/747], Loss: 0.6221
2025-04-04 17:25:07,108 - INFO - Epoch [1/10], Batch [700/747], Loss: 0.6131
2025-04-04 17:25:07,737 - INFO - Epoch [1/10], Batch [710/747], Loss: 0.6205
2025-04-04 17:25:08,365 - INFO - Epoch [1/10], Batch [720/747], Loss: 0.6832
2025-04-04 17:25:08,994 - INFO - Epoch [1/10], Batch [730/747], Loss: 0.6863
2025-04-04 17:25:09,624 - INFO - Epoch [1/10], Batch [740/747], Loss: 0.6111
2025-04-04 17:25:10,029 - INFO - Epoch 1/10 Train Loss: 0.6519, Train Accuracy: 61.03%
2025-04-04 17:25:13,742 - INFO - Epoch 1/10 Val Loss: 0.5802, Val Accuracy: 71.79%, AUC-ROC: 0.7192
2025-04-04 17:25:13,745 - INFO - New best model at epoch 1 with Val Accuracy: 71.79%
2025-04-04 17:25:14,219 - INFO - Epoch [2/10], Batch [0/747], Loss: 0.5819
2025-04-04 17:25:14,848 - INFO - Epoch [2/10], Batch [10/747], Loss: 0.5811
2025-04-04 17:25:15,477 - INFO - Epoch [2/10], Batch [20/747], Loss: 0.7407
2025-04-04 17:25:16,106 - INFO - Epoch [2/10], Batch [30/747], Loss: 0.6343
2025-04-04 17:25:16,734 - INFO - Epoch [2/10], Batch [40/747], Loss: 0.6186
2025-04-04 17:25:17,363 - INFO - Epoch [2/10], Batch [50/747], Loss: 0.5252
2025-04-04 17:25:17,992 - INFO - Epoch [2/10], Batch [60/747], Loss: 0.5114
2025-04-04 17:25:18,621 - INFO - Epoch [2/10], Batch [70/747], Loss: 0.5430
2025-04-04 17:25:19,249 - INFO - Epoch [2/10], Batch [80/747], Loss: 0.5495
2025-04-04 17:25:19,878 - INFO - Epoch [2/10], Batch [90/747], Loss: 0.7146
2025-04-04 17:25:20,506 - INFO - Epoch [2/10], Batch [100/747], Loss: 0.6458
2025-04-04 17:25:21,135 - INFO - Epoch [2/10], Batch [110/747], Loss: 0.5368
2025-04-04 17:25:21,764 - INFO - Epoch [2/10], Batch [120/747], Loss: 0.5436
2025-04-04 17:25:22,393 - INFO - Epoch [2/10], Batch [130/747], Loss: 0.6449
2025-04-04 17:25:23,021 - INFO - Epoch [2/10], Batch [140/747], Loss: 0.6655
2025-04-04 17:25:23,650 - INFO - Epoch [2/10], Batch [150/747], Loss: 0.6190
2025-04-04 17:25:24,278 - INFO - Epoch [2/10], Batch [160/747], Loss: 0.5515
2025-04-04 17:25:24,907 - INFO - Epoch [2/10], Batch [170/747], Loss: 0.8070
2025-04-04 17:25:25,535 - INFO - Epoch [2/10], Batch [180/747], Loss: 0.6969
2025-04-04 17:25:26,165 - INFO - Epoch [2/10], Batch [190/747], Loss: 0.5389
2025-04-04 17:25:26,794 - INFO - Epoch [2/10], Batch [200/747], Loss: 0.7544
2025-04-04 17:25:27,424 - INFO - Epoch [2/10], Batch [210/747], Loss: 0.7978
2025-04-04 17:25:28,053 - INFO - Epoch [2/10], Batch [220/747], Loss: 0.7001
2025-04-04 17:25:28,682 - INFO - Epoch [2/10], Batch [230/747], Loss: 0.5971
2025-04-04 17:25:29,312 - INFO - Epoch [2/10], Batch [240/747], Loss: 0.7901
2025-04-04 17:25:29,941 - INFO - Epoch [2/10], Batch [250/747], Loss: 0.6021
2025-04-04 17:25:30,570 - INFO - Epoch [2/10], Batch [260/747], Loss: 0.5974
2025-04-04 17:25:31,199 - INFO - Epoch [2/10], Batch [270/747], Loss: 0.5916
2025-04-04 17:25:31,828 - INFO - Epoch [2/10], Batch [280/747], Loss: 0.6251
2025-04-04 17:25:32,458 - INFO - Epoch [2/10], Batch [290/747], Loss: 0.5297
2025-04-04 17:25:33,087 - INFO - Epoch [2/10], Batch [300/747], Loss: 0.7531
2025-04-04 17:25:33,716 - INFO - Epoch [2/10], Batch [310/747], Loss: 0.5581
2025-04-04 17:25:34,346 - INFO - Epoch [2/10], Batch [320/747], Loss: 0.4551
2025-04-04 17:25:34,975 - INFO - Epoch [2/10], Batch [330/747], Loss: 0.7788
2025-04-04 17:25:35,604 - INFO - Epoch [2/10], Batch [340/747], Loss: 0.5950
2025-04-04 17:25:36,234 - INFO - Epoch [2/10], Batch [350/747], Loss: 0.5862
2025-04-04 17:25:36,863 - INFO - Epoch [2/10], Batch [360/747], Loss: 0.6983
2025-04-04 17:25:37,493 - INFO - Epoch [2/10], Batch [370/747], Loss: 0.5244
2025-04-04 17:25:38,122 - INFO - Epoch [2/10], Batch [380/747], Loss: 0.6249
2025-04-04 17:25:38,752 - INFO - Epoch [2/10], Batch [390/747], Loss: 0.5066
2025-04-04 17:25:39,381 - INFO - Epoch [2/10], Batch [400/747], Loss: 0.5513
2025-04-04 17:25:40,010 - INFO - Epoch [2/10], Batch [410/747], Loss: 0.6496
2025-04-04 17:25:40,639 - INFO - Epoch [2/10], Batch [420/747], Loss: 0.7258
2025-04-04 17:25:41,268 - INFO - Epoch [2/10], Batch [430/747], Loss: 0.4812
2025-04-04 17:25:41,897 - INFO - Epoch [2/10], Batch [440/747], Loss: 0.6122
2025-04-04 17:25:42,526 - INFO - Epoch [2/10], Batch [450/747], Loss: 0.5976
2025-04-04 17:25:43,155 - INFO - Epoch [2/10], Batch [460/747], Loss: 0.5099
2025-04-04 17:25:43,785 - INFO - Epoch [2/10], Batch [470/747], Loss: 0.6446
2025-04-04 17:25:44,414 - INFO - Epoch [2/10], Batch [480/747], Loss: 0.7451
2025-04-04 17:25:45,043 - INFO - Epoch [2/10], Batch [490/747], Loss: 0.5767
2025-04-04 17:25:45,673 - INFO - Epoch [2/10], Batch [500/747], Loss: 0.6719
2025-04-04 17:25:46,302 - INFO - Epoch [2/10], Batch [510/747], Loss: 0.5286
2025-04-04 17:25:46,932 - INFO - Epoch [2/10], Batch [520/747], Loss: 0.5694
2025-04-04 17:25:47,561 - INFO - Epoch [2/10], Batch [530/747], Loss: 0.5610
2025-04-04 17:25:48,191 - INFO - Epoch [2/10], Batch [540/747], Loss: 0.5668
2025-04-04 17:25:48,820 - INFO - Epoch [2/10], Batch [550/747], Loss: 0.4995
2025-04-04 17:25:49,450 - INFO - Epoch [2/10], Batch [560/747], Loss: 0.8863
2025-04-04 17:25:50,079 - INFO - Epoch [2/10], Batch [570/747], Loss: 0.6054
2025-04-04 17:25:50,708 - INFO - Epoch [2/10], Batch [580/747], Loss: 0.7605
2025-04-04 17:25:51,338 - INFO - Epoch [2/10], Batch [590/747], Loss: 0.7048
2025-04-04 17:25:51,967 - INFO - Epoch [2/10], Batch [600/747], Loss: 0.5831
2025-04-04 17:25:52,596 - INFO - Epoch [2/10], Batch [610/747], Loss: 0.4438
2025-04-04 17:25:53,226 - INFO - Epoch [2/10], Batch [620/747], Loss: 0.7270
2025-04-04 17:25:53,855 - INFO - Epoch [2/10], Batch [630/747], Loss: 0.5877
2025-04-04 17:25:54,484 - INFO - Epoch [2/10], Batch [640/747], Loss: 0.5457
2025-04-04 17:25:55,114 - INFO - Epoch [2/10], Batch [650/747], Loss: 0.5862
2025-04-04 17:25:55,743 - INFO - Epoch [2/10], Batch [660/747], Loss: 0.7365
2025-04-04 17:25:56,373 - INFO - Epoch [2/10], Batch [670/747], Loss: 0.6108
2025-04-04 17:25:57,002 - INFO - Epoch [2/10], Batch [680/747], Loss: 0.6280
2025-04-04 17:25:57,631 - INFO - Epoch [2/10], Batch [690/747], Loss: 0.5386
2025-04-04 17:25:58,261 - INFO - Epoch [2/10], Batch [700/747], Loss: 0.5990
2025-04-04 17:25:58,890 - INFO - Epoch [2/10], Batch [710/747], Loss: 0.5628
2025-04-04 17:25:59,519 - INFO - Epoch [2/10], Batch [720/747], Loss: 0.5132
2025-04-04 17:26:00,149 - INFO - Epoch [2/10], Batch [730/747], Loss: 0.5156
2025-04-04 17:26:00,779 - INFO - Epoch [2/10], Batch [740/747], Loss: 0.5415
2025-04-04 17:26:01,177 - INFO - Epoch 2/10 Train Loss: 0.6136, Train Accuracy: 65.49%
2025-04-04 17:26:16,180 - INFO - Running on node: d1004
2025-04-04 17:26:16,180 - INFO - Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single GPU...
2025-04-04 17:26:16,186 - INFO - Loading data...
2025-04-04 17:26:43,057 - INFO - Data shapes: Train (23898, 224, 224, 3), Val (5747, 224, 224, 3), Test (2874, 224, 224, 3)
2025-04-04 17:26:43,114 - INFO - Using device: cuda
2025-04-04 17:26:43,720 - INFO - Model architecture: MedicalCNN
2025-04-04 17:26:43,721 - INFO - Total layers: 129
2025-04-04 17:26:43,721 - INFO - Total parameters: 22,494,274
2025-04-04 17:26:44,930 - INFO - Epoch [1/10], Batch [0/747], Loss: 0.7060
2025-04-04 17:26:45,573 - INFO - Epoch [1/10], Batch [10/747], Loss: 0.6762
2025-04-04 17:26:46,203 - INFO - Epoch [1/10], Batch [20/747], Loss: 0.7399
2025-04-04 17:26:46,833 - INFO - Epoch [1/10], Batch [30/747], Loss: 0.7481
2025-04-04 17:26:47,460 - INFO - Epoch [1/10], Batch [40/747], Loss: 0.6709
2025-04-04 17:26:48,090 - INFO - Epoch [1/10], Batch [50/747], Loss: 0.6425
2025-04-04 17:26:48,719 - INFO - Epoch [1/10], Batch [60/747], Loss: 0.6451
2025-04-04 17:26:49,349 - INFO - Epoch [1/10], Batch [70/747], Loss: 0.6676
2025-04-04 17:26:49,978 - INFO - Epoch [1/10], Batch [80/747], Loss: 0.5724
2025-04-04 17:26:50,609 - INFO - Epoch [1/10], Batch [90/747], Loss: 0.6042
2025-04-04 17:26:51,238 - INFO - Epoch [1/10], Batch [100/747], Loss: 0.6673
2025-04-04 17:26:51,867 - INFO - Epoch [1/10], Batch [110/747], Loss: 0.6507
2025-04-04 17:26:52,497 - INFO - Epoch [1/10], Batch [120/747], Loss: 0.6771
2025-04-04 17:26:53,126 - INFO - Epoch [1/10], Batch [130/747], Loss: 0.5774
2025-04-04 17:26:53,755 - INFO - Epoch [1/10], Batch [140/747], Loss: 0.6925
2025-04-04 17:26:54,384 - INFO - Epoch [1/10], Batch [150/747], Loss: 0.7941
2025-04-04 17:26:55,012 - INFO - Epoch [1/10], Batch [160/747], Loss: 0.6486
2025-04-04 17:26:55,642 - INFO - Epoch [1/10], Batch [170/747], Loss: 0.5632
2025-04-04 17:26:56,272 - INFO - Epoch [1/10], Batch [180/747], Loss: 0.6257
2025-04-04 17:26:56,901 - INFO - Epoch [1/10], Batch [190/747], Loss: 0.5918
2025-04-04 17:26:57,530 - INFO - Epoch [1/10], Batch [200/747], Loss: 0.6498
2025-04-04 17:26:58,158 - INFO - Epoch [1/10], Batch [210/747], Loss: 0.6398
2025-04-04 17:26:58,788 - INFO - Epoch [1/10], Batch [220/747], Loss: 0.6722
2025-04-04 17:26:59,417 - INFO - Epoch [1/10], Batch [230/747], Loss: 0.7163
2025-04-04 17:27:00,045 - INFO - Epoch [1/10], Batch [240/747], Loss: 0.7323
2025-04-04 17:27:00,675 - INFO - Epoch [1/10], Batch [250/747], Loss: 0.6578
2025-04-04 17:27:01,304 - INFO - Epoch [1/10], Batch [260/747], Loss: 0.6664
2025-04-04 17:27:01,944 - INFO - Epoch [1/10], Batch [270/747], Loss: 0.6716
2025-04-04 17:27:02,612 - INFO - Epoch [1/10], Batch [280/747], Loss: 0.7521
2025-04-04 17:27:03,241 - INFO - Epoch [1/10], Batch [290/747], Loss: 0.6132
2025-04-04 17:27:03,870 - INFO - Epoch [1/10], Batch [300/747], Loss: 0.5217
2025-04-04 17:27:04,499 - INFO - Epoch [1/10], Batch [310/747], Loss: 0.6916
2025-04-04 17:27:05,128 - INFO - Epoch [1/10], Batch [320/747], Loss: 0.6201
2025-04-04 17:27:05,756 - INFO - Epoch [1/10], Batch [330/747], Loss: 0.6029
2025-04-04 17:27:06,384 - INFO - Epoch [1/10], Batch [340/747], Loss: 0.6288
2025-04-04 17:27:07,015 - INFO - Epoch [1/10], Batch [350/747], Loss: 0.6296
2025-04-04 17:27:07,644 - INFO - Epoch [1/10], Batch [360/747], Loss: 0.6514
2025-04-04 17:27:08,273 - INFO - Epoch [1/10], Batch [370/747], Loss: 0.6760
2025-04-04 17:27:08,902 - INFO - Epoch [1/10], Batch [380/747], Loss: 0.5734
2025-04-04 17:27:09,531 - INFO - Epoch [1/10], Batch [390/747], Loss: 0.5895
2025-04-04 17:27:10,159 - INFO - Epoch [1/10], Batch [400/747], Loss: 0.5934
2025-04-04 17:27:10,787 - INFO - Epoch [1/10], Batch [410/747], Loss: 0.7140
2025-04-04 17:27:11,417 - INFO - Epoch [1/10], Batch [420/747], Loss: 0.6718
2025-04-04 17:27:12,046 - INFO - Epoch [1/10], Batch [430/747], Loss: 0.7312
2025-04-04 17:27:12,676 - INFO - Epoch [1/10], Batch [440/747], Loss: 0.7356
2025-04-04 17:27:13,304 - INFO - Epoch [1/10], Batch [450/747], Loss: 0.5939
2025-04-04 17:27:13,933 - INFO - Epoch [1/10], Batch [460/747], Loss: 0.6910
2025-04-04 17:27:14,564 - INFO - Epoch [1/10], Batch [470/747], Loss: 0.8246
2025-04-04 17:27:15,192 - INFO - Epoch [1/10], Batch [480/747], Loss: 0.7292
2025-04-04 17:27:15,821 - INFO - Epoch [1/10], Batch [490/747], Loss: 0.6103
2025-04-04 17:27:16,450 - INFO - Epoch [1/10], Batch [500/747], Loss: 0.8626
2025-04-04 17:27:17,079 - INFO - Epoch [1/10], Batch [510/747], Loss: 0.6151
2025-04-04 17:27:17,709 - INFO - Epoch [1/10], Batch [520/747], Loss: 0.8032
2025-04-04 17:27:18,340 - INFO - Epoch [1/10], Batch [530/747], Loss: 0.6685
2025-04-04 17:27:18,968 - INFO - Epoch [1/10], Batch [540/747], Loss: 0.6669
2025-04-04 17:27:19,598 - INFO - Epoch [1/10], Batch [550/747], Loss: 0.7102
2025-04-04 17:27:20,227 - INFO - Epoch [1/10], Batch [560/747], Loss: 0.7054
2025-04-04 17:27:20,855 - INFO - Epoch [1/10], Batch [570/747], Loss: 0.5790
2025-04-04 17:27:21,484 - INFO - Epoch [1/10], Batch [580/747], Loss: 0.8206
2025-04-04 17:27:22,114 - INFO - Epoch [1/10], Batch [590/747], Loss: 0.6242
2025-04-04 17:27:22,743 - INFO - Epoch [1/10], Batch [600/747], Loss: 0.6703
2025-04-04 17:27:23,374 - INFO - Epoch [1/10], Batch [610/747], Loss: 0.6693
2025-04-04 17:27:24,003 - INFO - Epoch [1/10], Batch [620/747], Loss: 0.6300
2025-04-04 17:27:24,633 - INFO - Epoch [1/10], Batch [630/747], Loss: 0.6488
2025-04-04 17:27:25,262 - INFO - Epoch [1/10], Batch [640/747], Loss: 0.6767
2025-04-04 17:27:25,891 - INFO - Epoch [1/10], Batch [650/747], Loss: 0.7537
2025-04-04 17:27:26,519 - INFO - Epoch [1/10], Batch [660/747], Loss: 0.5889
2025-04-04 17:27:27,149 - INFO - Epoch [1/10], Batch [670/747], Loss: 0.6136
2025-04-04 17:27:27,778 - INFO - Epoch [1/10], Batch [680/747], Loss: 0.6173
2025-04-04 17:27:28,407 - INFO - Epoch [1/10], Batch [690/747], Loss: 0.6496
2025-04-04 17:27:29,036 - INFO - Epoch [1/10], Batch [700/747], Loss: 0.6680
2025-04-04 17:27:29,665 - INFO - Epoch [1/10], Batch [710/747], Loss: 0.5958
2025-04-04 17:27:30,293 - INFO - Epoch [1/10], Batch [720/747], Loss: 0.7212
2025-04-04 17:27:30,922 - INFO - Epoch [1/10], Batch [730/747], Loss: 0.6394
2025-04-04 17:27:31,552 - INFO - Epoch [1/10], Batch [740/747], Loss: 0.6195
2025-04-04 17:27:31,955 - INFO - Epoch 1/10 Train Loss: 0.6564, Train Accuracy: 60.49%
2025-04-04 17:27:35,688 - INFO - Epoch 1/10 Val Loss: 0.5789, Val Accuracy: 71.81%, AUC-ROC: 0.7365
2025-04-04 17:27:35,691 - INFO - New best model at epoch 1 with Val Accuracy: 71.81%
2025-04-04 17:27:36,165 - INFO - Epoch [2/10], Batch [0/747], Loss: 0.6200
2025-04-04 17:27:36,796 - INFO - Epoch [2/10], Batch [10/747], Loss: 0.5701
2025-04-04 17:27:37,427 - INFO - Epoch [2/10], Batch [20/747], Loss: 0.7462
2025-04-04 17:27:38,058 - INFO - Epoch [2/10], Batch [30/747], Loss: 0.6496
2025-04-04 17:27:38,688 - INFO - Epoch [2/10], Batch [40/747], Loss: 0.5964
2025-04-04 17:27:39,318 - INFO - Epoch [2/10], Batch [50/747], Loss: 0.5435
2025-04-04 17:27:39,949 - INFO - Epoch [2/10], Batch [60/747], Loss: 0.5325
2025-04-04 17:27:40,580 - INFO - Epoch [2/10], Batch [70/747], Loss: 0.5735
2025-04-04 17:27:41,209 - INFO - Epoch [2/10], Batch [80/747], Loss: 0.5560
2025-04-04 17:27:41,840 - INFO - Epoch [2/10], Batch [90/747], Loss: 0.7285
2025-04-04 17:27:42,470 - INFO - Epoch [2/10], Batch [100/747], Loss: 0.6377
2025-04-04 17:27:43,100 - INFO - Epoch [2/10], Batch [110/747], Loss: 0.6602
2025-04-04 17:27:43,731 - INFO - Epoch [2/10], Batch [120/747], Loss: 0.5613
2025-04-04 17:27:44,361 - INFO - Epoch [2/10], Batch [130/747], Loss: 0.6282
2025-04-04 17:27:44,991 - INFO - Epoch [2/10], Batch [140/747], Loss: 0.7047
2025-04-04 17:27:45,621 - INFO - Epoch [2/10], Batch [150/747], Loss: 0.5969
2025-04-04 17:27:46,251 - INFO - Epoch [2/10], Batch [160/747], Loss: 0.6182
2025-04-04 17:27:46,881 - INFO - Epoch [2/10], Batch [170/747], Loss: 0.7419
2025-04-04 17:27:47,512 - INFO - Epoch [2/10], Batch [180/747], Loss: 0.6049
2025-04-04 17:27:48,141 - INFO - Epoch [2/10], Batch [190/747], Loss: 0.5126
2025-04-04 17:27:48,773 - INFO - Epoch [2/10], Batch [200/747], Loss: 0.7419
2025-04-04 17:27:49,403 - INFO - Epoch [2/10], Batch [210/747], Loss: 0.6843
2025-04-04 17:27:50,033 - INFO - Epoch [2/10], Batch [220/747], Loss: 0.6438
2025-04-04 17:27:50,662 - INFO - Epoch [2/10], Batch [230/747], Loss: 0.5507
2025-04-04 17:27:51,293 - INFO - Epoch [2/10], Batch [240/747], Loss: 0.6761
2025-04-04 17:27:51,925 - INFO - Epoch [2/10], Batch [250/747], Loss: 0.5984
2025-04-04 17:27:52,555 - INFO - Epoch [2/10], Batch [260/747], Loss: 0.6136
2025-04-04 17:27:53,185 - INFO - Epoch [2/10], Batch [270/747], Loss: 0.6162
2025-04-04 17:27:53,816 - INFO - Epoch [2/10], Batch [280/747], Loss: 0.6096
2025-04-04 17:27:54,448 - INFO - Epoch [2/10], Batch [290/747], Loss: 0.5202
2025-04-04 17:27:55,078 - INFO - Epoch [2/10], Batch [300/747], Loss: 0.6578
2025-04-04 17:27:55,708 - INFO - Epoch [2/10], Batch [310/747], Loss: 0.4904
2025-04-04 17:27:56,339 - INFO - Epoch [2/10], Batch [320/747], Loss: 0.5042
2025-04-04 17:27:56,969 - INFO - Epoch [2/10], Batch [330/747], Loss: 0.7443
2025-04-04 17:27:57,600 - INFO - Epoch [2/10], Batch [340/747], Loss: 0.6425
2025-04-04 17:27:58,232 - INFO - Epoch [2/10], Batch [350/747], Loss: 0.5827
2025-04-04 17:27:58,863 - INFO - Epoch [2/10], Batch [360/747], Loss: 0.6976
2025-04-04 17:27:59,494 - INFO - Epoch [2/10], Batch [370/747], Loss: 0.5392
2025-04-04 17:28:00,125 - INFO - Epoch [2/10], Batch [380/747], Loss: 0.6237
2025-04-04 17:28:00,755 - INFO - Epoch [2/10], Batch [390/747], Loss: 0.5131
2025-04-04 17:28:01,387 - INFO - Epoch [2/10], Batch [400/747], Loss: 0.5678
2025-04-04 17:28:02,016 - INFO - Epoch [2/10], Batch [410/747], Loss: 0.6779
2025-04-04 17:28:02,646 - INFO - Epoch [2/10], Batch [420/747], Loss: 0.7438
2025-04-04 17:28:03,275 - INFO - Epoch [2/10], Batch [430/747], Loss: 0.5240
2025-04-04 17:28:03,905 - INFO - Epoch [2/10], Batch [440/747], Loss: 0.6227
2025-04-04 17:28:04,535 - INFO - Epoch [2/10], Batch [450/747], Loss: 0.6075
2025-04-04 17:28:05,166 - INFO - Epoch [2/10], Batch [460/747], Loss: 0.4230
2025-04-04 17:28:05,797 - INFO - Epoch [2/10], Batch [470/747], Loss: 0.6746
2025-04-04 17:28:06,427 - INFO - Epoch [2/10], Batch [480/747], Loss: 0.7482
2025-04-04 17:28:07,058 - INFO - Epoch [2/10], Batch [490/747], Loss: 0.6168
2025-04-04 17:28:07,688 - INFO - Epoch [2/10], Batch [500/747], Loss: 0.5529
2025-04-04 17:28:08,319 - INFO - Epoch [2/10], Batch [510/747], Loss: 0.5678
2025-04-04 17:28:08,950 - INFO - Epoch [2/10], Batch [520/747], Loss: 0.5322
2025-04-04 17:28:09,580 - INFO - Epoch [2/10], Batch [530/747], Loss: 0.6180
2025-04-04 17:28:10,212 - INFO - Epoch [2/10], Batch [540/747], Loss: 0.5846
2025-04-04 17:28:10,842 - INFO - Epoch [2/10], Batch [550/747], Loss: 0.6215
2025-04-04 17:28:11,474 - INFO - Epoch [2/10], Batch [560/747], Loss: 0.7978
2025-04-04 17:28:12,105 - INFO - Epoch [2/10], Batch [570/747], Loss: 0.6316
2025-04-04 17:28:12,736 - INFO - Epoch [2/10], Batch [580/747], Loss: 0.6627
2025-04-04 17:28:13,366 - INFO - Epoch [2/10], Batch [590/747], Loss: 0.5517
2025-04-04 17:28:13,997 - INFO - Epoch [2/10], Batch [600/747], Loss: 0.5612
2025-04-04 17:28:14,629 - INFO - Epoch [2/10], Batch [610/747], Loss: 0.4061
2025-04-04 17:28:15,260 - INFO - Epoch [2/10], Batch [620/747], Loss: 0.7108
2025-04-04 17:28:15,889 - INFO - Epoch [2/10], Batch [630/747], Loss: 0.5649
2025-04-04 17:28:16,520 - INFO - Epoch [2/10], Batch [640/747], Loss: 0.5352
2025-04-04 17:28:17,150 - INFO - Epoch [2/10], Batch [650/747], Loss: 0.5398
2025-04-04 17:28:17,780 - INFO - Epoch [2/10], Batch [660/747], Loss: 0.7270
2025-04-04 17:28:18,410 - INFO - Epoch [2/10], Batch [670/747], Loss: 0.5722
2025-04-04 17:28:19,041 - INFO - Epoch [2/10], Batch [680/747], Loss: 0.6567
2025-04-04 17:28:19,671 - INFO - Epoch [2/10], Batch [690/747], Loss: 0.5576
2025-04-04 17:28:20,302 - INFO - Epoch [2/10], Batch [700/747], Loss: 0.5882
2025-04-04 17:28:20,931 - INFO - Epoch [2/10], Batch [710/747], Loss: 0.5732
2025-04-04 17:28:21,561 - INFO - Epoch [2/10], Batch [720/747], Loss: 0.4886
2025-04-04 17:28:22,191 - INFO - Epoch [2/10], Batch [730/747], Loss: 0.5633
2025-04-04 17:28:22,821 - INFO - Epoch [2/10], Batch [740/747], Loss: 0.5948
2025-04-04 17:28:23,233 - INFO - Epoch 2/10 Train Loss: 0.6178, Train Accuracy: 65.90%
2025-04-04 17:28:26,935 - INFO - Epoch 2/10 Val Loss: 0.6058, Val Accuracy: 70.87%, AUC-ROC: 0.7593
2025-04-04 17:28:27,411 - INFO - Epoch [3/10], Batch [0/747], Loss: 0.5155
2025-04-04 17:28:28,041 - INFO - Epoch [3/10], Batch [10/747], Loss: 0.6646
2025-04-04 17:28:28,672 - INFO - Epoch [3/10], Batch [20/747], Loss: 0.5879
2025-04-04 17:28:29,301 - INFO - Epoch [3/10], Batch [30/747], Loss: 0.6520
2025-04-04 17:28:29,931 - INFO - Epoch [3/10], Batch [40/747], Loss: 0.8039
2025-04-04 17:28:30,561 - INFO - Epoch [3/10], Batch [50/747], Loss: 0.7999
2025-04-04 17:28:31,193 - INFO - Epoch [3/10], Batch [60/747], Loss: 0.4937
2025-04-04 17:28:31,822 - INFO - Epoch [3/10], Batch [70/747], Loss: 0.5657
2025-04-04 17:28:32,452 - INFO - Epoch [3/10], Batch [80/747], Loss: 0.6320
2025-04-04 17:28:33,082 - INFO - Epoch [3/10], Batch [90/747], Loss: 0.5598
2025-04-04 17:28:33,712 - INFO - Epoch [3/10], Batch [100/747], Loss: 0.7997
2025-04-04 17:28:34,342 - INFO - Epoch [3/10], Batch [110/747], Loss: 0.7436
2025-04-04 17:28:34,974 - INFO - Epoch [3/10], Batch [120/747], Loss: 0.4297
2025-04-04 17:28:35,605 - INFO - Epoch [3/10], Batch [130/747], Loss: 0.5961
2025-04-04 17:28:36,235 - INFO - Epoch [3/10], Batch [140/747], Loss: 0.6167
2025-04-04 17:28:36,867 - INFO - Epoch [3/10], Batch [150/747], Loss: 0.4091
2025-04-04 17:28:37,499 - INFO - Epoch [3/10], Batch [160/747], Loss: 0.5190
2025-04-04 17:28:38,128 - INFO - Epoch [3/10], Batch [170/747], Loss: 0.6049
2025-04-04 17:28:38,758 - INFO - Epoch [3/10], Batch [180/747], Loss: 0.6380
2025-04-04 17:28:39,388 - INFO - Epoch [3/10], Batch [190/747], Loss: 0.5150
2025-04-04 17:28:40,019 - INFO - Epoch [3/10], Batch [200/747], Loss: 0.6003
2025-04-04 17:28:40,648 - INFO - Epoch [3/10], Batch [210/747], Loss: 0.4893
2025-04-04 17:28:41,278 - INFO - Epoch [3/10], Batch [220/747], Loss: 0.5127
2025-04-04 17:28:41,910 - INFO - Epoch [3/10], Batch [230/747], Loss: 0.6463
2025-04-04 17:28:42,539 - INFO - Epoch [3/10], Batch [240/747], Loss: 0.5662
2025-04-04 17:28:43,169 - INFO - Epoch [3/10], Batch [250/747], Loss: 0.5642
2025-04-04 17:28:43,800 - INFO - Epoch [3/10], Batch [260/747], Loss: 0.4430
2025-04-04 17:28:44,429 - INFO - Epoch [3/10], Batch [270/747], Loss: 0.4896
2025-04-04 17:28:45,059 - INFO - Epoch [3/10], Batch [280/747], Loss: 0.6019
2025-04-04 17:28:45,689 - INFO - Epoch [3/10], Batch [290/747], Loss: 0.6771
2025-04-04 17:28:46,319 - INFO - Epoch [3/10], Batch [300/747], Loss: 0.6687
2025-04-04 17:28:46,949 - INFO - Epoch [3/10], Batch [310/747], Loss: 0.5743
2025-04-04 17:28:47,579 - INFO - Epoch [3/10], Batch [320/747], Loss: 0.5184
2025-04-04 17:28:48,209 - INFO - Epoch [3/10], Batch [330/747], Loss: 0.5984
2025-04-04 17:28:48,838 - INFO - Epoch [3/10], Batch [340/747], Loss: 0.6519
2025-04-04 17:28:49,468 - INFO - Epoch [3/10], Batch [350/747], Loss: 0.5384
2025-04-04 17:28:50,098 - INFO - Epoch [3/10], Batch [360/747], Loss: 0.7295
2025-04-04 17:28:50,728 - INFO - Epoch [3/10], Batch [370/747], Loss: 0.3891
2025-04-04 17:28:51,358 - INFO - Epoch [3/10], Batch [380/747], Loss: 0.6681
2025-04-04 17:28:51,988 - INFO - Epoch [3/10], Batch [390/747], Loss: 0.4285
2025-04-04 17:28:52,618 - INFO - Epoch [3/10], Batch [400/747], Loss: 0.4980
2025-04-04 17:28:53,248 - INFO - Epoch [3/10], Batch [410/747], Loss: 0.6386
2025-04-04 17:28:53,878 - INFO - Epoch [3/10], Batch [420/747], Loss: 0.4243
2025-04-04 17:28:54,508 - INFO - Epoch [3/10], Batch [430/747], Loss: 0.4590
2025-04-04 17:28:55,138 - INFO - Epoch [3/10], Batch [440/747], Loss: 0.5713
2025-04-04 17:28:55,769 - INFO - Epoch [3/10], Batch [450/747], Loss: 0.6144
2025-04-04 17:28:56,399 - INFO - Epoch [3/10], Batch [460/747], Loss: 0.5324
2025-04-04 17:28:57,030 - INFO - Epoch [3/10], Batch [470/747], Loss: 0.5160
2025-04-04 17:28:57,660 - INFO - Epoch [3/10], Batch [480/747], Loss: 0.6304
2025-04-04 17:28:58,290 - INFO - Epoch [3/10], Batch [490/747], Loss: 0.6687
2025-04-04 17:28:58,920 - INFO - Epoch [3/10], Batch [500/747], Loss: 0.4914
2025-04-04 17:28:59,550 - INFO - Epoch [3/10], Batch [510/747], Loss: 0.6527
2025-04-04 17:29:00,181 - INFO - Epoch [3/10], Batch [520/747], Loss: 0.4255
2025-04-04 17:29:00,811 - INFO - Epoch [3/10], Batch [530/747], Loss: 0.5867
2025-04-04 17:29:01,440 - INFO - Epoch [3/10], Batch [540/747], Loss: 0.5766
2025-04-04 17:29:02,072 - INFO - Epoch [3/10], Batch [550/747], Loss: 0.4543
2025-04-04 17:29:02,701 - INFO - Epoch [3/10], Batch [560/747], Loss: 0.5733
2025-04-04 17:29:03,330 - INFO - Epoch [3/10], Batch [570/747], Loss: 0.6090
2025-04-04 17:29:03,962 - INFO - Epoch [3/10], Batch [580/747], Loss: 0.5123
2025-04-04 17:29:04,592 - INFO - Epoch [3/10], Batch [590/747], Loss: 0.6876
2025-04-04 17:29:05,222 - INFO - Epoch [3/10], Batch [600/747], Loss: 0.4908
2025-04-04 17:29:05,852 - INFO - Epoch [3/10], Batch [610/747], Loss: 0.5659
2025-04-04 17:29:06,482 - INFO - Epoch [3/10], Batch [620/747], Loss: 0.4380
2025-04-04 17:29:07,112 - INFO - Epoch [3/10], Batch [630/747], Loss: 0.4352
2025-04-04 17:29:07,742 - INFO - Epoch [3/10], Batch [640/747], Loss: 0.4034
2025-04-04 17:29:08,372 - INFO - Epoch [3/10], Batch [650/747], Loss: 0.6636
2025-04-04 17:29:09,002 - INFO - Epoch [3/10], Batch [660/747], Loss: 0.4546
2025-04-04 17:29:09,633 - INFO - Epoch [3/10], Batch [670/747], Loss: 0.4936
2025-04-04 17:29:10,262 - INFO - Epoch [3/10], Batch [680/747], Loss: 0.5885
2025-04-04 17:29:10,892 - INFO - Epoch [3/10], Batch [690/747], Loss: 0.4527
2025-04-04 17:29:11,522 - INFO - Epoch [3/10], Batch [700/747], Loss: 0.4906
2025-04-04 17:29:12,152 - INFO - Epoch [3/10], Batch [710/747], Loss: 0.4177
2025-04-04 17:29:12,781 - INFO - Epoch [3/10], Batch [720/747], Loss: 0.6620
2025-04-04 17:29:13,412 - INFO - Epoch [3/10], Batch [730/747], Loss: 0.6671
2025-04-04 17:29:14,043 - INFO - Epoch [3/10], Batch [740/747], Loss: 0.7200
2025-04-04 17:29:14,450 - INFO - Epoch 3/10 Train Loss: 0.5613, Train Accuracy: 70.98%
2025-04-04 17:29:18,164 - INFO - Epoch 3/10 Val Loss: 0.5125, Val Accuracy: 76.20%, AUC-ROC: 0.8520
2025-04-04 17:29:18,168 - INFO - New best model at epoch 3 with Val Accuracy: 76.20%
2025-04-04 17:29:18,662 - INFO - Epoch [4/10], Batch [0/747], Loss: 0.5965
2025-04-04 17:29:19,293 - INFO - Epoch [4/10], Batch [10/747], Loss: 0.5724
2025-04-04 17:29:19,922 - INFO - Epoch [4/10], Batch [20/747], Loss: 0.4221
2025-04-04 17:29:20,552 - INFO - Epoch [4/10], Batch [30/747], Loss: 0.5469
2025-04-04 17:29:21,182 - INFO - Epoch [4/10], Batch [40/747], Loss: 0.4923
2025-04-04 17:29:21,812 - INFO - Epoch [4/10], Batch [50/747], Loss: 0.6392
2025-04-04 17:29:22,442 - INFO - Epoch [4/10], Batch [60/747], Loss: 0.4479
2025-04-04 17:29:23,071 - INFO - Epoch [4/10], Batch [70/747], Loss: 0.4417
2025-04-04 17:29:23,702 - INFO - Epoch [4/10], Batch [80/747], Loss: 0.4910
2025-04-04 17:29:24,332 - INFO - Epoch [4/10], Batch [90/747], Loss: 0.6170
2025-04-04 17:29:24,961 - INFO - Epoch [4/10], Batch [100/747], Loss: 0.4668
2025-04-04 17:29:25,591 - INFO - Epoch [4/10], Batch [110/747], Loss: 0.4872
2025-04-04 17:29:26,221 - INFO - Epoch [4/10], Batch [120/747], Loss: 0.3874
2025-04-04 17:29:26,851 - INFO - Epoch [4/10], Batch [130/747], Loss: 0.5485
2025-04-04 17:29:27,480 - INFO - Epoch [4/10], Batch [140/747], Loss: 0.4907
2025-04-04 17:29:28,110 - INFO - Epoch [4/10], Batch [150/747], Loss: 0.4670
2025-04-04 17:29:28,740 - INFO - Epoch [4/10], Batch [160/747], Loss: 0.4099
2025-04-04 17:29:29,370 - INFO - Epoch [4/10], Batch [170/747], Loss: 0.4264
2025-04-04 17:29:30,000 - INFO - Epoch [4/10], Batch [180/747], Loss: 0.4109
2025-04-04 17:29:30,629 - INFO - Epoch [4/10], Batch [190/747], Loss: 0.7164
2025-04-04 17:29:31,260 - INFO - Epoch [4/10], Batch [200/747], Loss: 0.7047
2025-04-04 17:29:31,891 - INFO - Epoch [4/10], Batch [210/747], Loss: 0.3785
2025-04-04 17:29:32,521 - INFO - Epoch [4/10], Batch [220/747], Loss: 0.4961
2025-04-04 17:29:33,152 - INFO - Epoch [4/10], Batch [230/747], Loss: 0.4339
2025-04-04 17:29:33,782 - INFO - Epoch [4/10], Batch [240/747], Loss: 0.4353
2025-04-04 17:29:34,413 - INFO - Epoch [4/10], Batch [250/747], Loss: 0.3764
2025-04-04 17:29:35,042 - INFO - Epoch [4/10], Batch [260/747], Loss: 0.5672
2025-04-04 17:29:35,672 - INFO - Epoch [4/10], Batch [270/747], Loss: 0.6864
2025-04-04 17:29:36,301 - INFO - Epoch [4/10], Batch [280/747], Loss: 0.4662
2025-04-04 17:29:36,932 - INFO - Epoch [4/10], Batch [290/747], Loss: 0.4676
2025-04-04 17:29:37,562 - INFO - Epoch [4/10], Batch [300/747], Loss: 0.6180
2025-04-04 17:29:38,192 - INFO - Epoch [4/10], Batch [310/747], Loss: 0.4535
2025-04-04 17:29:38,822 - INFO - Epoch [4/10], Batch [320/747], Loss: 0.5825
2025-04-04 17:29:39,452 - INFO - Epoch [4/10], Batch [330/747], Loss: 0.3816
2025-04-04 17:29:40,083 - INFO - Epoch [4/10], Batch [340/747], Loss: 0.6293
2025-04-04 17:29:40,714 - INFO - Epoch [4/10], Batch [350/747], Loss: 0.5000
2025-04-04 17:29:41,344 - INFO - Epoch [4/10], Batch [360/747], Loss: 0.4631
2025-04-04 17:29:41,974 - INFO - Epoch [4/10], Batch [370/747], Loss: 0.4434
2025-04-04 17:29:42,604 - INFO - Epoch [4/10], Batch [380/747], Loss: 0.4846
2025-04-04 17:29:43,235 - INFO - Epoch [4/10], Batch [390/747], Loss: 0.6205
2025-04-04 17:29:43,865 - INFO - Epoch [4/10], Batch [400/747], Loss: 0.5267
2025-04-04 17:29:44,495 - INFO - Epoch [4/10], Batch [410/747], Loss: 0.4995
2025-04-04 17:29:45,125 - INFO - Epoch [4/10], Batch [420/747], Loss: 0.7268
2025-04-04 17:29:45,755 - INFO - Epoch [4/10], Batch [430/747], Loss: 0.5845
2025-04-04 17:29:46,385 - INFO - Epoch [4/10], Batch [440/747], Loss: 0.4525
2025-04-04 17:29:47,015 - INFO - Epoch [4/10], Batch [450/747], Loss: 0.6399
2025-04-04 17:29:47,644 - INFO - Epoch [4/10], Batch [460/747], Loss: 0.5424
2025-04-04 17:29:48,274 - INFO - Epoch [4/10], Batch [470/747], Loss: 0.5010
2025-04-04 17:29:48,905 - INFO - Epoch [4/10], Batch [480/747], Loss: 0.5060
2025-04-04 17:29:49,535 - INFO - Epoch [4/10], Batch [490/747], Loss: 0.3094
2025-04-04 17:29:50,165 - INFO - Epoch [4/10], Batch [500/747], Loss: 0.7445
2025-04-04 17:29:50,795 - INFO - Epoch [4/10], Batch [510/747], Loss: 0.4860
2025-04-04 17:29:51,425 - INFO - Epoch [4/10], Batch [520/747], Loss: 0.4569
2025-04-04 17:29:52,055 - INFO - Epoch [4/10], Batch [530/747], Loss: 0.5912
2025-04-04 17:29:52,684 - INFO - Epoch [4/10], Batch [540/747], Loss: 0.4380
2025-04-04 17:29:53,315 - INFO - Epoch [4/10], Batch [550/747], Loss: 0.4170
2025-04-04 17:29:53,944 - INFO - Epoch [4/10], Batch [560/747], Loss: 0.6537
2025-04-04 17:29:54,574 - INFO - Epoch [4/10], Batch [570/747], Loss: 0.3536
2025-04-04 17:29:55,204 - INFO - Epoch [4/10], Batch [580/747], Loss: 0.3412
2025-04-04 17:29:55,834 - INFO - Epoch [4/10], Batch [590/747], Loss: 0.3707
2025-04-04 17:29:56,465 - INFO - Epoch [4/10], Batch [600/747], Loss: 0.2827
2025-04-04 17:29:57,094 - INFO - Epoch [4/10], Batch [610/747], Loss: 0.3267
2025-04-04 17:29:57,724 - INFO - Epoch [4/10], Batch [620/747], Loss: 0.3754
2025-04-04 17:29:58,354 - INFO - Epoch [4/10], Batch [630/747], Loss: 0.5928
2025-04-04 17:29:58,983 - INFO - Epoch [4/10], Batch [640/747], Loss: 0.4620
2025-04-04 17:29:59,613 - INFO - Epoch [4/10], Batch [650/747], Loss: 0.5080
2025-04-04 17:30:00,244 - INFO - Epoch [4/10], Batch [660/747], Loss: 0.3963
2025-04-04 17:30:00,875 - INFO - Epoch [4/10], Batch [670/747], Loss: 0.5754
2025-04-04 17:30:01,505 - INFO - Epoch [4/10], Batch [680/747], Loss: 0.4533
2025-04-04 17:30:02,134 - INFO - Epoch [4/10], Batch [690/747], Loss: 0.5033
2025-04-04 17:30:02,764 - INFO - Epoch [4/10], Batch [700/747], Loss: 0.4807
2025-04-04 17:30:03,393 - INFO - Epoch [4/10], Batch [710/747], Loss: 0.7182
2025-04-04 17:30:04,023 - INFO - Epoch [4/10], Batch [720/747], Loss: 0.3246
2025-04-04 17:30:04,652 - INFO - Epoch [4/10], Batch [730/747], Loss: 0.3435
2025-04-04 17:30:05,283 - INFO - Epoch [4/10], Batch [740/747], Loss: 0.5200
2025-04-04 17:30:05,691 - INFO - Epoch 4/10 Train Loss: 0.4873, Train Accuracy: 76.33%
2025-04-04 17:30:09,406 - INFO - Epoch 4/10 Val Loss: 0.4093, Val Accuracy: 81.85%, AUC-ROC: 0.8873
2025-04-04 17:30:09,409 - INFO - New best model at epoch 4 with Val Accuracy: 81.85%
2025-04-04 17:30:09,903 - INFO - Epoch [5/10], Batch [0/747], Loss: 0.4769
2025-04-04 17:30:10,533 - INFO - Epoch [5/10], Batch [10/747], Loss: 0.4854
2025-04-04 17:30:11,163 - INFO - Epoch [5/10], Batch [20/747], Loss: 0.3157
2025-04-04 17:30:11,793 - INFO - Epoch [5/10], Batch [30/747], Loss: 0.2914
2025-04-04 17:30:12,423 - INFO - Epoch [5/10], Batch [40/747], Loss: 0.6138
2025-04-04 17:30:13,053 - INFO - Epoch [5/10], Batch [50/747], Loss: 0.4790
2025-04-04 17:30:13,683 - INFO - Epoch [5/10], Batch [60/747], Loss: 0.5634
2025-04-04 17:30:14,313 - INFO - Epoch [5/10], Batch [70/747], Loss: 0.4417
2025-04-04 17:30:14,943 - INFO - Epoch [5/10], Batch [80/747], Loss: 0.4766
2025-04-04 17:30:15,574 - INFO - Epoch [5/10], Batch [90/747], Loss: 0.4318
2025-04-04 17:30:16,204 - INFO - Epoch [5/10], Batch [100/747], Loss: 0.4461
2025-04-04 17:30:16,834 - INFO - Epoch [5/10], Batch [110/747], Loss: 0.4361
2025-04-04 17:30:17,464 - INFO - Epoch [5/10], Batch [120/747], Loss: 0.3814
2025-04-04 17:30:18,094 - INFO - Epoch [5/10], Batch [130/747], Loss: 0.4133
2025-04-04 17:30:18,725 - INFO - Epoch [5/10], Batch [140/747], Loss: 0.6657
2025-04-04 17:30:19,355 - INFO - Epoch [5/10], Batch [150/747], Loss: 0.2780
2025-04-04 17:30:19,987 - INFO - Epoch [5/10], Batch [160/747], Loss: 0.5909
2025-04-04 17:30:20,618 - INFO - Epoch [5/10], Batch [170/747], Loss: 0.3661
2025-04-04 17:30:21,248 - INFO - Epoch [5/10], Batch [180/747], Loss: 0.5486
2025-04-04 17:30:21,878 - INFO - Epoch [5/10], Batch [190/747], Loss: 0.4977
2025-04-04 17:30:22,509 - INFO - Epoch [5/10], Batch [200/747], Loss: 0.5212
2025-04-04 17:30:23,138 - INFO - Epoch [5/10], Batch [210/747], Loss: 0.4189
2025-04-04 17:30:23,767 - INFO - Epoch [5/10], Batch [220/747], Loss: 0.4553
2025-04-04 17:30:24,397 - INFO - Epoch [5/10], Batch [230/747], Loss: 0.3519
2025-04-04 17:30:25,027 - INFO - Epoch [5/10], Batch [240/747], Loss: 0.4507
2025-04-04 17:30:25,658 - INFO - Epoch [5/10], Batch [250/747], Loss: 0.5833
2025-04-04 17:30:26,290 - INFO - Epoch [5/10], Batch [260/747], Loss: 0.2748
2025-04-04 17:30:26,920 - INFO - Epoch [5/10], Batch [270/747], Loss: 0.3609
2025-04-04 17:30:27,552 - INFO - Epoch [5/10], Batch [280/747], Loss: 0.3760
2025-04-04 17:30:28,182 - INFO - Epoch [5/10], Batch [290/747], Loss: 0.3644
2025-04-04 17:30:28,812 - INFO - Epoch [5/10], Batch [300/747], Loss: 0.4319
2025-04-04 17:30:29,442 - INFO - Epoch [5/10], Batch [310/747], Loss: 0.4852
2025-04-04 17:30:30,072 - INFO - Epoch [5/10], Batch [320/747], Loss: 0.6985
2025-04-04 17:30:30,702 - INFO - Epoch [5/10], Batch [330/747], Loss: 0.3558
2025-04-04 17:30:31,333 - INFO - Epoch [5/10], Batch [340/747], Loss: 0.5282
2025-04-04 17:30:31,963 - INFO - Epoch [5/10], Batch [350/747], Loss: 0.4134
2025-04-04 17:30:32,593 - INFO - Epoch [5/10], Batch [360/747], Loss: 0.4013
2025-04-04 17:30:33,224 - INFO - Epoch [5/10], Batch [370/747], Loss: 0.3355
2025-04-04 17:30:33,854 - INFO - Epoch [5/10], Batch [380/747], Loss: 0.3900
2025-04-04 17:30:34,484 - INFO - Epoch [5/10], Batch [390/747], Loss: 0.4251
2025-04-04 17:30:35,115 - INFO - Epoch [5/10], Batch [400/747], Loss: 0.5296
2025-04-04 17:30:35,745 - INFO - Epoch [5/10], Batch [410/747], Loss: 0.4584
2025-04-04 17:30:36,376 - INFO - Epoch [5/10], Batch [420/747], Loss: 0.4194
2025-04-04 17:30:37,006 - INFO - Epoch [5/10], Batch [430/747], Loss: 0.3425
2025-04-04 17:30:37,637 - INFO - Epoch [5/10], Batch [440/747], Loss: 0.3678
2025-04-04 17:30:38,267 - INFO - Epoch [5/10], Batch [450/747], Loss: 0.3263
2025-04-04 17:30:38,896 - INFO - Epoch [5/10], Batch [460/747], Loss: 0.4732
2025-04-04 17:30:39,527 - INFO - Epoch [5/10], Batch [470/747], Loss: 0.6138
2025-04-04 17:30:40,158 - INFO - Epoch [5/10], Batch [480/747], Loss: 0.2456
2025-04-04 17:30:40,789 - INFO - Epoch [5/10], Batch [490/747], Loss: 0.3252
2025-04-04 17:30:41,419 - INFO - Epoch [5/10], Batch [500/747], Loss: 0.5063
2025-04-04 17:30:42,050 - INFO - Epoch [5/10], Batch [510/747], Loss: 0.5715
2025-04-04 17:30:42,681 - INFO - Epoch [5/10], Batch [520/747], Loss: 0.6838
2025-04-04 17:30:43,311 - INFO - Epoch [5/10], Batch [530/747], Loss: 0.4408
2025-04-04 17:30:43,940 - INFO - Epoch [5/10], Batch [540/747], Loss: 0.3261
2025-04-04 17:30:44,570 - INFO - Epoch [5/10], Batch [550/747], Loss: 0.4428
2025-04-04 17:30:45,200 - INFO - Epoch [5/10], Batch [560/747], Loss: 0.4415
2025-04-04 17:30:45,829 - INFO - Epoch [5/10], Batch [570/747], Loss: 0.3611
2025-04-04 17:30:46,458 - INFO - Epoch [5/10], Batch [580/747], Loss: 0.3785
2025-04-04 17:30:47,088 - INFO - Epoch [5/10], Batch [590/747], Loss: 0.4172
2025-04-04 17:30:47,718 - INFO - Epoch [5/10], Batch [600/747], Loss: 0.5633
2025-04-04 17:30:48,348 - INFO - Epoch [5/10], Batch [610/747], Loss: 0.4205
2025-04-04 17:30:48,978 - INFO - Epoch [5/10], Batch [620/747], Loss: 0.4014
2025-04-04 17:30:49,608 - INFO - Epoch [5/10], Batch [630/747], Loss: 0.3278
2025-04-04 17:30:50,238 - INFO - Epoch [5/10], Batch [640/747], Loss: 0.4704
2025-04-04 17:30:50,868 - INFO - Epoch [5/10], Batch [650/747], Loss: 0.2923
2025-04-04 17:30:51,498 - INFO - Epoch [5/10], Batch [660/747], Loss: 0.4062
2025-04-04 17:30:52,128 - INFO - Epoch [5/10], Batch [670/747], Loss: 0.5082
2025-04-04 17:30:52,759 - INFO - Epoch [5/10], Batch [680/747], Loss: 0.3540
2025-04-04 17:30:53,390 - INFO - Epoch [5/10], Batch [690/747], Loss: 0.4682
2025-04-04 17:30:54,021 - INFO - Epoch [5/10], Batch [700/747], Loss: 0.3750
2025-04-04 17:30:54,651 - INFO - Epoch [5/10], Batch [710/747], Loss: 0.5851
2025-04-04 17:30:55,281 - INFO - Epoch [5/10], Batch [720/747], Loss: 0.3576
2025-04-04 17:30:55,913 - INFO - Epoch [5/10], Batch [730/747], Loss: 0.3017
2025-04-04 17:30:56,544 - INFO - Epoch [5/10], Batch [740/747], Loss: 0.3272
2025-04-04 17:30:56,951 - INFO - Epoch 5/10 Train Loss: 0.4343, Train Accuracy: 79.76%
2025-04-04 17:31:00,664 - INFO - Epoch 5/10 Val Loss: 0.3858, Val Accuracy: 84.37%, AUC-ROC: 0.9062
2025-04-04 17:31:00,667 - INFO - New best model at epoch 5 with Val Accuracy: 84.37%
2025-04-04 17:31:01,160 - INFO - Epoch [6/10], Batch [0/747], Loss: 0.3876
2025-04-04 17:31:01,790 - INFO - Epoch [6/10], Batch [10/747], Loss: 0.4216
2025-04-04 17:31:02,420 - INFO - Epoch [6/10], Batch [20/747], Loss: 0.2760
2025-04-04 17:31:03,051 - INFO - Epoch [6/10], Batch [30/747], Loss: 0.6726
2025-04-04 17:31:03,682 - INFO - Epoch [6/10], Batch [40/747], Loss: 0.2862
2025-04-04 17:31:04,312 - INFO - Epoch [6/10], Batch [50/747], Loss: 0.5116
2025-04-04 17:31:04,942 - INFO - Epoch [6/10], Batch [60/747], Loss: 0.2343
2025-04-04 17:31:05,571 - INFO - Epoch [6/10], Batch [70/747], Loss: 0.3696
2025-04-04 17:31:06,202 - INFO - Epoch [6/10], Batch [80/747], Loss: 0.5696
2025-04-04 17:31:06,831 - INFO - Epoch [6/10], Batch [90/747], Loss: 0.4072
2025-04-04 17:31:07,463 - INFO - Epoch [6/10], Batch [100/747], Loss: 0.3839
2025-04-04 17:31:08,093 - INFO - Epoch [6/10], Batch [110/747], Loss: 0.4044
2025-04-04 17:31:08,724 - INFO - Epoch [6/10], Batch [120/747], Loss: 0.5560
2025-04-04 17:31:09,354 - INFO - Epoch [6/10], Batch [130/747], Loss: 0.2985
2025-04-04 17:31:09,985 - INFO - Epoch [6/10], Batch [140/747], Loss: 0.4173
2025-04-04 17:31:10,616 - INFO - Epoch [6/10], Batch [150/747], Loss: 0.4779
2025-04-04 17:31:11,247 - INFO - Epoch [6/10], Batch [160/747], Loss: 0.2915
2025-04-04 17:31:11,878 - INFO - Epoch [6/10], Batch [170/747], Loss: 0.4459
2025-04-04 17:31:12,508 - INFO - Epoch [6/10], Batch [180/747], Loss: 0.2023
2025-04-04 17:31:13,139 - INFO - Epoch [6/10], Batch [190/747], Loss: 0.5155
2025-04-04 17:31:13,769 - INFO - Epoch [6/10], Batch [200/747], Loss: 0.4623
2025-04-04 17:31:14,398 - INFO - Epoch [6/10], Batch [210/747], Loss: 0.3559
2025-04-04 17:31:15,029 - INFO - Epoch [6/10], Batch [220/747], Loss: 0.2918
2025-04-04 17:31:15,660 - INFO - Epoch [6/10], Batch [230/747], Loss: 0.3831
2025-04-04 17:31:16,291 - INFO - Epoch [6/10], Batch [240/747], Loss: 0.4353
2025-04-04 17:31:16,921 - INFO - Epoch [6/10], Batch [250/747], Loss: 0.6269
2025-04-04 17:31:17,551 - INFO - Epoch [6/10], Batch [260/747], Loss: 0.2644
2025-04-04 17:31:18,183 - INFO - Epoch [6/10], Batch [270/747], Loss: 0.3949
2025-04-04 17:31:18,814 - INFO - Epoch [6/10], Batch [280/747], Loss: 0.3495
2025-04-04 17:31:19,444 - INFO - Epoch [6/10], Batch [290/747], Loss: 0.5010
2025-04-04 17:31:20,074 - INFO - Epoch [6/10], Batch [300/747], Loss: 0.3793
2025-04-04 17:31:20,704 - INFO - Epoch [6/10], Batch [310/747], Loss: 0.5464
2025-04-04 17:31:21,334 - INFO - Epoch [6/10], Batch [320/747], Loss: 0.2876
2025-04-04 17:31:21,964 - INFO - Epoch [6/10], Batch [330/747], Loss: 0.3988
2025-04-04 17:31:22,595 - INFO - Epoch [6/10], Batch [340/747], Loss: 0.5612
2025-04-04 17:31:23,225 - INFO - Epoch [6/10], Batch [350/747], Loss: 0.4615
2025-04-04 17:31:23,855 - INFO - Epoch [6/10], Batch [360/747], Loss: 0.3683
2025-04-04 17:31:24,485 - INFO - Epoch [6/10], Batch [370/747], Loss: 0.2800
2025-04-04 17:31:25,115 - INFO - Epoch [6/10], Batch [380/747], Loss: 0.4600
2025-04-04 17:31:25,746 - INFO - Epoch [6/10], Batch [390/747], Loss: 0.3631
2025-04-04 17:31:26,377 - INFO - Epoch [6/10], Batch [400/747], Loss: 0.4906
2025-04-04 17:31:27,007 - INFO - Epoch [6/10], Batch [410/747], Loss: 0.3994
2025-04-04 17:31:27,638 - INFO - Epoch [6/10], Batch [420/747], Loss: 0.2954
2025-04-04 17:31:28,268 - INFO - Epoch [6/10], Batch [430/747], Loss: 0.4251
2025-04-04 17:31:28,897 - INFO - Epoch [6/10], Batch [440/747], Loss: 0.3511
2025-04-04 17:31:29,527 - INFO - Epoch [6/10], Batch [450/747], Loss: 0.3250
2025-04-04 17:31:30,157 - INFO - Epoch [6/10], Batch [460/747], Loss: 0.6235
2025-04-04 17:31:30,787 - INFO - Epoch [6/10], Batch [470/747], Loss: 0.4831
2025-04-04 17:31:31,416 - INFO - Epoch [6/10], Batch [480/747], Loss: 0.3407
2025-04-04 17:31:32,046 - INFO - Epoch [6/10], Batch [490/747], Loss: 0.5008
2025-04-04 17:31:32,676 - INFO - Epoch [6/10], Batch [500/747], Loss: 0.3819
2025-04-04 17:31:33,306 - INFO - Epoch [6/10], Batch [510/747], Loss: 0.4911
2025-04-04 17:31:33,937 - INFO - Epoch [6/10], Batch [520/747], Loss: 0.4534
2025-04-04 17:31:34,566 - INFO - Epoch [6/10], Batch [530/747], Loss: 0.5403
2025-04-04 17:31:35,197 - INFO - Epoch [6/10], Batch [540/747], Loss: 0.3891
2025-04-04 17:31:35,828 - INFO - Epoch [6/10], Batch [550/747], Loss: 0.2485
2025-04-04 17:31:36,458 - INFO - Epoch [6/10], Batch [560/747], Loss: 0.2297
2025-04-04 17:31:37,088 - INFO - Epoch [6/10], Batch [570/747], Loss: 0.2588
2025-04-04 17:31:37,718 - INFO - Epoch [6/10], Batch [580/747], Loss: 0.4190
2025-04-04 17:31:38,349 - INFO - Epoch [6/10], Batch [590/747], Loss: 0.5464
2025-04-04 17:31:38,979 - INFO - Epoch [6/10], Batch [600/747], Loss: 0.2592
2025-04-04 17:31:39,610 - INFO - Epoch [6/10], Batch [610/747], Loss: 0.3269
2025-04-04 17:31:40,241 - INFO - Epoch [6/10], Batch [620/747], Loss: 0.5138
2025-04-04 17:31:40,871 - INFO - Epoch [6/10], Batch [630/747], Loss: 0.3141
2025-04-04 17:31:41,501 - INFO - Epoch [6/10], Batch [640/747], Loss: 0.4319
2025-04-04 17:31:42,132 - INFO - Epoch [6/10], Batch [650/747], Loss: 0.4128
2025-04-04 17:31:42,762 - INFO - Epoch [6/10], Batch [660/747], Loss: 0.3238
2025-04-04 17:31:43,393 - INFO - Epoch [6/10], Batch [670/747], Loss: 0.2470
2025-04-04 17:31:44,024 - INFO - Epoch [6/10], Batch [680/747], Loss: 0.4029
2025-04-04 17:31:44,655 - INFO - Epoch [6/10], Batch [690/747], Loss: 0.2704
2025-04-04 17:31:45,285 - INFO - Epoch [6/10], Batch [700/747], Loss: 0.5640
2025-04-04 17:31:45,915 - INFO - Epoch [6/10], Batch [710/747], Loss: 0.5211
2025-04-04 17:31:46,545 - INFO - Epoch [6/10], Batch [720/747], Loss: 0.4322
2025-04-04 17:31:47,177 - INFO - Epoch [6/10], Batch [730/747], Loss: 0.6698
2025-04-04 17:31:47,808 - INFO - Epoch [6/10], Batch [740/747], Loss: 0.5667
2025-04-04 17:31:48,213 - INFO - Epoch 6/10 Train Loss: 0.4060, Train Accuracy: 81.05%
2025-04-04 17:31:51,923 - INFO - Epoch 6/10 Val Loss: 0.3374, Val Accuracy: 85.59%, AUC-ROC: 0.9212
2025-04-04 17:31:51,927 - INFO - New best model at epoch 6 with Val Accuracy: 85.59%
2025-04-04 17:31:52,422 - INFO - Epoch [7/10], Batch [0/747], Loss: 0.2530
2025-04-04 17:31:53,053 - INFO - Epoch [7/10], Batch [10/747], Loss: 0.3063
2025-04-04 17:31:53,684 - INFO - Epoch [7/10], Batch [20/747], Loss: 0.4242
2025-04-04 17:31:54,314 - INFO - Epoch [7/10], Batch [30/747], Loss: 0.4055
2025-04-04 17:31:54,944 - INFO - Epoch [7/10], Batch [40/747], Loss: 0.2855
2025-04-04 17:31:55,575 - INFO - Epoch [7/10], Batch [50/747], Loss: 0.3079
2025-04-04 17:31:56,207 - INFO - Epoch [7/10], Batch [60/747], Loss: 0.3739
2025-04-04 17:31:56,840 - INFO - Epoch [7/10], Batch [70/747], Loss: 0.2373
2025-04-04 17:31:57,472 - INFO - Epoch [7/10], Batch [80/747], Loss: 0.4449
2025-04-04 17:31:58,103 - INFO - Epoch [7/10], Batch [90/747], Loss: 0.3352
2025-04-04 17:31:58,734 - INFO - Epoch [7/10], Batch [100/747], Loss: 0.3091
2025-04-04 17:31:59,365 - INFO - Epoch [7/10], Batch [110/747], Loss: 0.2719
2025-04-04 17:31:59,997 - INFO - Epoch [7/10], Batch [120/747], Loss: 0.3900
2025-04-04 17:32:00,629 - INFO - Epoch [7/10], Batch [130/747], Loss: 0.4448
2025-04-04 17:32:01,262 - INFO - Epoch [7/10], Batch [140/747], Loss: 0.4315
2025-04-04 17:32:01,893 - INFO - Epoch [7/10], Batch [150/747], Loss: 0.4758
2025-04-04 17:32:02,524 - INFO - Epoch [7/10], Batch [160/747], Loss: 0.4309
2025-04-04 17:32:03,155 - INFO - Epoch [7/10], Batch [170/747], Loss: 0.2740
2025-04-04 17:32:03,785 - INFO - Epoch [7/10], Batch [180/747], Loss: 0.4297
2025-04-04 17:32:04,416 - INFO - Epoch [7/10], Batch [190/747], Loss: 0.5227
2025-04-04 17:32:05,047 - INFO - Epoch [7/10], Batch [200/747], Loss: 0.4902
2025-04-04 17:32:05,679 - INFO - Epoch [7/10], Batch [210/747], Loss: 0.2858
2025-04-04 17:32:06,310 - INFO - Epoch [7/10], Batch [220/747], Loss: 0.4510
2025-04-04 17:32:06,941 - INFO - Epoch [7/10], Batch [230/747], Loss: 0.3159
2025-04-04 17:32:07,572 - INFO - Epoch [7/10], Batch [240/747], Loss: 0.2653
2025-04-04 17:32:08,203 - INFO - Epoch [7/10], Batch [250/747], Loss: 0.4732
2025-04-04 17:32:08,834 - INFO - Epoch [7/10], Batch [260/747], Loss: 0.4709
2025-04-04 17:32:09,465 - INFO - Epoch [7/10], Batch [270/747], Loss: 0.3195
2025-04-04 17:32:10,098 - INFO - Epoch [7/10], Batch [280/747], Loss: 0.3460
2025-04-04 17:32:10,729 - INFO - Epoch [7/10], Batch [290/747], Loss: 0.3215
2025-04-04 17:32:11,359 - INFO - Epoch [7/10], Batch [300/747], Loss: 0.3094
2025-04-04 17:32:11,992 - INFO - Epoch [7/10], Batch [310/747], Loss: 0.6705
2025-04-04 17:32:12,621 - INFO - Epoch [7/10], Batch [320/747], Loss: 0.3154
2025-04-04 17:32:13,254 - INFO - Epoch [7/10], Batch [330/747], Loss: 0.3076
2025-04-04 17:32:13,885 - INFO - Epoch [7/10], Batch [340/747], Loss: 0.2671
2025-04-04 17:32:14,516 - INFO - Epoch [7/10], Batch [350/747], Loss: 0.2924
2025-04-04 17:32:15,148 - INFO - Epoch [7/10], Batch [360/747], Loss: 0.3023
2025-04-04 17:32:15,779 - INFO - Epoch [7/10], Batch [370/747], Loss: 0.4734
2025-04-04 17:32:16,411 - INFO - Epoch [7/10], Batch [380/747], Loss: 0.2786
2025-04-04 17:32:17,042 - INFO - Epoch [7/10], Batch [390/747], Loss: 0.3445
2025-04-04 17:32:17,674 - INFO - Epoch [7/10], Batch [400/747], Loss: 0.4151
2025-04-04 17:32:18,304 - INFO - Epoch [7/10], Batch [410/747], Loss: 0.2715
2025-04-04 17:32:18,936 - INFO - Epoch [7/10], Batch [420/747], Loss: 0.3479
2025-04-04 17:32:19,568 - INFO - Epoch [7/10], Batch [430/747], Loss: 0.2533
2025-04-04 17:32:20,199 - INFO - Epoch [7/10], Batch [440/747], Loss: 0.3923
2025-04-04 17:32:20,828 - INFO - Epoch [7/10], Batch [450/747], Loss: 0.2728
2025-04-04 17:32:21,458 - INFO - Epoch [7/10], Batch [460/747], Loss: 0.3670
2025-04-04 17:32:22,090 - INFO - Epoch [7/10], Batch [470/747], Loss: 0.3838
2025-04-04 17:32:22,721 - INFO - Epoch [7/10], Batch [480/747], Loss: 0.4125
2025-04-04 17:32:23,352 - INFO - Epoch [7/10], Batch [490/747], Loss: 0.3848
2025-04-04 17:32:23,982 - INFO - Epoch [7/10], Batch [500/747], Loss: 0.4874
2025-04-04 17:32:24,614 - INFO - Epoch [7/10], Batch [510/747], Loss: 0.3173
2025-04-04 17:32:25,245 - INFO - Epoch [7/10], Batch [520/747], Loss: 0.2273
2025-04-04 17:32:25,878 - INFO - Epoch [7/10], Batch [530/747], Loss: 0.2213
2025-04-04 17:32:26,511 - INFO - Epoch [7/10], Batch [540/747], Loss: 0.4913
2025-04-04 17:32:27,143 - INFO - Epoch [7/10], Batch [550/747], Loss: 0.3651
2025-04-04 17:32:27,774 - INFO - Epoch [7/10], Batch [560/747], Loss: 0.3802
2025-04-04 17:32:28,405 - INFO - Epoch [7/10], Batch [570/747], Loss: 0.3000
2025-04-04 17:32:29,035 - INFO - Epoch [7/10], Batch [580/747], Loss: 0.3880
2025-04-04 17:32:29,665 - INFO - Epoch [7/10], Batch [590/747], Loss: 0.5180
2025-04-04 17:32:30,296 - INFO - Epoch [7/10], Batch [600/747], Loss: 0.5169
2025-04-04 17:32:30,927 - INFO - Epoch [7/10], Batch [610/747], Loss: 0.2794
2025-04-04 17:32:31,558 - INFO - Epoch [7/10], Batch [620/747], Loss: 0.5248
2025-04-04 17:32:32,188 - INFO - Epoch [7/10], Batch [630/747], Loss: 0.2846
2025-04-04 17:32:32,821 - INFO - Epoch [7/10], Batch [640/747], Loss: 0.3613
2025-04-04 17:32:33,453 - INFO - Epoch [7/10], Batch [650/747], Loss: 0.3921
2025-04-04 17:32:34,083 - INFO - Epoch [7/10], Batch [660/747], Loss: 0.3624
2025-04-04 17:32:34,714 - INFO - Epoch [7/10], Batch [670/747], Loss: 0.2296
2025-04-04 17:32:35,345 - INFO - Epoch [7/10], Batch [680/747], Loss: 0.2775
2025-04-04 17:32:35,978 - INFO - Epoch [7/10], Batch [690/747], Loss: 0.3102
2025-04-04 17:32:36,610 - INFO - Epoch [7/10], Batch [700/747], Loss: 0.2974
2025-04-04 17:32:37,243 - INFO - Epoch [7/10], Batch [710/747], Loss: 0.4102
2025-04-04 17:32:37,874 - INFO - Epoch [7/10], Batch [720/747], Loss: 0.5145
2025-04-04 17:32:38,506 - INFO - Epoch [7/10], Batch [730/747], Loss: 0.2735
2025-04-04 17:32:39,139 - INFO - Epoch [7/10], Batch [740/747], Loss: 0.4533
2025-04-04 17:32:39,559 - INFO - Epoch 7/10 Train Loss: 0.3720, Train Accuracy: 82.78%
2025-04-04 17:32:43,304 - INFO - Epoch 7/10 Val Loss: 0.3110, Val Accuracy: 86.93%, AUC-ROC: 0.9333
2025-04-04 17:32:43,307 - INFO - New best model at epoch 7 with Val Accuracy: 86.93%
2025-04-04 17:32:43,772 - INFO - Epoch [8/10], Batch [0/747], Loss: 0.2312
2025-04-04 17:32:44,403 - INFO - Epoch [8/10], Batch [10/747], Loss: 0.2523
2025-04-04 17:32:45,033 - INFO - Epoch [8/10], Batch [20/747], Loss: 0.2350
2025-04-04 17:32:45,663 - INFO - Epoch [8/10], Batch [30/747], Loss: 0.3340
2025-04-04 17:32:46,294 - INFO - Epoch [8/10], Batch [40/747], Loss: 0.3449
2025-04-04 17:32:46,925 - INFO - Epoch [8/10], Batch [50/747], Loss: 0.3612
2025-04-04 17:32:47,556 - INFO - Epoch [8/10], Batch [60/747], Loss: 0.4914
2025-04-04 17:32:48,186 - INFO - Epoch [8/10], Batch [70/747], Loss: 0.3684
2025-04-04 17:32:48,817 - INFO - Epoch [8/10], Batch [80/747], Loss: 0.2663
2025-04-04 17:32:49,447 - INFO - Epoch [8/10], Batch [90/747], Loss: 0.4078
2025-04-04 17:32:50,077 - INFO - Epoch [8/10], Batch [100/747], Loss: 0.4269
2025-04-04 17:32:50,708 - INFO - Epoch [8/10], Batch [110/747], Loss: 0.6483
2025-04-04 17:32:51,338 - INFO - Epoch [8/10], Batch [120/747], Loss: 0.4552
2025-04-04 17:32:51,969 - INFO - Epoch [8/10], Batch [130/747], Loss: 0.3410
2025-04-04 17:32:52,598 - INFO - Epoch [8/10], Batch [140/747], Loss: 0.3282
2025-04-04 17:32:53,229 - INFO - Epoch [8/10], Batch [150/747], Loss: 0.3102
2025-04-04 17:32:53,860 - INFO - Epoch [8/10], Batch [160/747], Loss: 0.3955
2025-04-04 17:32:54,491 - INFO - Epoch [8/10], Batch [170/747], Loss: 0.3516
2025-04-04 17:32:55,122 - INFO - Epoch [8/10], Batch [180/747], Loss: 0.5281
2025-04-04 17:32:55,752 - INFO - Epoch [8/10], Batch [190/747], Loss: 0.2768
2025-04-04 17:32:56,382 - INFO - Epoch [8/10], Batch [200/747], Loss: 0.2762
2025-04-04 17:32:57,013 - INFO - Epoch [8/10], Batch [210/747], Loss: 0.5016
2025-04-04 17:32:57,645 - INFO - Epoch [8/10], Batch [220/747], Loss: 0.3129
2025-04-04 17:32:58,276 - INFO - Epoch [8/10], Batch [230/747], Loss: 0.4705
2025-04-04 17:32:58,905 - INFO - Epoch [8/10], Batch [240/747], Loss: 0.4075
2025-04-04 17:32:59,536 - INFO - Epoch [8/10], Batch [250/747], Loss: 0.4100
2025-04-04 17:33:00,168 - INFO - Epoch [8/10], Batch [260/747], Loss: 0.2515
2025-04-04 17:33:00,798 - INFO - Epoch [8/10], Batch [270/747], Loss: 0.5454
2025-04-04 17:33:01,429 - INFO - Epoch [8/10], Batch [280/747], Loss: 0.5747
2025-04-04 17:33:02,060 - INFO - Epoch [8/10], Batch [290/747], Loss: 0.1942
2025-04-04 17:33:02,689 - INFO - Epoch [8/10], Batch [300/747], Loss: 0.3534
2025-04-04 17:33:03,319 - INFO - Epoch [8/10], Batch [310/747], Loss: 0.3353
2025-04-04 17:33:03,950 - INFO - Epoch [8/10], Batch [320/747], Loss: 0.3963
2025-04-04 17:33:04,581 - INFO - Epoch [8/10], Batch [330/747], Loss: 0.4199
2025-04-04 17:33:05,210 - INFO - Epoch [8/10], Batch [340/747], Loss: 0.2608
2025-04-04 17:33:05,840 - INFO - Epoch [8/10], Batch [350/747], Loss: 0.2889
2025-04-04 17:33:06,472 - INFO - Epoch [8/10], Batch [360/747], Loss: 0.2408
2025-04-04 17:33:07,103 - INFO - Epoch [8/10], Batch [370/747], Loss: 0.2830
2025-04-04 17:33:07,734 - INFO - Epoch [8/10], Batch [380/747], Loss: 0.2946
2025-04-04 17:33:08,364 - INFO - Epoch [8/10], Batch [390/747], Loss: 0.3561
2025-04-04 17:33:08,994 - INFO - Epoch [8/10], Batch [400/747], Loss: 0.4880
2025-04-04 17:33:09,624 - INFO - Epoch [8/10], Batch [410/747], Loss: 0.3777
2025-04-04 17:33:10,253 - INFO - Epoch [8/10], Batch [420/747], Loss: 0.3642
2025-04-04 17:33:10,884 - INFO - Epoch [8/10], Batch [430/747], Loss: 0.2921
2025-04-04 17:33:11,514 - INFO - Epoch [8/10], Batch [440/747], Loss: 0.2611
2025-04-04 17:33:12,145 - INFO - Epoch [8/10], Batch [450/747], Loss: 0.3444
2025-04-04 17:33:12,775 - INFO - Epoch [8/10], Batch [460/747], Loss: 0.4256
2025-04-04 17:33:13,405 - INFO - Epoch [8/10], Batch [470/747], Loss: 0.4040
2025-04-04 17:33:14,034 - INFO - Epoch [8/10], Batch [480/747], Loss: 0.2334
2025-04-04 17:33:14,665 - INFO - Epoch [8/10], Batch [490/747], Loss: 0.2872
2025-04-04 17:33:15,296 - INFO - Epoch [8/10], Batch [500/747], Loss: 0.3099
2025-04-04 17:33:15,927 - INFO - Epoch [8/10], Batch [510/747], Loss: 0.2756
2025-04-04 17:33:16,558 - INFO - Epoch [8/10], Batch [520/747], Loss: 0.4287
2025-04-04 17:33:17,189 - INFO - Epoch [8/10], Batch [530/747], Loss: 0.3308
2025-04-04 17:33:17,820 - INFO - Epoch [8/10], Batch [540/747], Loss: 0.2987
2025-04-04 17:33:18,450 - INFO - Epoch [8/10], Batch [550/747], Loss: 0.4366
2025-04-04 17:33:19,080 - INFO - Epoch [8/10], Batch [560/747], Loss: 0.3224
2025-04-04 17:33:19,710 - INFO - Epoch [8/10], Batch [570/747], Loss: 0.1615
2025-04-04 17:33:20,340 - INFO - Epoch [8/10], Batch [580/747], Loss: 0.2494
2025-04-04 17:33:20,969 - INFO - Epoch [8/10], Batch [590/747], Loss: 0.3338
2025-04-04 17:33:21,600 - INFO - Epoch [8/10], Batch [600/747], Loss: 0.2886
2025-04-04 17:33:22,230 - INFO - Epoch [8/10], Batch [610/747], Loss: 0.3004
2025-04-04 17:33:22,860 - INFO - Epoch [8/10], Batch [620/747], Loss: 0.4024
2025-04-04 17:33:23,489 - INFO - Epoch [8/10], Batch [630/747], Loss: 0.5070
2025-04-04 17:33:24,120 - INFO - Epoch [8/10], Batch [640/747], Loss: 0.2854
2025-04-04 17:33:24,750 - INFO - Epoch [8/10], Batch [650/747], Loss: 0.2873
2025-04-04 17:33:25,381 - INFO - Epoch [8/10], Batch [660/747], Loss: 0.4956
2025-04-04 17:33:26,012 - INFO - Epoch [8/10], Batch [670/747], Loss: 0.5760
2025-04-04 17:33:26,643 - INFO - Epoch [8/10], Batch [680/747], Loss: 0.6263
2025-04-04 17:33:27,274 - INFO - Epoch [8/10], Batch [690/747], Loss: 0.2280
2025-04-04 17:33:27,905 - INFO - Epoch [8/10], Batch [700/747], Loss: 0.2021
2025-04-04 17:33:28,535 - INFO - Epoch [8/10], Batch [710/747], Loss: 0.5055
2025-04-04 17:33:29,165 - INFO - Epoch [8/10], Batch [720/747], Loss: 0.3972
2025-04-04 17:33:29,795 - INFO - Epoch [8/10], Batch [730/747], Loss: 0.1770
2025-04-04 17:33:30,426 - INFO - Epoch [8/10], Batch [740/747], Loss: 0.3708
2025-04-04 17:33:30,831 - INFO - Epoch 8/10 Train Loss: 0.3575, Train Accuracy: 83.50%
2025-04-04 17:33:34,554 - INFO - Epoch 8/10 Val Loss: 0.2854, Val Accuracy: 87.92%, AUC-ROC: 0.9423
2025-04-04 17:33:34,558 - INFO - New best model at epoch 8 with Val Accuracy: 87.92%
2025-04-04 17:33:35,041 - INFO - Epoch [9/10], Batch [0/747], Loss: 0.3800
2025-04-04 17:33:35,671 - INFO - Epoch [9/10], Batch [10/747], Loss: 0.3359
2025-04-04 17:33:36,301 - INFO - Epoch [9/10], Batch [20/747], Loss: 0.2795
2025-04-04 17:33:36,932 - INFO - Epoch [9/10], Batch [30/747], Loss: 0.3166
2025-04-04 17:33:37,562 - INFO - Epoch [9/10], Batch [40/747], Loss: 0.3366
2025-04-04 17:33:38,192 - INFO - Epoch [9/10], Batch [50/747], Loss: 0.2378
2025-04-04 17:33:38,823 - INFO - Epoch [9/10], Batch [60/747], Loss: 0.4348
2025-04-04 17:33:39,454 - INFO - Epoch [9/10], Batch [70/747], Loss: 0.3897
2025-04-04 17:33:40,083 - INFO - Epoch [9/10], Batch [80/747], Loss: 0.5025
2025-04-04 17:33:40,713 - INFO - Epoch [9/10], Batch [90/747], Loss: 0.2101
2025-04-04 17:33:41,343 - INFO - Epoch [9/10], Batch [100/747], Loss: 0.3514
2025-04-04 17:33:41,973 - INFO - Epoch [9/10], Batch [110/747], Loss: 0.5482
2025-04-04 17:33:42,602 - INFO - Epoch [9/10], Batch [120/747], Loss: 0.2871
2025-04-04 17:33:43,231 - INFO - Epoch [9/10], Batch [130/747], Loss: 0.2567
2025-04-04 17:33:43,861 - INFO - Epoch [9/10], Batch [140/747], Loss: 0.2612
2025-04-04 17:33:44,491 - INFO - Epoch [9/10], Batch [150/747], Loss: 0.4169
2025-04-04 17:33:45,121 - INFO - Epoch [9/10], Batch [160/747], Loss: 0.2940
2025-04-04 17:33:45,751 - INFO - Epoch [9/10], Batch [170/747], Loss: 0.2841
2025-04-04 17:33:46,381 - INFO - Epoch [9/10], Batch [180/747], Loss: 0.5060
2025-04-04 17:33:47,011 - INFO - Epoch [9/10], Batch [190/747], Loss: 0.4602
2025-04-04 17:33:47,641 - INFO - Epoch [9/10], Batch [200/747], Loss: 0.3763
2025-04-04 17:33:48,272 - INFO - Epoch [9/10], Batch [210/747], Loss: 0.3508
2025-04-04 17:33:48,903 - INFO - Epoch [9/10], Batch [220/747], Loss: 0.3586
2025-04-04 17:33:49,532 - INFO - Epoch [9/10], Batch [230/747], Loss: 0.3804
2025-04-04 17:33:50,162 - INFO - Epoch [9/10], Batch [240/747], Loss: 0.2797
2025-04-04 17:33:50,791 - INFO - Epoch [9/10], Batch [250/747], Loss: 0.3982
2025-04-04 17:33:51,421 - INFO - Epoch [9/10], Batch [260/747], Loss: 0.4020
2025-04-04 17:33:52,052 - INFO - Epoch [9/10], Batch [270/747], Loss: 0.3621
2025-04-04 17:33:52,681 - INFO - Epoch [9/10], Batch [280/747], Loss: 0.4117
2025-04-04 17:33:53,311 - INFO - Epoch [9/10], Batch [290/747], Loss: 0.4812
2025-04-04 17:33:53,941 - INFO - Epoch [9/10], Batch [300/747], Loss: 0.3244
2025-04-04 17:33:54,570 - INFO - Epoch [9/10], Batch [310/747], Loss: 0.3922
2025-04-04 17:33:55,199 - INFO - Epoch [9/10], Batch [320/747], Loss: 0.4266
2025-04-04 17:33:55,829 - INFO - Epoch [9/10], Batch [330/747], Loss: 0.3345
2025-04-04 17:33:56,459 - INFO - Epoch [9/10], Batch [340/747], Loss: 0.2987
2025-04-04 17:33:57,089 - INFO - Epoch [9/10], Batch [350/747], Loss: 0.3223
2025-04-04 17:33:57,720 - INFO - Epoch [9/10], Batch [360/747], Loss: 0.2211
2025-04-04 17:33:58,350 - INFO - Epoch [9/10], Batch [370/747], Loss: 0.3020
2025-04-04 17:33:58,980 - INFO - Epoch [9/10], Batch [380/747], Loss: 0.2764
2025-04-04 17:33:59,610 - INFO - Epoch [9/10], Batch [390/747], Loss: 0.6747
2025-04-04 17:34:00,239 - INFO - Epoch [9/10], Batch [400/747], Loss: 0.2429
2025-04-04 17:34:00,869 - INFO - Epoch [9/10], Batch [410/747], Loss: 0.4011
2025-04-04 17:34:01,499 - INFO - Epoch [9/10], Batch [420/747], Loss: 0.2203
2025-04-04 17:34:02,129 - INFO - Epoch [9/10], Batch [430/747], Loss: 0.2302
2025-04-04 17:34:02,758 - INFO - Epoch [9/10], Batch [440/747], Loss: 0.2103
2025-04-04 17:34:03,389 - INFO - Epoch [9/10], Batch [450/747], Loss: 0.1664
2025-04-04 17:34:04,018 - INFO - Epoch [9/10], Batch [460/747], Loss: 0.3939
2025-04-04 17:34:04,648 - INFO - Epoch [9/10], Batch [470/747], Loss: 0.4459
2025-04-04 17:34:05,279 - INFO - Epoch [9/10], Batch [480/747], Loss: 0.4123
2025-04-04 17:34:05,908 - INFO - Epoch [9/10], Batch [490/747], Loss: 0.3293
2025-04-04 17:34:06,538 - INFO - Epoch [9/10], Batch [500/747], Loss: 0.2804
2025-04-04 17:34:07,167 - INFO - Epoch [9/10], Batch [510/747], Loss: 0.2819
2025-04-04 17:34:07,797 - INFO - Epoch [9/10], Batch [520/747], Loss: 0.3546
2025-04-04 17:34:08,428 - INFO - Epoch [9/10], Batch [530/747], Loss: 0.2066
2025-04-04 17:34:09,058 - INFO - Epoch [9/10], Batch [540/747], Loss: 0.2920
2025-04-04 17:34:09,690 - INFO - Epoch [9/10], Batch [550/747], Loss: 0.2794
2025-04-04 17:34:10,320 - INFO - Epoch [9/10], Batch [560/747], Loss: 0.4791
2025-04-04 17:34:10,950 - INFO - Epoch [9/10], Batch [570/747], Loss: 0.3700
2025-04-04 17:34:11,579 - INFO - Epoch [9/10], Batch [580/747], Loss: 0.3952
2025-04-04 17:34:12,209 - INFO - Epoch [9/10], Batch [590/747], Loss: 0.1925
2025-04-04 17:34:12,840 - INFO - Epoch [9/10], Batch [600/747], Loss: 0.1421
2025-04-04 17:34:13,470 - INFO - Epoch [9/10], Batch [610/747], Loss: 0.2933
2025-04-04 17:34:14,100 - INFO - Epoch [9/10], Batch [620/747], Loss: 0.2423
2025-04-04 17:34:14,729 - INFO - Epoch [9/10], Batch [630/747], Loss: 0.4613
2025-04-04 17:34:15,359 - INFO - Epoch [9/10], Batch [640/747], Loss: 0.3798
2025-04-04 17:34:15,988 - INFO - Epoch [9/10], Batch [650/747], Loss: 0.2864
2025-04-04 17:34:16,618 - INFO - Epoch [9/10], Batch [660/747], Loss: 0.6162
2025-04-04 17:34:17,248 - INFO - Epoch [9/10], Batch [670/747], Loss: 0.2295
2025-04-04 17:34:17,877 - INFO - Epoch [9/10], Batch [680/747], Loss: 0.2731
2025-04-04 17:34:18,509 - INFO - Epoch [9/10], Batch [690/747], Loss: 0.3869
2025-04-04 17:34:19,140 - INFO - Epoch [9/10], Batch [700/747], Loss: 0.3854
2025-04-04 17:34:19,770 - INFO - Epoch [9/10], Batch [710/747], Loss: 0.2286
2025-04-04 17:34:20,401 - INFO - Epoch [9/10], Batch [720/747], Loss: 0.2152
2025-04-04 17:34:21,031 - INFO - Epoch [9/10], Batch [730/747], Loss: 0.2044
2025-04-04 17:34:21,661 - INFO - Epoch [9/10], Batch [740/747], Loss: 0.3158
2025-04-04 17:34:22,074 - INFO - Epoch 9/10 Train Loss: 0.3421, Train Accuracy: 84.52%
2025-04-04 17:34:25,791 - INFO - Epoch 9/10 Val Loss: 0.2796, Val Accuracy: 88.05%, AUC-ROC: 0.9455
2025-04-04 17:34:25,794 - INFO - New best model at epoch 9 with Val Accuracy: 88.05%
2025-04-04 17:34:26,290 - INFO - Epoch [10/10], Batch [0/747], Loss: 0.3159
2025-04-04 17:34:26,921 - INFO - Epoch [10/10], Batch [10/747], Loss: 0.3237
2025-04-04 17:34:27,552 - INFO - Epoch [10/10], Batch [20/747], Loss: 0.2211
2025-04-04 17:34:28,183 - INFO - Epoch [10/10], Batch [30/747], Loss: 0.2268
2025-04-04 17:34:28,814 - INFO - Epoch [10/10], Batch [40/747], Loss: 0.2644
2025-04-04 17:34:29,444 - INFO - Epoch [10/10], Batch [50/747], Loss: 0.3779
2025-04-04 17:34:30,075 - INFO - Epoch [10/10], Batch [60/747], Loss: 0.2503
2025-04-04 17:34:30,705 - INFO - Epoch [10/10], Batch [70/747], Loss: 0.3373
2025-04-04 17:34:31,334 - INFO - Epoch [10/10], Batch [80/747], Loss: 0.2966
2025-04-04 17:34:31,964 - INFO - Epoch [10/10], Batch [90/747], Loss: 0.1682
2025-04-04 17:34:32,594 - INFO - Epoch [10/10], Batch [100/747], Loss: 0.2036
2025-04-04 17:34:33,224 - INFO - Epoch [10/10], Batch [110/747], Loss: 0.1801
2025-04-04 17:34:33,855 - INFO - Epoch [10/10], Batch [120/747], Loss: 0.3990
2025-04-04 17:34:34,486 - INFO - Epoch [10/10], Batch [130/747], Loss: 0.2326
2025-04-04 17:34:35,116 - INFO - Epoch [10/10], Batch [140/747], Loss: 0.2990
2025-04-04 17:34:35,746 - INFO - Epoch [10/10], Batch [150/747], Loss: 0.2239
2025-04-04 17:34:36,376 - INFO - Epoch [10/10], Batch [160/747], Loss: 0.3524
2025-04-04 17:34:37,007 - INFO - Epoch [10/10], Batch [170/747], Loss: 0.2986
2025-04-04 17:34:37,636 - INFO - Epoch [10/10], Batch [180/747], Loss: 0.5324
2025-04-04 17:34:38,266 - INFO - Epoch [10/10], Batch [190/747], Loss: 0.2322
2025-04-04 17:34:38,897 - INFO - Epoch [10/10], Batch [200/747], Loss: 0.4662
2025-04-04 17:34:39,528 - INFO - Epoch [10/10], Batch [210/747], Loss: 0.3486
2025-04-04 17:34:40,159 - INFO - Epoch [10/10], Batch [220/747], Loss: 0.5344
2025-04-04 17:34:40,790 - INFO - Epoch [10/10], Batch [230/747], Loss: 0.2501
2025-04-04 17:34:41,420 - INFO - Epoch [10/10], Batch [240/747], Loss: 0.3085
2025-04-04 17:34:42,050 - INFO - Epoch [10/10], Batch [250/747], Loss: 0.3041
2025-04-04 17:34:42,679 - INFO - Epoch [10/10], Batch [260/747], Loss: 0.3042
2025-04-04 17:34:43,309 - INFO - Epoch [10/10], Batch [270/747], Loss: 0.2946
2025-04-04 17:34:43,939 - INFO - Epoch [10/10], Batch [280/747], Loss: 0.1889
2025-04-04 17:34:44,571 - INFO - Epoch [10/10], Batch [290/747], Loss: 0.3133
2025-04-04 17:34:45,202 - INFO - Epoch [10/10], Batch [300/747], Loss: 0.2687
2025-04-04 17:34:45,832 - INFO - Epoch [10/10], Batch [310/747], Loss: 0.2368
2025-04-04 17:34:46,461 - INFO - Epoch [10/10], Batch [320/747], Loss: 0.3663
2025-04-04 17:34:47,091 - INFO - Epoch [10/10], Batch [330/747], Loss: 0.2723
2025-04-04 17:34:47,721 - INFO - Epoch [10/10], Batch [340/747], Loss: 0.3015
2025-04-04 17:34:48,351 - INFO - Epoch [10/10], Batch [350/747], Loss: 0.1767
2025-04-04 17:34:48,984 - INFO - Epoch [10/10], Batch [360/747], Loss: 0.2447
2025-04-04 17:34:49,614 - INFO - Epoch [10/10], Batch [370/747], Loss: 0.5244
2025-04-04 17:34:50,244 - INFO - Epoch [10/10], Batch [380/747], Loss: 0.2620
2025-04-04 17:34:50,877 - INFO - Epoch [10/10], Batch [390/747], Loss: 0.3024
2025-04-04 17:34:51,507 - INFO - Epoch [10/10], Batch [400/747], Loss: 0.2424
2025-04-04 17:34:52,137 - INFO - Epoch [10/10], Batch [410/747], Loss: 0.2944
2025-04-04 17:34:52,767 - INFO - Epoch [10/10], Batch [420/747], Loss: 0.4503
2025-04-04 17:34:53,396 - INFO - Epoch [10/10], Batch [430/747], Loss: 0.4168
2025-04-04 17:34:54,026 - INFO - Epoch [10/10], Batch [440/747], Loss: 0.3498
2025-04-04 17:34:54,656 - INFO - Epoch [10/10], Batch [450/747], Loss: 0.2396
2025-04-04 17:34:55,286 - INFO - Epoch [10/10], Batch [460/747], Loss: 0.3535
2025-04-04 17:34:55,915 - INFO - Epoch [10/10], Batch [470/747], Loss: 0.2855
2025-04-04 17:34:56,546 - INFO - Epoch [10/10], Batch [480/747], Loss: 0.3828
2025-04-04 17:34:57,176 - INFO - Epoch [10/10], Batch [490/747], Loss: 0.5376
2025-04-04 17:34:57,806 - INFO - Epoch [10/10], Batch [500/747], Loss: 0.1939
2025-04-04 17:34:58,437 - INFO - Epoch [10/10], Batch [510/747], Loss: 0.4615
2025-04-04 17:34:59,067 - INFO - Epoch [10/10], Batch [520/747], Loss: 0.2506
2025-04-04 17:34:59,700 - INFO - Epoch [10/10], Batch [530/747], Loss: 0.3822
2025-04-04 17:35:00,329 - INFO - Epoch [10/10], Batch [540/747], Loss: 0.3021
2025-04-04 17:35:00,961 - INFO - Epoch [10/10], Batch [550/747], Loss: 0.4976
2025-04-04 17:35:01,592 - INFO - Epoch [10/10], Batch [560/747], Loss: 0.3117
2025-04-04 17:35:02,222 - INFO - Epoch [10/10], Batch [570/747], Loss: 0.2225
2025-04-04 17:35:02,853 - INFO - Epoch [10/10], Batch [580/747], Loss: 0.5336
2025-04-04 17:35:03,483 - INFO - Epoch [10/10], Batch [590/747], Loss: 0.3845
2025-04-04 17:35:04,113 - INFO - Epoch [10/10], Batch [600/747], Loss: 0.3668
2025-04-04 17:35:04,743 - INFO - Epoch [10/10], Batch [610/747], Loss: 0.5414
2025-04-04 17:35:05,373 - INFO - Epoch [10/10], Batch [620/747], Loss: 0.4018
2025-04-04 17:35:06,003 - INFO - Epoch [10/10], Batch [630/747], Loss: 0.2333
2025-04-04 17:35:06,634 - INFO - Epoch [10/10], Batch [640/747], Loss: 0.4081
2025-04-04 17:35:07,264 - INFO - Epoch [10/10], Batch [650/747], Loss: 0.2167
2025-04-04 17:35:07,894 - INFO - Epoch [10/10], Batch [660/747], Loss: 0.2555
2025-04-04 17:35:08,524 - INFO - Epoch [10/10], Batch [670/747], Loss: 0.3775
2025-04-04 17:35:09,155 - INFO - Epoch [10/10], Batch [680/747], Loss: 0.2879
2025-04-04 17:35:09,786 - INFO - Epoch [10/10], Batch [690/747], Loss: 0.3838
2025-04-04 17:35:10,417 - INFO - Epoch [10/10], Batch [700/747], Loss: 0.1964
2025-04-04 17:35:11,049 - INFO - Epoch [10/10], Batch [710/747], Loss: 0.4796
2025-04-04 17:35:11,679 - INFO - Epoch [10/10], Batch [720/747], Loss: 0.2928
2025-04-04 17:35:12,310 - INFO - Epoch [10/10], Batch [730/747], Loss: 0.3422
2025-04-04 17:35:12,940 - INFO - Epoch [10/10], Batch [740/747], Loss: 0.3656
2025-04-04 17:35:13,350 - INFO - Epoch 10/10 Train Loss: 0.3333, Train Accuracy: 84.75%
2025-04-04 17:35:17,067 - INFO - Epoch 10/10 Val Loss: 0.2729, Val Accuracy: 88.25%, AUC-ROC: 0.9477
2025-04-04 17:35:17,071 - INFO - New best model at epoch 10 with Val Accuracy: 88.25%
2025-04-04 17:35:17,071 - INFO - Total training time: 513.35 seconds
2025-04-04 17:35:17,076 - INFO - Loaded best model with Val Accuracy: 88.25%
2025-04-04 17:35:19,093 - INFO - 
===== Final Test Results =====
2025-04-04 17:35:19,094 - INFO - Test Loss: 0.2887
2025-04-04 17:35:19,094 - INFO - Test Accuracy: 88.27%
2025-04-04 17:35:19,094 - INFO - Test AUC-ROC: 0.9429
2025-04-04 17:35:19,307 - INFO - Model saved to models/training_using_gpus_1_model.pth
2025-04-04 17:35:19,989 - INFO - Training plots saved as plots/training_using_gpus_1_results.png
2025-04-04 17:35:20,002 - INFO - Runtime parameters saved as metrics/training_using_gpus_1_params.json
2025-04-04 17:35:20,082 - INFO - Training completed.
2025-04-04 17:35:20,083 - INFO - Test Accuracy: 88.27%
2025-04-04 17:35:20,083 - INFO - Total Layers: 129
2025-04-04 17:35:20,083 - INFO - Total Parameters: 22,494,274
