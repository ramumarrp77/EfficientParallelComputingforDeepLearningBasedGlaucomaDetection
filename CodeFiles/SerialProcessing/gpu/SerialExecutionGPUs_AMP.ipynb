{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b011ea-331c-4edf-b310-a67e8d41413d",
   "metadata": {},
   "source": [
    "# CSYE 7105 - High Perfoamnce Machine Learning & AI - Project - Team 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0178d-11bf-40f1-a290-9b2f67733a09",
   "metadata": {},
   "source": [
    "## Analysis of Efficient Parallel Computing for Deep Learning-Based Glaucoma Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522f596-5f48-4c7c-a170-30118065407f",
   "metadata": {},
   "source": [
    "### Serial Execution using GPU with AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d70aa-79e7-478b-860a-af211039e509",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc414e40-dda5-4fa5-9301-b3c2f2b1442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_fscore_support\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0923c-f289-43b1-9316-66a392e6cdc8",
   "metadata": {},
   "source": [
    "#### Defining the Glaucoma - Medical CNN model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ade6c-dd2f-4d5f-9920-f3ad28f7ef42",
   "metadata": {},
   "source": [
    "This MedicalCNN model, with 129 layers, uses a combination of convolutional, residual, multi-dilated, and fully connected blocks, tailored for glaucoma classification. Iâ€™ve included Residual Blocks to help with vanishing gradients, MultiDilated Blocks for enhanced feature extraction, and Squeeze-and-Excitation (SE) Blocks to fine-tune feature importance across channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c1b153-34ed-44b3-9cfd-01bdb9d87d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        image = torch.from_numpy(image).float()\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.permute(2, 0, 1)  # (H,W,C) -> (C,H,W)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, \n",
    "                              stride=stride, padding=padding, bias=False, dilation=dilation)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class MultiDilatedBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MultiDilatedBlock, self).__init__()\n",
    "        self.conv1 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv2 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv3 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=3, dilation=3)\n",
    "        self.conv4 = ConvBlock(in_channels, out_channels//4, kernel_size=3, padding=4, dilation=4)\n",
    "        self.conv_fusion = ConvBlock(out_channels, out_channels, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "        x4 = self.conv4(x)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        return self.conv_fusion(x)\n",
    "\n",
    "class MedicalCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, base_filters=64):\n",
    "        super(MedicalCNN, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, base_filters, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(base_filters),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.stage1 = self._make_stage(base_filters, base_filters, blocks=3)\n",
    "        self.stage2 = self._make_stage(base_filters, base_filters*2, blocks=4, stride=2)\n",
    "        self.stage3_res = self._make_stage(base_filters*2, base_filters*4, blocks=6, stride=2)\n",
    "        self.stage3_md = MultiDilatedBlock(base_filters*4, base_filters*4)\n",
    "        self.stage4_res = self._make_stage(base_filters*4, base_filters*8, blocks=3, stride=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(base_filters*8, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_stage(self, in_channels, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3_res(x)\n",
    "        x = self.stage3_md(x)\n",
    "        x = self.stage4_res(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def count_layers(self):\n",
    "        stem_layers = 4\n",
    "        stage1_layers = 3 * 6\n",
    "        stage2_layers = 4 * 6\n",
    "        stage3_layers = 6 * 6 + 10\n",
    "        stage4_layers = 3 * 6 + 10\n",
    "        fc_layers = 9\n",
    "        total_layers = stem_layers + stage1_layers + stage2_layers + stage3_layers + stage4_layers + fc_layers\n",
    "        return total_layers\n",
    "\n",
    "def get_medical_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05, hue=0.05),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef906db0-9136-47fe-bcd3-aeb04aebd040",
   "metadata": {},
   "source": [
    "#### Defining the functions to train the model in 1GPU without DDP or other parallelization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f7c3c0-e3f5-4744-8421-1f5635c74a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level to INFO\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logs/training_using_gpus_1_amp_logs.txt\"),\n",
    "        logging.StreamHandler()  # Also log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_and_evaluate(data_dir=\"../../preprocessed_glaucoma_data\"):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    logging.info(\"Loading data...\")\n",
    "    X_train = np.load(os.path.join(data_dir, 'X_train.npy'))\n",
    "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
    "    X_val = np.load(os.path.join(data_dir, 'X_val.npy'))\n",
    "    y_val = np.load(os.path.join(data_dir, 'y_val.npy'))\n",
    "    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))\n",
    "    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "    \n",
    "    logging.info(f\"Data shapes: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}\")\n",
    "    \n",
    "    train_transform, val_transform = get_medical_transforms()\n",
    "    \n",
    "    train_dataset = GlaucomaDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = GlaucomaDataset(X_val, y_val, transform=val_transform)\n",
    "    test_dataset = GlaucomaDataset(X_test, y_test, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "    class_weights = torch.FloatTensor([1.0 / count for count in class_counts])\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    num_classes = 2\n",
    "    model = MedicalCNN(num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_layers = model.count_layers()\n",
    "    logging.info(f\"Model architecture: MedicalCNN\")\n",
    "    logging.info(f\"Total layers: {total_layers}\")\n",
    "    logging.info(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    initial_lr = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "    num_epochs = 10\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    # Initialize AMP\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Lists to record metrics for plotting\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with AMP\n",
    "            with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass and optimization step with AMP\n",
    "            scaler.scale(loss).backward()  # Scaled loss for backpropagation\n",
    "            scaler.step(optimizer)  # Step optimizer\n",
    "            scaler.update()  # Update the scale for next iteration\n",
    "            \n",
    "            epoch_train_loss += loss.item() * data.size(0)\n",
    "            total_train_samples += data.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                logging.info(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_loss = epoch_train_loss / total_train_samples\n",
    "        train_accuracy = 100.0 * correct_train / total_train_samples\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        logging.info(f'Epoch {epoch+1}/{num_epochs} Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "        \n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val_samples = 0\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                epoch_val_loss += loss.item() * data.size(0)\n",
    "                total_val_samples += data.size(0)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct_val += (predicted == target).sum().item()\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_probabilities.extend(F.softmax(output, dim=1).cpu().numpy())\n",
    "        \n",
    "        val_loss = epoch_val_loss / total_val_samples\n",
    "        val_accuracy = 100.0 * correct_val / total_val_samples\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        if num_classes == 2:\n",
    "            all_targets = np.array(all_targets)\n",
    "            all_probabilities = np.array(all_probabilities)\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(all_targets, all_probabilities[:, 1])\n",
    "                logging.info(f'Epoch {epoch+1}/{num_epochs} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, AUC-ROC: {roc_auc:.4f}')\n",
    "            except Exception as e:\n",
    "                logging.info(f'Epoch {epoch+1}/{num_epochs} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        else:\n",
    "            logging.info(f'Epoch {epoch+1}/{num_epochs} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            logging.info(f\"New best model at epoch {epoch+1} with Val Accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"Total training time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Load best model for testing\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        logging.info(f\"Loaded best model with Val Accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test_samples = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            epoch_test_loss += loss.item() * data.size(0)\n",
    "            total_test_samples += data.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_test += (predicted == target).sum().item()\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_probabilities.extend(F.softmax(output, dim=1).cpu().numpy())\n",
    "    \n",
    "    test_loss = epoch_test_loss / total_test_samples\n",
    "    test_accuracy = 100.0 * correct_test / total_test_samples\n",
    "    \n",
    "    if num_classes == 2:\n",
    "        try:\n",
    "            test_auc = roc_auc_score(np.array(all_targets), np.array(all_probabilities)[:, 1])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculating Test AUC-ROC: {e}\")\n",
    "            test_auc = None\n",
    "    else:\n",
    "        test_auc = None\n",
    "    \n",
    "    logging.info(\"\\n===== Final Test Results =====\")\n",
    "    logging.info(f\"Test Loss: {test_loss:.4f}\")\n",
    "    logging.info(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    if test_auc is not None:\n",
    "        logging.info(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    model_filename = \"models/training_using_gpus_1_amp_model.pth\"\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    logging.info(f\"Model saved to {model_filename}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    epochs_range = range(1, num_epochs+1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    plot_filename = \"plots/training_using_gpus_1_amp_results.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    logging.info(f\"Training plots saved as {plot_filename}\")\n",
    "    \n",
    "    # Save runtime parameters and metrics as JSON\n",
    "    result = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_auc_roc\": test_auc,\n",
    "        \"computing_time\": total_time,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"initial_lr\": initial_lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"num_classes\": num_classes\n",
    "        },\n",
    "        \"total_parameters\": total_params,\n",
    "        \"total_layers\": total_layers\n",
    "    }\n",
    "    os.makedirs('metrics', exist_ok=True)\n",
    "    params_filename = \"metrics/training_using_gpus_1_amp_params.json\"\n",
    "    with open(params_filename, 'w') as f:\n",
    "        json.dump(result, f, indent=4)\n",
    "    logging.info(f\"Runtime parameters saved as {params_filename}\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f19ee-67dc-4785-b808-a9f421a84a08",
   "metadata": {},
   "source": [
    "#### Main function to handle and call all the functional flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3503d324-7fdb-46a1-99e9-89483e0ca1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 20:21:00,144 - INFO - Running on node: d1004\n",
      "2025-04-04 20:21:00,145 - INFO - Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single GPU...\n",
      "2025-04-04 20:21:00,169 - INFO - Loading data...\n",
      "2025-04-04 20:22:55,307 - INFO - Data shapes: Train (23898, 224, 224, 3), Val (5747, 224, 224, 3), Test (2874, 224, 224, 3)\n",
      "2025-04-04 20:22:55,346 - INFO - Using device: cuda\n",
      "2025-04-04 20:22:55,936 - INFO - Model architecture: MedicalCNN\n",
      "2025-04-04 20:22:55,938 - INFO - Total layers: 129\n",
      "2025-04-04 20:22:55,938 - INFO - Total parameters: 22,494,274\n",
      "/tmp/ipykernel_1095769/965209251.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:22:57,484 - INFO - Epoch [1/10], Batch [0/747], Loss: 0.7058\n",
      "2025-04-04 20:22:57,887 - INFO - Epoch [1/10], Batch [10/747], Loss: 0.6738\n",
      "2025-04-04 20:22:58,290 - INFO - Epoch [1/10], Batch [20/747], Loss: 0.6454\n",
      "2025-04-04 20:22:58,726 - INFO - Epoch [1/10], Batch [30/747], Loss: 0.6368\n",
      "2025-04-04 20:22:59,193 - INFO - Epoch [1/10], Batch [40/747], Loss: 0.7188\n",
      "2025-04-04 20:22:59,686 - INFO - Epoch [1/10], Batch [50/747], Loss: 0.6776\n",
      "2025-04-04 20:23:00,154 - INFO - Epoch [1/10], Batch [60/747], Loss: 0.6732\n",
      "2025-04-04 20:23:00,644 - INFO - Epoch [1/10], Batch [70/747], Loss: 0.6494\n",
      "2025-04-04 20:23:01,114 - INFO - Epoch [1/10], Batch [80/747], Loss: 0.6342\n",
      "2025-04-04 20:23:01,600 - INFO - Epoch [1/10], Batch [90/747], Loss: 0.5757\n",
      "2025-04-04 20:23:02,076 - INFO - Epoch [1/10], Batch [100/747], Loss: 0.7729\n",
      "2025-04-04 20:23:02,558 - INFO - Epoch [1/10], Batch [110/747], Loss: 0.7481\n",
      "2025-04-04 20:23:03,032 - INFO - Epoch [1/10], Batch [120/747], Loss: 0.6578\n",
      "2025-04-04 20:23:03,514 - INFO - Epoch [1/10], Batch [130/747], Loss: 0.5830\n",
      "2025-04-04 20:23:03,986 - INFO - Epoch [1/10], Batch [140/747], Loss: 0.5944\n",
      "2025-04-04 20:23:04,468 - INFO - Epoch [1/10], Batch [150/747], Loss: 0.6324\n",
      "2025-04-04 20:23:04,945 - INFO - Epoch [1/10], Batch [160/747], Loss: 0.6124\n",
      "2025-04-04 20:23:05,423 - INFO - Epoch [1/10], Batch [170/747], Loss: 0.5893\n",
      "2025-04-04 20:23:05,904 - INFO - Epoch [1/10], Batch [180/747], Loss: 0.6013\n",
      "2025-04-04 20:23:06,380 - INFO - Epoch [1/10], Batch [190/747], Loss: 0.5779\n",
      "2025-04-04 20:23:06,859 - INFO - Epoch [1/10], Batch [200/747], Loss: 0.6588\n",
      "2025-04-04 20:23:07,339 - INFO - Epoch [1/10], Batch [210/747], Loss: 0.6089\n",
      "2025-04-04 20:23:07,821 - INFO - Epoch [1/10], Batch [220/747], Loss: 0.6957\n",
      "2025-04-04 20:23:08,298 - INFO - Epoch [1/10], Batch [230/747], Loss: 0.6701\n",
      "2025-04-04 20:23:08,797 - INFO - Epoch [1/10], Batch [240/747], Loss: 0.7334\n",
      "2025-04-04 20:23:09,274 - INFO - Epoch [1/10], Batch [250/747], Loss: 0.5886\n",
      "2025-04-04 20:23:09,757 - INFO - Epoch [1/10], Batch [260/747], Loss: 0.6144\n",
      "2025-04-04 20:23:10,231 - INFO - Epoch [1/10], Batch [270/747], Loss: 0.6378\n",
      "2025-04-04 20:23:10,710 - INFO - Epoch [1/10], Batch [280/747], Loss: 0.6858\n",
      "2025-04-04 20:23:11,186 - INFO - Epoch [1/10], Batch [290/747], Loss: 0.5790\n",
      "2025-04-04 20:23:11,674 - INFO - Epoch [1/10], Batch [300/747], Loss: 0.5856\n",
      "2025-04-04 20:23:12,143 - INFO - Epoch [1/10], Batch [310/747], Loss: 0.6527\n",
      "2025-04-04 20:23:12,630 - INFO - Epoch [1/10], Batch [320/747], Loss: 0.6119\n",
      "2025-04-04 20:23:13,104 - INFO - Epoch [1/10], Batch [330/747], Loss: 0.5655\n",
      "2025-04-04 20:23:13,590 - INFO - Epoch [1/10], Batch [340/747], Loss: 0.6442\n",
      "2025-04-04 20:23:14,058 - INFO - Epoch [1/10], Batch [350/747], Loss: 0.5664\n",
      "2025-04-04 20:23:14,555 - INFO - Epoch [1/10], Batch [360/747], Loss: 0.5319\n",
      "2025-04-04 20:23:15,020 - INFO - Epoch [1/10], Batch [370/747], Loss: 0.5914\n",
      "2025-04-04 20:23:15,518 - INFO - Epoch [1/10], Batch [380/747], Loss: 0.5571\n",
      "2025-04-04 20:23:15,981 - INFO - Epoch [1/10], Batch [390/747], Loss: 0.6473\n",
      "2025-04-04 20:23:16,478 - INFO - Epoch [1/10], Batch [400/747], Loss: 0.4864\n",
      "2025-04-04 20:23:16,943 - INFO - Epoch [1/10], Batch [410/747], Loss: 0.7911\n",
      "2025-04-04 20:23:17,441 - INFO - Epoch [1/10], Batch [420/747], Loss: 0.5942\n",
      "2025-04-04 20:23:17,905 - INFO - Epoch [1/10], Batch [430/747], Loss: 0.6440\n",
      "2025-04-04 20:23:18,398 - INFO - Epoch [1/10], Batch [440/747], Loss: 0.6740\n",
      "2025-04-04 20:23:18,866 - INFO - Epoch [1/10], Batch [450/747], Loss: 0.7172\n",
      "2025-04-04 20:23:19,361 - INFO - Epoch [1/10], Batch [460/747], Loss: 0.6591\n",
      "2025-04-04 20:23:19,825 - INFO - Epoch [1/10], Batch [470/747], Loss: 0.8040\n",
      "2025-04-04 20:23:20,322 - INFO - Epoch [1/10], Batch [480/747], Loss: 0.7419\n",
      "2025-04-04 20:23:20,783 - INFO - Epoch [1/10], Batch [490/747], Loss: 0.5413\n",
      "2025-04-04 20:23:21,274 - INFO - Epoch [1/10], Batch [500/747], Loss: 0.6686\n",
      "2025-04-04 20:23:21,740 - INFO - Epoch [1/10], Batch [510/747], Loss: 0.5663\n",
      "2025-04-04 20:23:22,234 - INFO - Epoch [1/10], Batch [520/747], Loss: 0.5947\n",
      "2025-04-04 20:23:22,699 - INFO - Epoch [1/10], Batch [530/747], Loss: 0.5123\n",
      "2025-04-04 20:23:23,195 - INFO - Epoch [1/10], Batch [540/747], Loss: 0.6727\n",
      "2025-04-04 20:23:23,660 - INFO - Epoch [1/10], Batch [550/747], Loss: 0.6438\n",
      "2025-04-04 20:23:24,155 - INFO - Epoch [1/10], Batch [560/747], Loss: 0.5629\n",
      "2025-04-04 20:23:24,618 - INFO - Epoch [1/10], Batch [570/747], Loss: 0.5952\n",
      "2025-04-04 20:23:25,110 - INFO - Epoch [1/10], Batch [580/747], Loss: 0.5244\n",
      "2025-04-04 20:23:25,577 - INFO - Epoch [1/10], Batch [590/747], Loss: 0.4771\n",
      "2025-04-04 20:23:26,073 - INFO - Epoch [1/10], Batch [600/747], Loss: 0.5377\n",
      "2025-04-04 20:23:26,537 - INFO - Epoch [1/10], Batch [610/747], Loss: 0.5386\n",
      "2025-04-04 20:23:27,038 - INFO - Epoch [1/10], Batch [620/747], Loss: 0.6881\n",
      "2025-04-04 20:23:27,500 - INFO - Epoch [1/10], Batch [630/747], Loss: 0.5684\n",
      "2025-04-04 20:23:28,004 - INFO - Epoch [1/10], Batch [640/747], Loss: 0.5237\n",
      "2025-04-04 20:23:28,470 - INFO - Epoch [1/10], Batch [650/747], Loss: 0.5679\n",
      "2025-04-04 20:23:28,971 - INFO - Epoch [1/10], Batch [660/747], Loss: 0.5217\n",
      "2025-04-04 20:23:29,434 - INFO - Epoch [1/10], Batch [670/747], Loss: 0.5968\n",
      "2025-04-04 20:23:29,934 - INFO - Epoch [1/10], Batch [680/747], Loss: 0.5865\n",
      "2025-04-04 20:23:30,399 - INFO - Epoch [1/10], Batch [690/747], Loss: 0.6011\n",
      "2025-04-04 20:23:30,889 - INFO - Epoch [1/10], Batch [700/747], Loss: 0.6162\n",
      "2025-04-04 20:23:31,353 - INFO - Epoch [1/10], Batch [710/747], Loss: 0.6244\n",
      "2025-04-04 20:23:31,848 - INFO - Epoch [1/10], Batch [720/747], Loss: 0.6067\n",
      "2025-04-04 20:23:32,314 - INFO - Epoch [1/10], Batch [730/747], Loss: 0.5220\n",
      "2025-04-04 20:23:32,806 - INFO - Epoch [1/10], Batch [740/747], Loss: 0.5510\n",
      "2025-04-04 20:23:33,252 - INFO - Epoch 1/10 Train Loss: 0.6211, Train Accuracy: 64.61%\n",
      "2025-04-04 20:23:37,029 - INFO - Epoch 1/10 Val Loss: 0.5677, Val Accuracy: 73.33%, AUC-ROC: 0.8068\n",
      "2025-04-04 20:23:37,032 - INFO - New best model at epoch 1 with Val Accuracy: 73.33%\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:23:37,486 - INFO - Epoch [2/10], Batch [0/747], Loss: 0.5722\n",
      "2025-04-04 20:23:37,965 - INFO - Epoch [2/10], Batch [10/747], Loss: 0.6808\n",
      "2025-04-04 20:23:38,515 - INFO - Epoch [2/10], Batch [20/747], Loss: 0.4809\n",
      "2025-04-04 20:23:39,016 - INFO - Epoch [2/10], Batch [30/747], Loss: 0.5173\n",
      "2025-04-04 20:23:39,566 - INFO - Epoch [2/10], Batch [40/747], Loss: 0.5596\n",
      "2025-04-04 20:23:40,069 - INFO - Epoch [2/10], Batch [50/747], Loss: 0.5769\n",
      "2025-04-04 20:23:40,624 - INFO - Epoch [2/10], Batch [60/747], Loss: 0.3571\n",
      "2025-04-04 20:23:41,123 - INFO - Epoch [2/10], Batch [70/747], Loss: 0.5828\n",
      "2025-04-04 20:23:41,677 - INFO - Epoch [2/10], Batch [80/747], Loss: 0.4224\n",
      "2025-04-04 20:23:42,174 - INFO - Epoch [2/10], Batch [90/747], Loss: 0.5871\n",
      "2025-04-04 20:23:42,728 - INFO - Epoch [2/10], Batch [100/747], Loss: 0.6827\n",
      "2025-04-04 20:23:43,227 - INFO - Epoch [2/10], Batch [110/747], Loss: 0.5459\n",
      "2025-04-04 20:23:43,786 - INFO - Epoch [2/10], Batch [120/747], Loss: 0.4224\n",
      "2025-04-04 20:23:44,288 - INFO - Epoch [2/10], Batch [130/747], Loss: 0.4761\n",
      "2025-04-04 20:23:44,838 - INFO - Epoch [2/10], Batch [140/747], Loss: 0.6669\n",
      "2025-04-04 20:23:45,338 - INFO - Epoch [2/10], Batch [150/747], Loss: 0.5006\n",
      "2025-04-04 20:23:45,893 - INFO - Epoch [2/10], Batch [160/747], Loss: 0.5563\n",
      "2025-04-04 20:23:46,392 - INFO - Epoch [2/10], Batch [170/747], Loss: 0.7454\n",
      "2025-04-04 20:23:46,950 - INFO - Epoch [2/10], Batch [180/747], Loss: 0.6958\n",
      "2025-04-04 20:23:47,453 - INFO - Epoch [2/10], Batch [190/747], Loss: 0.4431\n",
      "2025-04-04 20:23:48,010 - INFO - Epoch [2/10], Batch [200/747], Loss: 0.6406\n",
      "2025-04-04 20:23:48,517 - INFO - Epoch [2/10], Batch [210/747], Loss: 0.6644\n",
      "2025-04-04 20:23:49,071 - INFO - Epoch [2/10], Batch [220/747], Loss: 0.5771\n",
      "2025-04-04 20:23:49,573 - INFO - Epoch [2/10], Batch [230/747], Loss: 0.5820\n",
      "2025-04-04 20:23:50,126 - INFO - Epoch [2/10], Batch [240/747], Loss: 0.5949\n",
      "2025-04-04 20:23:50,627 - INFO - Epoch [2/10], Batch [250/747], Loss: 0.5946\n",
      "2025-04-04 20:23:51,181 - INFO - Epoch [2/10], Batch [260/747], Loss: 0.4278\n",
      "2025-04-04 20:23:51,683 - INFO - Epoch [2/10], Batch [270/747], Loss: 0.4664\n",
      "2025-04-04 20:23:52,232 - INFO - Epoch [2/10], Batch [280/747], Loss: 0.5681\n",
      "2025-04-04 20:23:52,735 - INFO - Epoch [2/10], Batch [290/747], Loss: 0.4865\n",
      "2025-04-04 20:23:53,284 - INFO - Epoch [2/10], Batch [300/747], Loss: 0.6310\n",
      "2025-04-04 20:23:53,784 - INFO - Epoch [2/10], Batch [310/747], Loss: 0.4215\n",
      "2025-04-04 20:23:54,333 - INFO - Epoch [2/10], Batch [320/747], Loss: 0.3806\n",
      "2025-04-04 20:23:54,835 - INFO - Epoch [2/10], Batch [330/747], Loss: 0.4597\n",
      "2025-04-04 20:23:55,387 - INFO - Epoch [2/10], Batch [340/747], Loss: 0.5183\n",
      "2025-04-04 20:23:55,888 - INFO - Epoch [2/10], Batch [350/747], Loss: 0.5220\n",
      "2025-04-04 20:23:56,442 - INFO - Epoch [2/10], Batch [360/747], Loss: 0.4955\n",
      "2025-04-04 20:23:56,944 - INFO - Epoch [2/10], Batch [370/747], Loss: 0.4871\n",
      "2025-04-04 20:23:57,496 - INFO - Epoch [2/10], Batch [380/747], Loss: 0.4764\n",
      "2025-04-04 20:23:57,995 - INFO - Epoch [2/10], Batch [390/747], Loss: 0.3943\n",
      "2025-04-04 20:23:58,545 - INFO - Epoch [2/10], Batch [400/747], Loss: 0.6053\n",
      "2025-04-04 20:23:59,048 - INFO - Epoch [2/10], Batch [410/747], Loss: 0.5980\n",
      "2025-04-04 20:23:59,597 - INFO - Epoch [2/10], Batch [420/747], Loss: 0.6314\n",
      "2025-04-04 20:24:00,098 - INFO - Epoch [2/10], Batch [430/747], Loss: 0.5148\n",
      "2025-04-04 20:24:00,653 - INFO - Epoch [2/10], Batch [440/747], Loss: 0.4974\n",
      "2025-04-04 20:24:01,153 - INFO - Epoch [2/10], Batch [450/747], Loss: 0.3831\n",
      "2025-04-04 20:24:01,703 - INFO - Epoch [2/10], Batch [460/747], Loss: 0.3644\n",
      "2025-04-04 20:24:02,202 - INFO - Epoch [2/10], Batch [470/747], Loss: 0.4742\n",
      "2025-04-04 20:24:02,754 - INFO - Epoch [2/10], Batch [480/747], Loss: 0.4661\n",
      "2025-04-04 20:24:03,256 - INFO - Epoch [2/10], Batch [490/747], Loss: 0.4312\n",
      "2025-04-04 20:24:03,810 - INFO - Epoch [2/10], Batch [500/747], Loss: 0.4802\n",
      "2025-04-04 20:24:04,310 - INFO - Epoch [2/10], Batch [510/747], Loss: 0.4377\n",
      "2025-04-04 20:24:04,865 - INFO - Epoch [2/10], Batch [520/747], Loss: 0.4482\n",
      "2025-04-04 20:24:05,367 - INFO - Epoch [2/10], Batch [530/747], Loss: 0.4063\n",
      "2025-04-04 20:24:05,924 - INFO - Epoch [2/10], Batch [540/747], Loss: 0.6174\n",
      "2025-04-04 20:24:06,427 - INFO - Epoch [2/10], Batch [550/747], Loss: 0.4624\n",
      "2025-04-04 20:24:06,980 - INFO - Epoch [2/10], Batch [560/747], Loss: 0.6218\n",
      "2025-04-04 20:24:07,485 - INFO - Epoch [2/10], Batch [570/747], Loss: 0.5381\n",
      "2025-04-04 20:24:08,044 - INFO - Epoch [2/10], Batch [580/747], Loss: 0.4922\n",
      "2025-04-04 20:24:08,551 - INFO - Epoch [2/10], Batch [590/747], Loss: 0.5191\n",
      "2025-04-04 20:24:09,109 - INFO - Epoch [2/10], Batch [600/747], Loss: 0.4549\n",
      "2025-04-04 20:24:09,613 - INFO - Epoch [2/10], Batch [610/747], Loss: 0.3683\n",
      "2025-04-04 20:24:10,164 - INFO - Epoch [2/10], Batch [620/747], Loss: 0.4774\n",
      "2025-04-04 20:24:10,666 - INFO - Epoch [2/10], Batch [630/747], Loss: 0.4298\n",
      "2025-04-04 20:24:11,220 - INFO - Epoch [2/10], Batch [640/747], Loss: 0.4646\n",
      "2025-04-04 20:24:11,722 - INFO - Epoch [2/10], Batch [650/747], Loss: 0.5982\n",
      "2025-04-04 20:24:12,277 - INFO - Epoch [2/10], Batch [660/747], Loss: 0.4785\n",
      "2025-04-04 20:24:12,783 - INFO - Epoch [2/10], Batch [670/747], Loss: 0.4239\n",
      "2025-04-04 20:24:13,335 - INFO - Epoch [2/10], Batch [680/747], Loss: 0.5629\n",
      "2025-04-04 20:24:13,835 - INFO - Epoch [2/10], Batch [690/747], Loss: 0.5082\n",
      "2025-04-04 20:24:14,385 - INFO - Epoch [2/10], Batch [700/747], Loss: 0.4211\n",
      "2025-04-04 20:24:14,888 - INFO - Epoch [2/10], Batch [710/747], Loss: 0.3659\n",
      "2025-04-04 20:24:15,440 - INFO - Epoch [2/10], Batch [720/747], Loss: 0.4178\n",
      "2025-04-04 20:24:15,942 - INFO - Epoch [2/10], Batch [730/747], Loss: 0.3162\n",
      "2025-04-04 20:24:16,490 - INFO - Epoch [2/10], Batch [740/747], Loss: 0.7366\n",
      "2025-04-04 20:24:16,810 - INFO - Epoch 2/10 Train Loss: 0.5158, Train Accuracy: 74.73%\n",
      "2025-04-04 20:24:20,497 - INFO - Epoch 2/10 Val Loss: 0.4782, Val Accuracy: 77.88%, AUC-ROC: 0.8627\n",
      "2025-04-04 20:24:20,500 - INFO - New best model at epoch 2 with Val Accuracy: 77.88%\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:24:20,947 - INFO - Epoch [3/10], Batch [0/747], Loss: 0.5497\n",
      "2025-04-04 20:24:21,429 - INFO - Epoch [3/10], Batch [10/747], Loss: 0.5059\n",
      "2025-04-04 20:24:21,959 - INFO - Epoch [3/10], Batch [20/747], Loss: 0.4485\n",
      "2025-04-04 20:24:22,449 - INFO - Epoch [3/10], Batch [30/747], Loss: 0.4059\n",
      "2025-04-04 20:24:22,976 - INFO - Epoch [3/10], Batch [40/747], Loss: 0.3937\n",
      "2025-04-04 20:24:23,462 - INFO - Epoch [3/10], Batch [50/747], Loss: 0.6578\n",
      "2025-04-04 20:24:23,991 - INFO - Epoch [3/10], Batch [60/747], Loss: 0.5508\n",
      "2025-04-04 20:24:24,478 - INFO - Epoch [3/10], Batch [70/747], Loss: 0.4682\n",
      "2025-04-04 20:24:25,009 - INFO - Epoch [3/10], Batch [80/747], Loss: 0.5121\n",
      "2025-04-04 20:24:25,499 - INFO - Epoch [3/10], Batch [90/747], Loss: 0.3641\n",
      "2025-04-04 20:24:26,025 - INFO - Epoch [3/10], Batch [100/747], Loss: 0.3909\n",
      "2025-04-04 20:24:26,512 - INFO - Epoch [3/10], Batch [110/747], Loss: 0.3237\n",
      "2025-04-04 20:24:27,043 - INFO - Epoch [3/10], Batch [120/747], Loss: 0.2935\n",
      "2025-04-04 20:24:27,530 - INFO - Epoch [3/10], Batch [130/747], Loss: 0.4758\n",
      "2025-04-04 20:24:28,062 - INFO - Epoch [3/10], Batch [140/747], Loss: 0.5344\n",
      "2025-04-04 20:24:28,550 - INFO - Epoch [3/10], Batch [150/747], Loss: 0.5088\n",
      "2025-04-04 20:24:29,075 - INFO - Epoch [3/10], Batch [160/747], Loss: 0.5789\n",
      "2025-04-04 20:24:29,563 - INFO - Epoch [3/10], Batch [170/747], Loss: 0.4917\n",
      "2025-04-04 20:24:30,105 - INFO - Epoch [3/10], Batch [180/747], Loss: 0.4268\n",
      "2025-04-04 20:24:30,594 - INFO - Epoch [3/10], Batch [190/747], Loss: 0.4362\n",
      "2025-04-04 20:24:31,124 - INFO - Epoch [3/10], Batch [200/747], Loss: 0.5092\n",
      "2025-04-04 20:24:31,610 - INFO - Epoch [3/10], Batch [210/747], Loss: 0.4428\n",
      "2025-04-04 20:24:32,142 - INFO - Epoch [3/10], Batch [220/747], Loss: 0.4004\n",
      "2025-04-04 20:24:32,628 - INFO - Epoch [3/10], Batch [230/747], Loss: 0.5247\n",
      "2025-04-04 20:24:33,158 - INFO - Epoch [3/10], Batch [240/747], Loss: 0.4412\n",
      "2025-04-04 20:24:33,648 - INFO - Epoch [3/10], Batch [250/747], Loss: 0.5590\n",
      "2025-04-04 20:24:34,181 - INFO - Epoch [3/10], Batch [260/747], Loss: 0.4643\n",
      "2025-04-04 20:24:34,668 - INFO - Epoch [3/10], Batch [270/747], Loss: 0.4455\n",
      "2025-04-04 20:24:35,194 - INFO - Epoch [3/10], Batch [280/747], Loss: 0.3324\n",
      "2025-04-04 20:24:35,684 - INFO - Epoch [3/10], Batch [290/747], Loss: 0.5762\n",
      "2025-04-04 20:24:36,216 - INFO - Epoch [3/10], Batch [300/747], Loss: 0.5908\n",
      "2025-04-04 20:24:36,703 - INFO - Epoch [3/10], Batch [310/747], Loss: 0.5631\n",
      "2025-04-04 20:24:37,239 - INFO - Epoch [3/10], Batch [320/747], Loss: 0.4360\n",
      "2025-04-04 20:24:37,728 - INFO - Epoch [3/10], Batch [330/747], Loss: 0.4179\n",
      "2025-04-04 20:24:38,257 - INFO - Epoch [3/10], Batch [340/747], Loss: 0.4532\n",
      "2025-04-04 20:24:38,743 - INFO - Epoch [3/10], Batch [350/747], Loss: 0.7057\n",
      "2025-04-04 20:24:39,275 - INFO - Epoch [3/10], Batch [360/747], Loss: 0.5117\n",
      "2025-04-04 20:24:39,765 - INFO - Epoch [3/10], Batch [370/747], Loss: 0.3967\n",
      "2025-04-04 20:24:40,297 - INFO - Epoch [3/10], Batch [380/747], Loss: 0.4944\n",
      "2025-04-04 20:24:40,787 - INFO - Epoch [3/10], Batch [390/747], Loss: 0.3690\n",
      "2025-04-04 20:24:41,320 - INFO - Epoch [3/10], Batch [400/747], Loss: 0.4383\n",
      "2025-04-04 20:24:41,808 - INFO - Epoch [3/10], Batch [410/747], Loss: 0.5825\n",
      "2025-04-04 20:24:42,342 - INFO - Epoch [3/10], Batch [420/747], Loss: 0.4084\n",
      "2025-04-04 20:24:42,831 - INFO - Epoch [3/10], Batch [430/747], Loss: 0.4748\n",
      "2025-04-04 20:24:43,363 - INFO - Epoch [3/10], Batch [440/747], Loss: 0.5202\n",
      "2025-04-04 20:24:43,847 - INFO - Epoch [3/10], Batch [450/747], Loss: 0.5350\n",
      "2025-04-04 20:24:44,379 - INFO - Epoch [3/10], Batch [460/747], Loss: 0.3930\n",
      "2025-04-04 20:24:44,865 - INFO - Epoch [3/10], Batch [470/747], Loss: 0.4572\n",
      "2025-04-04 20:24:45,391 - INFO - Epoch [3/10], Batch [480/747], Loss: 0.5743\n",
      "2025-04-04 20:24:45,880 - INFO - Epoch [3/10], Batch [490/747], Loss: 0.3996\n",
      "2025-04-04 20:24:46,411 - INFO - Epoch [3/10], Batch [500/747], Loss: 0.5019\n",
      "2025-04-04 20:24:46,899 - INFO - Epoch [3/10], Batch [510/747], Loss: 0.6498\n",
      "2025-04-04 20:24:47,432 - INFO - Epoch [3/10], Batch [520/747], Loss: 0.4059\n",
      "2025-04-04 20:24:47,919 - INFO - Epoch [3/10], Batch [530/747], Loss: 0.5006\n",
      "2025-04-04 20:24:48,452 - INFO - Epoch [3/10], Batch [540/747], Loss: 0.4409\n",
      "2025-04-04 20:24:48,939 - INFO - Epoch [3/10], Batch [550/747], Loss: 0.4434\n",
      "2025-04-04 20:24:49,465 - INFO - Epoch [3/10], Batch [560/747], Loss: 0.5041\n",
      "2025-04-04 20:24:49,958 - INFO - Epoch [3/10], Batch [570/747], Loss: 0.5176\n",
      "2025-04-04 20:24:50,500 - INFO - Epoch [3/10], Batch [580/747], Loss: 0.3629\n",
      "2025-04-04 20:24:50,986 - INFO - Epoch [3/10], Batch [590/747], Loss: 0.5732\n",
      "2025-04-04 20:24:51,516 - INFO - Epoch [3/10], Batch [600/747], Loss: 0.3766\n",
      "2025-04-04 20:24:52,005 - INFO - Epoch [3/10], Batch [610/747], Loss: 0.6073\n",
      "2025-04-04 20:24:52,538 - INFO - Epoch [3/10], Batch [620/747], Loss: 0.4317\n",
      "2025-04-04 20:24:53,026 - INFO - Epoch [3/10], Batch [630/747], Loss: 0.3086\n",
      "2025-04-04 20:24:53,559 - INFO - Epoch [3/10], Batch [640/747], Loss: 0.2766\n",
      "2025-04-04 20:24:54,046 - INFO - Epoch [3/10], Batch [650/747], Loss: 0.5512\n",
      "2025-04-04 20:24:54,578 - INFO - Epoch [3/10], Batch [660/747], Loss: 0.4745\n",
      "2025-04-04 20:24:55,062 - INFO - Epoch [3/10], Batch [670/747], Loss: 0.3949\n",
      "2025-04-04 20:24:55,592 - INFO - Epoch [3/10], Batch [680/747], Loss: 0.5587\n",
      "2025-04-04 20:24:56,080 - INFO - Epoch [3/10], Batch [690/747], Loss: 0.3356\n",
      "2025-04-04 20:24:56,607 - INFO - Epoch [3/10], Batch [700/747], Loss: 0.3356\n",
      "2025-04-04 20:24:57,092 - INFO - Epoch [3/10], Batch [710/747], Loss: 0.3714\n",
      "2025-04-04 20:24:57,617 - INFO - Epoch [3/10], Batch [720/747], Loss: 0.4551\n",
      "2025-04-04 20:24:58,104 - INFO - Epoch [3/10], Batch [730/747], Loss: 0.6120\n",
      "2025-04-04 20:24:58,634 - INFO - Epoch [3/10], Batch [740/747], Loss: 0.6321\n",
      "2025-04-04 20:24:58,944 - INFO - Epoch 3/10 Train Loss: 0.4667, Train Accuracy: 77.38%\n",
      "2025-04-04 20:25:02,652 - INFO - Epoch 3/10 Val Loss: 0.4239, Val Accuracy: 81.75%, AUC-ROC: 0.8835\n",
      "2025-04-04 20:25:02,655 - INFO - New best model at epoch 3 with Val Accuracy: 81.75%\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:25:03,116 - INFO - Epoch [4/10], Batch [0/747], Loss: 0.5574\n",
      "2025-04-04 20:25:03,568 - INFO - Epoch [4/10], Batch [10/747], Loss: 0.5044\n",
      "2025-04-04 20:25:04,062 - INFO - Epoch [4/10], Batch [20/747], Loss: 0.3650\n",
      "2025-04-04 20:25:04,582 - INFO - Epoch [4/10], Batch [30/747], Loss: 0.5230\n",
      "2025-04-04 20:25:05,076 - INFO - Epoch [4/10], Batch [40/747], Loss: 0.4977\n",
      "2025-04-04 20:25:05,620 - INFO - Epoch [4/10], Batch [50/747], Loss: 0.5960\n",
      "2025-04-04 20:25:06,119 - INFO - Epoch [4/10], Batch [60/747], Loss: 0.4372\n",
      "2025-04-04 20:25:06,665 - INFO - Epoch [4/10], Batch [70/747], Loss: 0.3425\n",
      "2025-04-04 20:25:07,177 - INFO - Epoch [4/10], Batch [80/747], Loss: 0.5788\n",
      "2025-04-04 20:25:07,712 - INFO - Epoch [4/10], Batch [90/747], Loss: 0.4796\n",
      "2025-04-04 20:25:08,242 - INFO - Epoch [4/10], Batch [100/747], Loss: 0.3922\n",
      "2025-04-04 20:25:08,754 - INFO - Epoch [4/10], Batch [110/747], Loss: 0.5114\n",
      "2025-04-04 20:25:09,309 - INFO - Epoch [4/10], Batch [120/747], Loss: 0.3452\n",
      "2025-04-04 20:25:09,819 - INFO - Epoch [4/10], Batch [130/747], Loss: 0.5288\n",
      "2025-04-04 20:25:10,377 - INFO - Epoch [4/10], Batch [140/747], Loss: 0.4030\n",
      "2025-04-04 20:25:10,886 - INFO - Epoch [4/10], Batch [150/747], Loss: 0.4360\n",
      "2025-04-04 20:25:11,451 - INFO - Epoch [4/10], Batch [160/747], Loss: 0.3694\n",
      "2025-04-04 20:25:11,957 - INFO - Epoch [4/10], Batch [170/747], Loss: 0.4376\n",
      "2025-04-04 20:25:12,514 - INFO - Epoch [4/10], Batch [180/747], Loss: 0.4015\n",
      "2025-04-04 20:25:13,018 - INFO - Epoch [4/10], Batch [190/747], Loss: 0.4516\n",
      "2025-04-04 20:25:13,583 - INFO - Epoch [4/10], Batch [200/747], Loss: 0.6341\n",
      "2025-04-04 20:25:14,089 - INFO - Epoch [4/10], Batch [210/747], Loss: 0.4023\n",
      "2025-04-04 20:25:14,641 - INFO - Epoch [4/10], Batch [220/747], Loss: 0.4862\n",
      "2025-04-04 20:25:15,148 - INFO - Epoch [4/10], Batch [230/747], Loss: 0.3998\n",
      "2025-04-04 20:25:15,707 - INFO - Epoch [4/10], Batch [240/747], Loss: 0.4225\n",
      "2025-04-04 20:25:16,214 - INFO - Epoch [4/10], Batch [250/747], Loss: 0.3412\n",
      "2025-04-04 20:25:16,772 - INFO - Epoch [4/10], Batch [260/747], Loss: 0.5271\n",
      "2025-04-04 20:25:17,281 - INFO - Epoch [4/10], Batch [270/747], Loss: 0.5419\n",
      "2025-04-04 20:25:17,841 - INFO - Epoch [4/10], Batch [280/747], Loss: 0.3319\n",
      "2025-04-04 20:25:18,351 - INFO - Epoch [4/10], Batch [290/747], Loss: 0.4394\n",
      "2025-04-04 20:25:18,907 - INFO - Epoch [4/10], Batch [300/747], Loss: 0.4867\n",
      "2025-04-04 20:25:19,413 - INFO - Epoch [4/10], Batch [310/747], Loss: 0.4132\n",
      "2025-04-04 20:25:19,966 - INFO - Epoch [4/10], Batch [320/747], Loss: 0.4560\n",
      "2025-04-04 20:25:20,472 - INFO - Epoch [4/10], Batch [330/747], Loss: 0.3186\n",
      "2025-04-04 20:25:21,029 - INFO - Epoch [4/10], Batch [340/747], Loss: 0.4009\n",
      "2025-04-04 20:25:21,531 - INFO - Epoch [4/10], Batch [350/747], Loss: 0.4009\n",
      "2025-04-04 20:25:22,086 - INFO - Epoch [4/10], Batch [360/747], Loss: 0.4856\n",
      "2025-04-04 20:25:22,591 - INFO - Epoch [4/10], Batch [370/747], Loss: 0.4723\n",
      "2025-04-04 20:25:23,148 - INFO - Epoch [4/10], Batch [380/747], Loss: 0.3801\n",
      "2025-04-04 20:25:23,653 - INFO - Epoch [4/10], Batch [390/747], Loss: 0.4890\n",
      "2025-04-04 20:25:24,208 - INFO - Epoch [4/10], Batch [400/747], Loss: 0.5417\n",
      "2025-04-04 20:25:24,711 - INFO - Epoch [4/10], Batch [410/747], Loss: 0.5086\n",
      "2025-04-04 20:25:25,268 - INFO - Epoch [4/10], Batch [420/747], Loss: 0.5618\n",
      "2025-04-04 20:25:25,776 - INFO - Epoch [4/10], Batch [430/747], Loss: 0.4290\n",
      "2025-04-04 20:25:26,334 - INFO - Epoch [4/10], Batch [440/747], Loss: 0.4031\n",
      "2025-04-04 20:25:26,841 - INFO - Epoch [4/10], Batch [450/747], Loss: 0.4639\n",
      "2025-04-04 20:25:27,396 - INFO - Epoch [4/10], Batch [460/747], Loss: 0.5229\n",
      "2025-04-04 20:25:27,900 - INFO - Epoch [4/10], Batch [470/747], Loss: 0.4582\n",
      "2025-04-04 20:25:28,460 - INFO - Epoch [4/10], Batch [480/747], Loss: 0.4756\n",
      "2025-04-04 20:25:28,967 - INFO - Epoch [4/10], Batch [490/747], Loss: 0.3736\n",
      "2025-04-04 20:25:29,527 - INFO - Epoch [4/10], Batch [500/747], Loss: 0.6470\n",
      "2025-04-04 20:25:30,033 - INFO - Epoch [4/10], Batch [510/747], Loss: 0.5384\n",
      "2025-04-04 20:25:30,594 - INFO - Epoch [4/10], Batch [520/747], Loss: 0.3821\n",
      "2025-04-04 20:25:31,109 - INFO - Epoch [4/10], Batch [530/747], Loss: 0.4950\n",
      "2025-04-04 20:25:31,672 - INFO - Epoch [4/10], Batch [540/747], Loss: 0.4492\n",
      "2025-04-04 20:25:32,178 - INFO - Epoch [4/10], Batch [550/747], Loss: 0.4153\n",
      "2025-04-04 20:25:32,735 - INFO - Epoch [4/10], Batch [560/747], Loss: 0.5300\n",
      "2025-04-04 20:25:33,239 - INFO - Epoch [4/10], Batch [570/747], Loss: 0.3440\n",
      "2025-04-04 20:25:33,798 - INFO - Epoch [4/10], Batch [580/747], Loss: 0.2967\n",
      "2025-04-04 20:25:34,305 - INFO - Epoch [4/10], Batch [590/747], Loss: 0.2994\n",
      "2025-04-04 20:25:34,865 - INFO - Epoch [4/10], Batch [600/747], Loss: 0.2672\n",
      "2025-04-04 20:25:35,370 - INFO - Epoch [4/10], Batch [610/747], Loss: 0.2593\n",
      "2025-04-04 20:25:35,930 - INFO - Epoch [4/10], Batch [620/747], Loss: 0.4707\n",
      "2025-04-04 20:25:36,436 - INFO - Epoch [4/10], Batch [630/747], Loss: 0.5924\n",
      "2025-04-04 20:25:36,995 - INFO - Epoch [4/10], Batch [640/747], Loss: 0.3977\n",
      "2025-04-04 20:25:37,504 - INFO - Epoch [4/10], Batch [650/747], Loss: 0.4378\n",
      "2025-04-04 20:25:38,065 - INFO - Epoch [4/10], Batch [660/747], Loss: 0.2734\n",
      "2025-04-04 20:25:38,570 - INFO - Epoch [4/10], Batch [670/747], Loss: 0.5520\n",
      "2025-04-04 20:25:39,129 - INFO - Epoch [4/10], Batch [680/747], Loss: 0.3601\n",
      "2025-04-04 20:25:39,635 - INFO - Epoch [4/10], Batch [690/747], Loss: 0.4355\n",
      "2025-04-04 20:25:40,196 - INFO - Epoch [4/10], Batch [700/747], Loss: 0.6326\n",
      "2025-04-04 20:25:40,705 - INFO - Epoch [4/10], Batch [710/747], Loss: 0.5723\n",
      "2025-04-04 20:25:41,264 - INFO - Epoch [4/10], Batch [720/747], Loss: 0.3621\n",
      "2025-04-04 20:25:41,769 - INFO - Epoch [4/10], Batch [730/747], Loss: 0.3592\n",
      "2025-04-04 20:25:42,328 - INFO - Epoch [4/10], Batch [740/747], Loss: 0.3967\n",
      "2025-04-04 20:25:42,653 - INFO - Epoch 4/10 Train Loss: 0.4322, Train Accuracy: 79.73%\n",
      "2025-04-04 20:25:46,347 - INFO - Epoch 4/10 Val Loss: 0.3634, Val Accuracy: 84.91%, AUC-ROC: 0.9137\n",
      "2025-04-04 20:25:46,351 - INFO - New best model at epoch 4 with Val Accuracy: 84.91%\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:25:46,816 - INFO - Epoch [5/10], Batch [0/747], Loss: 0.4008\n",
      "2025-04-04 20:25:47,292 - INFO - Epoch [5/10], Batch [10/747], Loss: 0.3997\n",
      "2025-04-04 20:25:47,847 - INFO - Epoch [5/10], Batch [20/747], Loss: 0.2269\n",
      "2025-04-04 20:25:48,352 - INFO - Epoch [5/10], Batch [30/747], Loss: 0.2554\n",
      "2025-04-04 20:25:48,905 - INFO - Epoch [5/10], Batch [40/747], Loss: 0.3840\n",
      "2025-04-04 20:25:49,409 - INFO - Epoch [5/10], Batch [50/747], Loss: 0.3820\n",
      "2025-04-04 20:25:49,959 - INFO - Epoch [5/10], Batch [60/747], Loss: 0.3883\n",
      "2025-04-04 20:25:50,463 - INFO - Epoch [5/10], Batch [70/747], Loss: 0.4432\n",
      "2025-04-04 20:25:51,016 - INFO - Epoch [5/10], Batch [80/747], Loss: 0.3779\n",
      "2025-04-04 20:25:51,522 - INFO - Epoch [5/10], Batch [90/747], Loss: 0.5320\n",
      "2025-04-04 20:25:52,074 - INFO - Epoch [5/10], Batch [100/747], Loss: 0.4205\n",
      "2025-04-04 20:25:52,576 - INFO - Epoch [5/10], Batch [110/747], Loss: 0.3502\n",
      "2025-04-04 20:25:53,143 - INFO - Epoch [5/10], Batch [120/747], Loss: 0.3597\n",
      "2025-04-04 20:25:53,644 - INFO - Epoch [5/10], Batch [130/747], Loss: 0.3913\n",
      "2025-04-04 20:25:54,191 - INFO - Epoch [5/10], Batch [140/747], Loss: 0.5063\n",
      "2025-04-04 20:25:54,698 - INFO - Epoch [5/10], Batch [150/747], Loss: 0.2301\n",
      "2025-04-04 20:25:55,253 - INFO - Epoch [5/10], Batch [160/747], Loss: 0.5412\n",
      "2025-04-04 20:25:55,757 - INFO - Epoch [5/10], Batch [170/747], Loss: 0.3296\n",
      "2025-04-04 20:25:56,309 - INFO - Epoch [5/10], Batch [180/747], Loss: 0.4996\n",
      "2025-04-04 20:25:56,814 - INFO - Epoch [5/10], Batch [190/747], Loss: 0.3886\n",
      "2025-04-04 20:25:57,371 - INFO - Epoch [5/10], Batch [200/747], Loss: 0.4960\n",
      "2025-04-04 20:25:57,876 - INFO - Epoch [5/10], Batch [210/747], Loss: 0.2769\n",
      "2025-04-04 20:25:58,433 - INFO - Epoch [5/10], Batch [220/747], Loss: 0.4797\n",
      "2025-04-04 20:25:58,937 - INFO - Epoch [5/10], Batch [230/747], Loss: 0.4316\n",
      "2025-04-04 20:25:59,493 - INFO - Epoch [5/10], Batch [240/747], Loss: 0.4548\n",
      "2025-04-04 20:25:59,996 - INFO - Epoch [5/10], Batch [250/747], Loss: 0.4954\n",
      "2025-04-04 20:26:00,552 - INFO - Epoch [5/10], Batch [260/747], Loss: 0.2189\n",
      "2025-04-04 20:26:01,055 - INFO - Epoch [5/10], Batch [270/747], Loss: 0.3763\n",
      "2025-04-04 20:26:01,610 - INFO - Epoch [5/10], Batch [280/747], Loss: 0.4289\n",
      "2025-04-04 20:26:02,116 - INFO - Epoch [5/10], Batch [290/747], Loss: 0.3749\n",
      "2025-04-04 20:26:02,674 - INFO - Epoch [5/10], Batch [300/747], Loss: 0.4354\n",
      "2025-04-04 20:26:03,174 - INFO - Epoch [5/10], Batch [310/747], Loss: 0.4671\n",
      "2025-04-04 20:26:03,729 - INFO - Epoch [5/10], Batch [320/747], Loss: 0.5560\n",
      "2025-04-04 20:26:04,236 - INFO - Epoch [5/10], Batch [330/747], Loss: 0.2312\n",
      "2025-04-04 20:26:04,790 - INFO - Epoch [5/10], Batch [340/747], Loss: 0.4412\n",
      "2025-04-04 20:26:05,294 - INFO - Epoch [5/10], Batch [350/747], Loss: 0.4248\n",
      "2025-04-04 20:26:05,855 - INFO - Epoch [5/10], Batch [360/747], Loss: 0.3492\n",
      "2025-04-04 20:26:06,360 - INFO - Epoch [5/10], Batch [370/747], Loss: 0.3048\n",
      "2025-04-04 20:26:06,914 - INFO - Epoch [5/10], Batch [380/747], Loss: 0.4477\n",
      "2025-04-04 20:26:07,418 - INFO - Epoch [5/10], Batch [390/747], Loss: 0.3889\n",
      "2025-04-04 20:26:07,975 - INFO - Epoch [5/10], Batch [400/747], Loss: 0.4550\n",
      "2025-04-04 20:26:08,481 - INFO - Epoch [5/10], Batch [410/747], Loss: 0.3665\n",
      "2025-04-04 20:26:09,034 - INFO - Epoch [5/10], Batch [420/747], Loss: 0.4638\n",
      "2025-04-04 20:26:09,539 - INFO - Epoch [5/10], Batch [430/747], Loss: 0.2976\n",
      "2025-04-04 20:26:10,094 - INFO - Epoch [5/10], Batch [440/747], Loss: 0.3147\n",
      "2025-04-04 20:26:10,596 - INFO - Epoch [5/10], Batch [450/747], Loss: 0.2546\n",
      "2025-04-04 20:26:11,152 - INFO - Epoch [5/10], Batch [460/747], Loss: 0.4490\n",
      "2025-04-04 20:26:11,657 - INFO - Epoch [5/10], Batch [470/747], Loss: 0.4979\n",
      "2025-04-04 20:26:12,209 - INFO - Epoch [5/10], Batch [480/747], Loss: 0.2262\n",
      "2025-04-04 20:26:12,713 - INFO - Epoch [5/10], Batch [490/747], Loss: 0.2002\n",
      "2025-04-04 20:26:13,280 - INFO - Epoch [5/10], Batch [500/747], Loss: 0.5448\n",
      "2025-04-04 20:26:13,784 - INFO - Epoch [5/10], Batch [510/747], Loss: 0.4461\n",
      "2025-04-04 20:26:14,338 - INFO - Epoch [5/10], Batch [520/747], Loss: 0.6570\n",
      "2025-04-04 20:26:14,836 - INFO - Epoch [5/10], Batch [530/747], Loss: 0.4409\n",
      "2025-04-04 20:26:15,391 - INFO - Epoch [5/10], Batch [540/747], Loss: 0.3447\n",
      "2025-04-04 20:26:15,894 - INFO - Epoch [5/10], Batch [550/747], Loss: 0.3895\n",
      "2025-04-04 20:26:16,445 - INFO - Epoch [5/10], Batch [560/747], Loss: 0.6413\n",
      "2025-04-04 20:26:16,953 - INFO - Epoch [5/10], Batch [570/747], Loss: 0.3626\n",
      "2025-04-04 20:26:17,510 - INFO - Epoch [5/10], Batch [580/747], Loss: 0.3278\n",
      "2025-04-04 20:26:18,012 - INFO - Epoch [5/10], Batch [590/747], Loss: 0.3811\n",
      "2025-04-04 20:26:18,569 - INFO - Epoch [5/10], Batch [600/747], Loss: 0.4156\n",
      "2025-04-04 20:26:19,072 - INFO - Epoch [5/10], Batch [610/747], Loss: 0.2513\n",
      "2025-04-04 20:26:19,630 - INFO - Epoch [5/10], Batch [620/747], Loss: 0.3373\n",
      "2025-04-04 20:26:20,134 - INFO - Epoch [5/10], Batch [630/747], Loss: 0.2706\n",
      "2025-04-04 20:26:20,690 - INFO - Epoch [5/10], Batch [640/747], Loss: 0.3573\n",
      "2025-04-04 20:26:21,191 - INFO - Epoch [5/10], Batch [650/747], Loss: 0.4043\n",
      "2025-04-04 20:26:21,742 - INFO - Epoch [5/10], Batch [660/747], Loss: 0.4094\n",
      "2025-04-04 20:26:22,245 - INFO - Epoch [5/10], Batch [670/747], Loss: 0.4527\n",
      "2025-04-04 20:26:22,800 - INFO - Epoch [5/10], Batch [680/747], Loss: 0.4096\n",
      "2025-04-04 20:26:23,307 - INFO - Epoch [5/10], Batch [690/747], Loss: 0.3591\n",
      "2025-04-04 20:26:23,861 - INFO - Epoch [5/10], Batch [700/747], Loss: 0.3838\n",
      "2025-04-04 20:26:24,363 - INFO - Epoch [5/10], Batch [710/747], Loss: 0.4711\n",
      "2025-04-04 20:26:24,920 - INFO - Epoch [5/10], Batch [720/747], Loss: 0.3097\n",
      "2025-04-04 20:26:25,424 - INFO - Epoch [5/10], Batch [730/747], Loss: 0.2999\n",
      "2025-04-04 20:26:25,981 - INFO - Epoch [5/10], Batch [740/747], Loss: 0.2830\n",
      "2025-04-04 20:26:26,293 - INFO - Epoch 5/10 Train Loss: 0.4049, Train Accuracy: 81.27%\n",
      "2025-04-04 20:26:29,995 - INFO - Epoch 5/10 Val Loss: 0.3471, Val Accuracy: 85.91%, AUC-ROC: 0.9267\n",
      "2025-04-04 20:26:29,999 - INFO - New best model at epoch 5 with Val Accuracy: 85.91%\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:26:30,452 - INFO - Epoch [6/10], Batch [0/747], Loss: 0.4333\n",
      "2025-04-04 20:26:30,917 - INFO - Epoch [6/10], Batch [10/747], Loss: 0.3659\n",
      "2025-04-04 20:26:31,442 - INFO - Epoch [6/10], Batch [20/747], Loss: 0.2712\n",
      "2025-04-04 20:26:31,944 - INFO - Epoch [6/10], Batch [30/747], Loss: 0.5729\n",
      "2025-04-04 20:26:32,459 - INFO - Epoch [6/10], Batch [40/747], Loss: 0.2703\n",
      "2025-04-04 20:26:33,003 - INFO - Epoch [6/10], Batch [50/747], Loss: 0.5380\n",
      "2025-04-04 20:26:33,506 - INFO - Epoch [6/10], Batch [60/747], Loss: 0.2917\n",
      "2025-04-04 20:26:34,059 - INFO - Epoch [6/10], Batch [70/747], Loss: 0.3035\n",
      "2025-04-04 20:26:34,562 - INFO - Epoch [6/10], Batch [80/747], Loss: 0.5146\n",
      "2025-04-04 20:26:35,124 - INFO - Epoch [6/10], Batch [90/747], Loss: 0.3871\n",
      "2025-04-04 20:26:35,637 - INFO - Epoch [6/10], Batch [100/747], Loss: 0.3016\n",
      "2025-04-04 20:26:36,189 - INFO - Epoch [6/10], Batch [110/747], Loss: 0.4038\n",
      "2025-04-04 20:26:36,686 - INFO - Epoch [6/10], Batch [120/747], Loss: 0.4472\n",
      "2025-04-04 20:26:37,241 - INFO - Epoch [6/10], Batch [130/747], Loss: 0.3007\n",
      "2025-04-04 20:26:37,745 - INFO - Epoch [6/10], Batch [140/747], Loss: 0.3395\n",
      "2025-04-04 20:26:38,295 - INFO - Epoch [6/10], Batch [150/747], Loss: 0.3777\n",
      "2025-04-04 20:26:38,796 - INFO - Epoch [6/10], Batch [160/747], Loss: 0.2855\n",
      "2025-04-04 20:26:39,355 - INFO - Epoch [6/10], Batch [170/747], Loss: 0.4113\n",
      "2025-04-04 20:26:39,856 - INFO - Epoch [6/10], Batch [180/747], Loss: 0.1883\n",
      "2025-04-04 20:26:40,409 - INFO - Epoch [6/10], Batch [190/747], Loss: 0.4135\n",
      "2025-04-04 20:26:40,915 - INFO - Epoch [6/10], Batch [200/747], Loss: 0.4569\n",
      "2025-04-04 20:26:41,469 - INFO - Epoch [6/10], Batch [210/747], Loss: 0.3470\n",
      "2025-04-04 20:26:41,972 - INFO - Epoch [6/10], Batch [220/747], Loss: 0.3296\n",
      "2025-04-04 20:26:42,527 - INFO - Epoch [6/10], Batch [230/747], Loss: 0.3074\n",
      "2025-04-04 20:26:43,032 - INFO - Epoch [6/10], Batch [240/747], Loss: 0.3394\n",
      "2025-04-04 20:26:43,583 - INFO - Epoch [6/10], Batch [250/747], Loss: 0.6129\n",
      "2025-04-04 20:26:44,087 - INFO - Epoch [6/10], Batch [260/747], Loss: 0.2550\n",
      "2025-04-04 20:26:44,644 - INFO - Epoch [6/10], Batch [270/747], Loss: 0.3984\n",
      "2025-04-04 20:26:45,145 - INFO - Epoch [6/10], Batch [280/747], Loss: 0.3844\n",
      "2025-04-04 20:26:45,700 - INFO - Epoch [6/10], Batch [290/747], Loss: 0.5019\n",
      "2025-04-04 20:26:46,205 - INFO - Epoch [6/10], Batch [300/747], Loss: 0.3042\n",
      "2025-04-04 20:26:46,756 - INFO - Epoch [6/10], Batch [310/747], Loss: 0.3748\n",
      "2025-04-04 20:26:47,262 - INFO - Epoch [6/10], Batch [320/747], Loss: 0.2535\n",
      "2025-04-04 20:26:47,820 - INFO - Epoch [6/10], Batch [330/747], Loss: 0.3099\n",
      "2025-04-04 20:26:48,323 - INFO - Epoch [6/10], Batch [340/747], Loss: 0.4473\n",
      "2025-04-04 20:26:48,880 - INFO - Epoch [6/10], Batch [350/747], Loss: 0.5816\n",
      "2025-04-04 20:26:49,379 - INFO - Epoch [6/10], Batch [360/747], Loss: 0.2435\n",
      "2025-04-04 20:26:49,932 - INFO - Epoch [6/10], Batch [370/747], Loss: 0.3108\n",
      "2025-04-04 20:26:50,436 - INFO - Epoch [6/10], Batch [380/747], Loss: 0.3906\n",
      "2025-04-04 20:26:50,988 - INFO - Epoch [6/10], Batch [390/747], Loss: 0.3867\n",
      "2025-04-04 20:26:51,491 - INFO - Epoch [6/10], Batch [400/747], Loss: 0.4771\n",
      "2025-04-04 20:26:52,049 - INFO - Epoch [6/10], Batch [410/747], Loss: 0.3405\n",
      "2025-04-04 20:26:52,551 - INFO - Epoch [6/10], Batch [420/747], Loss: 0.2465\n",
      "2025-04-04 20:26:53,105 - INFO - Epoch [6/10], Batch [430/747], Loss: 0.3677\n",
      "2025-04-04 20:26:53,608 - INFO - Epoch [6/10], Batch [440/747], Loss: 0.3563\n",
      "2025-04-04 20:26:54,164 - INFO - Epoch [6/10], Batch [450/747], Loss: 0.4297\n",
      "2025-04-04 20:26:54,667 - INFO - Epoch [6/10], Batch [460/747], Loss: 0.6718\n",
      "2025-04-04 20:26:55,229 - INFO - Epoch [6/10], Batch [470/747], Loss: 0.5260\n",
      "2025-04-04 20:26:55,744 - INFO - Epoch [6/10], Batch [480/747], Loss: 0.3508\n",
      "2025-04-04 20:26:56,300 - INFO - Epoch [6/10], Batch [490/747], Loss: 0.4756\n",
      "2025-04-04 20:26:56,806 - INFO - Epoch [6/10], Batch [500/747], Loss: 0.3104\n",
      "2025-04-04 20:26:57,362 - INFO - Epoch [6/10], Batch [510/747], Loss: 0.4083\n",
      "2025-04-04 20:26:57,866 - INFO - Epoch [6/10], Batch [520/747], Loss: 0.4248\n",
      "2025-04-04 20:26:58,423 - INFO - Epoch [6/10], Batch [530/747], Loss: 0.4671\n",
      "2025-04-04 20:26:58,924 - INFO - Epoch [6/10], Batch [540/747], Loss: 0.4408\n",
      "2025-04-04 20:26:59,480 - INFO - Epoch [6/10], Batch [550/747], Loss: 0.2184\n",
      "2025-04-04 20:26:59,984 - INFO - Epoch [6/10], Batch [560/747], Loss: 0.2239\n",
      "2025-04-04 20:27:00,540 - INFO - Epoch [6/10], Batch [570/747], Loss: 0.2434\n",
      "2025-04-04 20:27:01,047 - INFO - Epoch [6/10], Batch [580/747], Loss: 0.3047\n",
      "2025-04-04 20:27:01,604 - INFO - Epoch [6/10], Batch [590/747], Loss: 0.5154\n",
      "2025-04-04 20:27:02,106 - INFO - Epoch [6/10], Batch [600/747], Loss: 0.2015\n",
      "2025-04-04 20:27:02,664 - INFO - Epoch [6/10], Batch [610/747], Loss: 0.3568\n",
      "2025-04-04 20:27:03,168 - INFO - Epoch [6/10], Batch [620/747], Loss: 0.4239\n",
      "2025-04-04 20:27:03,720 - INFO - Epoch [6/10], Batch [630/747], Loss: 0.3234\n",
      "2025-04-04 20:27:04,225 - INFO - Epoch [6/10], Batch [640/747], Loss: 0.3761\n",
      "2025-04-04 20:27:04,780 - INFO - Epoch [6/10], Batch [650/747], Loss: 0.5040\n",
      "2025-04-04 20:27:05,284 - INFO - Epoch [6/10], Batch [660/747], Loss: 0.2444\n",
      "2025-04-04 20:27:05,837 - INFO - Epoch [6/10], Batch [670/747], Loss: 0.2689\n",
      "2025-04-04 20:27:06,339 - INFO - Epoch [6/10], Batch [680/747], Loss: 0.3075\n",
      "2025-04-04 20:27:06,895 - INFO - Epoch [6/10], Batch [690/747], Loss: 0.3146\n",
      "2025-04-04 20:27:07,403 - INFO - Epoch [6/10], Batch [700/747], Loss: 0.5241\n",
      "2025-04-04 20:27:07,961 - INFO - Epoch [6/10], Batch [710/747], Loss: 0.4455\n",
      "2025-04-04 20:27:08,467 - INFO - Epoch [6/10], Batch [720/747], Loss: 0.4804\n",
      "2025-04-04 20:27:09,021 - INFO - Epoch [6/10], Batch [730/747], Loss: 0.6006\n",
      "2025-04-04 20:27:09,525 - INFO - Epoch [6/10], Batch [740/747], Loss: 0.4209\n",
      "2025-04-04 20:27:09,908 - INFO - Epoch 6/10 Train Loss: 0.3829, Train Accuracy: 82.45%\n",
      "2025-04-04 20:27:13,599 - INFO - Epoch 6/10 Val Loss: 0.3154, Val Accuracy: 86.72%, AUC-ROC: 0.9328\n",
      "2025-04-04 20:27:13,602 - INFO - New best model at epoch 6 with Val Accuracy: 86.72%\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:27:14,056 - INFO - Epoch [7/10], Batch [0/747], Loss: 0.2017\n",
      "2025-04-04 20:27:14,536 - INFO - Epoch [7/10], Batch [10/747], Loss: 0.2695\n",
      "2025-04-04 20:27:15,099 - INFO - Epoch [7/10], Batch [20/747], Loss: 0.4475\n",
      "2025-04-04 20:27:15,609 - INFO - Epoch [7/10], Batch [30/747], Loss: 0.4422\n",
      "2025-04-04 20:27:16,167 - INFO - Epoch [7/10], Batch [40/747], Loss: 0.3093\n",
      "2025-04-04 20:27:16,676 - INFO - Epoch [7/10], Batch [50/747], Loss: 0.3591\n",
      "2025-04-04 20:27:17,242 - INFO - Epoch [7/10], Batch [60/747], Loss: 0.4728\n",
      "2025-04-04 20:27:17,753 - INFO - Epoch [7/10], Batch [70/747], Loss: 0.3310\n",
      "2025-04-04 20:27:18,314 - INFO - Epoch [7/10], Batch [80/747], Loss: 0.3258\n",
      "2025-04-04 20:27:18,819 - INFO - Epoch [7/10], Batch [90/747], Loss: 0.3668\n",
      "2025-04-04 20:27:19,380 - INFO - Epoch [7/10], Batch [100/747], Loss: 0.3428\n",
      "2025-04-04 20:27:19,885 - INFO - Epoch [7/10], Batch [110/747], Loss: 0.2184\n",
      "2025-04-04 20:27:20,446 - INFO - Epoch [7/10], Batch [120/747], Loss: 0.3390\n",
      "2025-04-04 20:27:20,953 - INFO - Epoch [7/10], Batch [130/747], Loss: 0.5359\n",
      "2025-04-04 20:27:21,510 - INFO - Epoch [7/10], Batch [140/747], Loss: 0.3667\n",
      "2025-04-04 20:27:22,015 - INFO - Epoch [7/10], Batch [150/747], Loss: 0.4196\n",
      "2025-04-04 20:27:22,573 - INFO - Epoch [7/10], Batch [160/747], Loss: 0.3681\n",
      "2025-04-04 20:27:23,076 - INFO - Epoch [7/10], Batch [170/747], Loss: 0.2612\n",
      "2025-04-04 20:27:23,638 - INFO - Epoch [7/10], Batch [180/747], Loss: 0.3804\n",
      "2025-04-04 20:27:24,140 - INFO - Epoch [7/10], Batch [190/747], Loss: 0.5296\n",
      "2025-04-04 20:27:24,699 - INFO - Epoch [7/10], Batch [200/747], Loss: 0.4030\n",
      "2025-04-04 20:27:25,204 - INFO - Epoch [7/10], Batch [210/747], Loss: 0.3167\n",
      "2025-04-04 20:27:25,764 - INFO - Epoch [7/10], Batch [220/747], Loss: 0.4413\n",
      "2025-04-04 20:27:26,269 - INFO - Epoch [7/10], Batch [230/747], Loss: 0.3278\n",
      "2025-04-04 20:27:26,825 - INFO - Epoch [7/10], Batch [240/747], Loss: 0.3049\n",
      "2025-04-04 20:27:27,331 - INFO - Epoch [7/10], Batch [250/747], Loss: 0.3866\n",
      "2025-04-04 20:27:27,889 - INFO - Epoch [7/10], Batch [260/747], Loss: 0.4158\n",
      "2025-04-04 20:27:28,394 - INFO - Epoch [7/10], Batch [270/747], Loss: 0.3584\n",
      "2025-04-04 20:27:28,952 - INFO - Epoch [7/10], Batch [280/747], Loss: 0.3804\n",
      "2025-04-04 20:27:29,460 - INFO - Epoch [7/10], Batch [290/747], Loss: 0.2886\n",
      "2025-04-04 20:27:30,021 - INFO - Epoch [7/10], Batch [300/747], Loss: 0.3015\n",
      "2025-04-04 20:27:30,528 - INFO - Epoch [7/10], Batch [310/747], Loss: 0.5341\n",
      "2025-04-04 20:27:31,086 - INFO - Epoch [7/10], Batch [320/747], Loss: 0.2931\n",
      "2025-04-04 20:27:31,594 - INFO - Epoch [7/10], Batch [330/747], Loss: 0.2131\n",
      "2025-04-04 20:27:32,155 - INFO - Epoch [7/10], Batch [340/747], Loss: 0.2227\n",
      "2025-04-04 20:27:32,660 - INFO - Epoch [7/10], Batch [350/747], Loss: 0.2631\n",
      "2025-04-04 20:27:33,217 - INFO - Epoch [7/10], Batch [360/747], Loss: 0.3038\n",
      "2025-04-04 20:27:33,723 - INFO - Epoch [7/10], Batch [370/747], Loss: 0.4498\n",
      "2025-04-04 20:27:34,283 - INFO - Epoch [7/10], Batch [380/747], Loss: 0.2253\n",
      "2025-04-04 20:27:34,791 - INFO - Epoch [7/10], Batch [390/747], Loss: 0.3775\n",
      "2025-04-04 20:27:35,353 - INFO - Epoch [7/10], Batch [400/747], Loss: 0.4131\n",
      "2025-04-04 20:27:35,858 - INFO - Epoch [7/10], Batch [410/747], Loss: 0.3444\n",
      "2025-04-04 20:27:36,423 - INFO - Epoch [7/10], Batch [420/747], Loss: 0.3791\n",
      "2025-04-04 20:27:36,930 - INFO - Epoch [7/10], Batch [430/747], Loss: 0.2244\n",
      "2025-04-04 20:27:37,495 - INFO - Epoch [7/10], Batch [440/747], Loss: 0.4168\n",
      "2025-04-04 20:27:38,011 - INFO - Epoch [7/10], Batch [450/747], Loss: 0.3149\n",
      "2025-04-04 20:27:38,572 - INFO - Epoch [7/10], Batch [460/747], Loss: 0.3022\n",
      "2025-04-04 20:27:39,078 - INFO - Epoch [7/10], Batch [470/747], Loss: 0.2348\n",
      "2025-04-04 20:27:39,638 - INFO - Epoch [7/10], Batch [480/747], Loss: 0.3692\n",
      "2025-04-04 20:27:40,138 - INFO - Epoch [7/10], Batch [490/747], Loss: 0.3372\n",
      "2025-04-04 20:27:40,699 - INFO - Epoch [7/10], Batch [500/747], Loss: 0.4543\n",
      "2025-04-04 20:27:41,206 - INFO - Epoch [7/10], Batch [510/747], Loss: 0.3601\n",
      "2025-04-04 20:27:41,766 - INFO - Epoch [7/10], Batch [520/747], Loss: 0.2536\n",
      "2025-04-04 20:27:42,271 - INFO - Epoch [7/10], Batch [530/747], Loss: 0.2109\n",
      "2025-04-04 20:27:42,827 - INFO - Epoch [7/10], Batch [540/747], Loss: 0.4852\n",
      "2025-04-04 20:27:43,335 - INFO - Epoch [7/10], Batch [550/747], Loss: 0.3726\n",
      "2025-04-04 20:27:43,894 - INFO - Epoch [7/10], Batch [560/747], Loss: 0.4169\n",
      "2025-04-04 20:27:44,400 - INFO - Epoch [7/10], Batch [570/747], Loss: 0.3740\n",
      "2025-04-04 20:27:44,959 - INFO - Epoch [7/10], Batch [580/747], Loss: 0.3641\n",
      "2025-04-04 20:27:45,464 - INFO - Epoch [7/10], Batch [590/747], Loss: 0.3200\n",
      "2025-04-04 20:27:46,023 - INFO - Epoch [7/10], Batch [600/747], Loss: 0.4485\n",
      "2025-04-04 20:27:46,530 - INFO - Epoch [7/10], Batch [610/747], Loss: 0.2854\n",
      "2025-04-04 20:27:47,090 - INFO - Epoch [7/10], Batch [620/747], Loss: 0.4568\n",
      "2025-04-04 20:27:47,597 - INFO - Epoch [7/10], Batch [630/747], Loss: 0.2793\n",
      "2025-04-04 20:27:48,155 - INFO - Epoch [7/10], Batch [640/747], Loss: 0.3450\n",
      "2025-04-04 20:27:48,662 - INFO - Epoch [7/10], Batch [650/747], Loss: 0.3314\n",
      "2025-04-04 20:27:49,224 - INFO - Epoch [7/10], Batch [660/747], Loss: 0.3326\n",
      "2025-04-04 20:27:49,730 - INFO - Epoch [7/10], Batch [670/747], Loss: 0.1997\n",
      "2025-04-04 20:27:50,284 - INFO - Epoch [7/10], Batch [680/747], Loss: 0.2670\n",
      "2025-04-04 20:27:50,787 - INFO - Epoch [7/10], Batch [690/747], Loss: 0.3164\n",
      "2025-04-04 20:27:51,347 - INFO - Epoch [7/10], Batch [700/747], Loss: 0.2766\n",
      "2025-04-04 20:27:51,853 - INFO - Epoch [7/10], Batch [710/747], Loss: 0.4472\n",
      "2025-04-04 20:27:52,412 - INFO - Epoch [7/10], Batch [720/747], Loss: 0.4908\n",
      "2025-04-04 20:27:52,918 - INFO - Epoch [7/10], Batch [730/747], Loss: 0.2743\n",
      "2025-04-04 20:27:53,477 - INFO - Epoch [7/10], Batch [740/747], Loss: 0.4464\n",
      "2025-04-04 20:27:53,798 - INFO - Epoch 7/10 Train Loss: 0.3595, Train Accuracy: 83.50%\n",
      "2025-04-04 20:27:57,514 - INFO - Epoch 7/10 Val Loss: 0.3015, Val Accuracy: 87.14%, AUC-ROC: 0.9377\n",
      "2025-04-04 20:27:57,518 - INFO - New best model at epoch 7 with Val Accuracy: 87.14%\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:27:57,980 - INFO - Epoch [8/10], Batch [0/747], Loss: 0.2360\n",
      "2025-04-04 20:27:58,471 - INFO - Epoch [8/10], Batch [10/747], Loss: 0.2365\n",
      "2025-04-04 20:27:59,035 - INFO - Epoch [8/10], Batch [20/747], Loss: 0.2660\n",
      "2025-04-04 20:27:59,546 - INFO - Epoch [8/10], Batch [30/747], Loss: 0.3620\n",
      "2025-04-04 20:28:00,113 - INFO - Epoch [8/10], Batch [40/747], Loss: 0.3313\n",
      "2025-04-04 20:28:00,620 - INFO - Epoch [8/10], Batch [50/747], Loss: 0.3744\n",
      "2025-04-04 20:28:01,182 - INFO - Epoch [8/10], Batch [60/747], Loss: 0.3146\n",
      "2025-04-04 20:28:01,685 - INFO - Epoch [8/10], Batch [70/747], Loss: 0.3837\n",
      "2025-04-04 20:28:02,246 - INFO - Epoch [8/10], Batch [80/747], Loss: 0.2434\n",
      "2025-04-04 20:28:02,747 - INFO - Epoch [8/10], Batch [90/747], Loss: 0.4622\n",
      "2025-04-04 20:28:03,311 - INFO - Epoch [8/10], Batch [100/747], Loss: 0.4905\n",
      "2025-04-04 20:28:03,819 - INFO - Epoch [8/10], Batch [110/747], Loss: 0.6022\n",
      "2025-04-04 20:28:04,379 - INFO - Epoch [8/10], Batch [120/747], Loss: 0.3513\n",
      "2025-04-04 20:28:04,886 - INFO - Epoch [8/10], Batch [130/747], Loss: 0.3677\n",
      "2025-04-04 20:28:05,446 - INFO - Epoch [8/10], Batch [140/747], Loss: 0.2797\n",
      "2025-04-04 20:28:05,951 - INFO - Epoch [8/10], Batch [150/747], Loss: 0.2600\n",
      "2025-04-04 20:28:06,515 - INFO - Epoch [8/10], Batch [160/747], Loss: 0.3861\n",
      "2025-04-04 20:28:07,020 - INFO - Epoch [8/10], Batch [170/747], Loss: 0.4599\n",
      "2025-04-04 20:28:07,578 - INFO - Epoch [8/10], Batch [180/747], Loss: 0.4339\n",
      "2025-04-04 20:28:08,084 - INFO - Epoch [8/10], Batch [190/747], Loss: 0.2944\n",
      "2025-04-04 20:28:08,644 - INFO - Epoch [8/10], Batch [200/747], Loss: 0.2507\n",
      "2025-04-04 20:28:09,152 - INFO - Epoch [8/10], Batch [210/747], Loss: 0.4679\n",
      "2025-04-04 20:28:09,707 - INFO - Epoch [8/10], Batch [220/747], Loss: 0.3419\n",
      "2025-04-04 20:28:10,213 - INFO - Epoch [8/10], Batch [230/747], Loss: 0.4275\n",
      "2025-04-04 20:28:10,780 - INFO - Epoch [8/10], Batch [240/747], Loss: 0.4708\n",
      "2025-04-04 20:28:11,284 - INFO - Epoch [8/10], Batch [250/747], Loss: 0.3971\n",
      "2025-04-04 20:28:11,842 - INFO - Epoch [8/10], Batch [260/747], Loss: 0.2551\n",
      "2025-04-04 20:28:12,348 - INFO - Epoch [8/10], Batch [270/747], Loss: 0.5165\n",
      "2025-04-04 20:28:12,909 - INFO - Epoch [8/10], Batch [280/747], Loss: 0.3624\n",
      "2025-04-04 20:28:13,412 - INFO - Epoch [8/10], Batch [290/747], Loss: 0.2398\n",
      "2025-04-04 20:28:13,975 - INFO - Epoch [8/10], Batch [300/747], Loss: 0.3692\n",
      "2025-04-04 20:28:14,479 - INFO - Epoch [8/10], Batch [310/747], Loss: 0.3842\n",
      "2025-04-04 20:28:15,043 - INFO - Epoch [8/10], Batch [320/747], Loss: 0.3617\n",
      "2025-04-04 20:28:15,548 - INFO - Epoch [8/10], Batch [330/747], Loss: 0.4196\n",
      "2025-04-04 20:28:16,109 - INFO - Epoch [8/10], Batch [340/747], Loss: 0.2586\n",
      "2025-04-04 20:28:16,617 - INFO - Epoch [8/10], Batch [350/747], Loss: 0.2614\n",
      "2025-04-04 20:28:17,177 - INFO - Epoch [8/10], Batch [360/747], Loss: 0.2821\n",
      "2025-04-04 20:28:17,687 - INFO - Epoch [8/10], Batch [370/747], Loss: 0.3119\n",
      "2025-04-04 20:28:18,250 - INFO - Epoch [8/10], Batch [380/747], Loss: 0.2465\n",
      "2025-04-04 20:28:18,759 - INFO - Epoch [8/10], Batch [390/747], Loss: 0.3880\n",
      "2025-04-04 20:28:19,315 - INFO - Epoch [8/10], Batch [400/747], Loss: 0.5619\n",
      "2025-04-04 20:28:19,827 - INFO - Epoch [8/10], Batch [410/747], Loss: 0.4022\n",
      "2025-04-04 20:28:20,393 - INFO - Epoch [8/10], Batch [420/747], Loss: 0.3238\n",
      "2025-04-04 20:28:20,898 - INFO - Epoch [8/10], Batch [430/747], Loss: 0.2648\n",
      "2025-04-04 20:28:21,459 - INFO - Epoch [8/10], Batch [440/747], Loss: 0.2148\n",
      "2025-04-04 20:28:21,963 - INFO - Epoch [8/10], Batch [450/747], Loss: 0.2647\n",
      "2025-04-04 20:28:22,523 - INFO - Epoch [8/10], Batch [460/747], Loss: 0.3305\n",
      "2025-04-04 20:28:23,031 - INFO - Epoch [8/10], Batch [470/747], Loss: 0.4578\n",
      "2025-04-04 20:28:23,590 - INFO - Epoch [8/10], Batch [480/747], Loss: 0.2126\n",
      "2025-04-04 20:28:24,099 - INFO - Epoch [8/10], Batch [490/747], Loss: 0.2883\n",
      "2025-04-04 20:28:24,660 - INFO - Epoch [8/10], Batch [500/747], Loss: 0.3188\n",
      "2025-04-04 20:28:25,165 - INFO - Epoch [8/10], Batch [510/747], Loss: 0.2235\n",
      "2025-04-04 20:28:25,726 - INFO - Epoch [8/10], Batch [520/747], Loss: 0.4310\n",
      "2025-04-04 20:28:26,235 - INFO - Epoch [8/10], Batch [530/747], Loss: 0.3573\n",
      "2025-04-04 20:28:26,795 - INFO - Epoch [8/10], Batch [540/747], Loss: 0.2586\n",
      "2025-04-04 20:28:27,297 - INFO - Epoch [8/10], Batch [550/747], Loss: 0.4238\n",
      "2025-04-04 20:28:27,860 - INFO - Epoch [8/10], Batch [560/747], Loss: 0.2665\n",
      "2025-04-04 20:28:28,367 - INFO - Epoch [8/10], Batch [570/747], Loss: 0.1359\n",
      "2025-04-04 20:28:28,929 - INFO - Epoch [8/10], Batch [580/747], Loss: 0.2808\n",
      "2025-04-04 20:28:29,437 - INFO - Epoch [8/10], Batch [590/747], Loss: 0.2976\n",
      "2025-04-04 20:28:29,995 - INFO - Epoch [8/10], Batch [600/747], Loss: 0.3114\n",
      "2025-04-04 20:28:30,499 - INFO - Epoch [8/10], Batch [610/747], Loss: 0.2378\n",
      "2025-04-04 20:28:31,057 - INFO - Epoch [8/10], Batch [620/747], Loss: 0.3725\n",
      "2025-04-04 20:28:31,559 - INFO - Epoch [8/10], Batch [630/747], Loss: 0.4516\n",
      "2025-04-04 20:28:32,120 - INFO - Epoch [8/10], Batch [640/747], Loss: 0.2055\n",
      "2025-04-04 20:28:32,623 - INFO - Epoch [8/10], Batch [650/747], Loss: 0.2678\n",
      "2025-04-04 20:28:33,187 - INFO - Epoch [8/10], Batch [660/747], Loss: 0.4641\n",
      "2025-04-04 20:28:33,697 - INFO - Epoch [8/10], Batch [670/747], Loss: 0.5486\n",
      "2025-04-04 20:28:34,259 - INFO - Epoch [8/10], Batch [680/747], Loss: 0.7045\n",
      "2025-04-04 20:28:34,768 - INFO - Epoch [8/10], Batch [690/747], Loss: 0.2867\n",
      "2025-04-04 20:28:35,332 - INFO - Epoch [8/10], Batch [700/747], Loss: 0.2206\n",
      "2025-04-04 20:28:35,837 - INFO - Epoch [8/10], Batch [710/747], Loss: 0.4458\n",
      "2025-04-04 20:28:36,393 - INFO - Epoch [8/10], Batch [720/747], Loss: 0.4190\n",
      "2025-04-04 20:28:36,898 - INFO - Epoch [8/10], Batch [730/747], Loss: 0.2538\n",
      "2025-04-04 20:28:37,461 - INFO - Epoch [8/10], Batch [740/747], Loss: 0.2723\n",
      "2025-04-04 20:28:37,785 - INFO - Epoch 8/10 Train Loss: 0.3438, Train Accuracy: 84.20%\n",
      "2025-04-04 20:28:41,498 - INFO - Epoch 8/10 Val Loss: 0.2771, Val Accuracy: 88.17%, AUC-ROC: 0.9465\n",
      "2025-04-04 20:28:41,501 - INFO - New best model at epoch 8 with Val Accuracy: 88.17%\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:28:41,952 - INFO - Epoch [9/10], Batch [0/747], Loss: 0.3899\n",
      "2025-04-04 20:28:42,402 - INFO - Epoch [9/10], Batch [10/747], Loss: 0.3191\n",
      "2025-04-04 20:28:42,893 - INFO - Epoch [9/10], Batch [20/747], Loss: 0.2427\n",
      "2025-04-04 20:28:43,394 - INFO - Epoch [9/10], Batch [30/747], Loss: 0.3100\n",
      "2025-04-04 20:28:43,882 - INFO - Epoch [9/10], Batch [40/747], Loss: 0.2766\n",
      "2025-04-04 20:28:44,407 - INFO - Epoch [9/10], Batch [50/747], Loss: 0.2596\n",
      "2025-04-04 20:28:44,891 - INFO - Epoch [9/10], Batch [60/747], Loss: 0.3075\n",
      "2025-04-04 20:28:45,421 - INFO - Epoch [9/10], Batch [70/747], Loss: 0.3825\n",
      "2025-04-04 20:28:45,905 - INFO - Epoch [9/10], Batch [80/747], Loss: 0.3933\n",
      "2025-04-04 20:28:46,433 - INFO - Epoch [9/10], Batch [90/747], Loss: 0.2680\n",
      "2025-04-04 20:28:46,923 - INFO - Epoch [9/10], Batch [100/747], Loss: 0.3583\n",
      "2025-04-04 20:28:47,453 - INFO - Epoch [9/10], Batch [110/747], Loss: 0.5598\n",
      "2025-04-04 20:28:47,934 - INFO - Epoch [9/10], Batch [120/747], Loss: 0.2774\n",
      "2025-04-04 20:28:48,468 - INFO - Epoch [9/10], Batch [130/747], Loss: 0.2451\n",
      "2025-04-04 20:28:48,957 - INFO - Epoch [9/10], Batch [140/747], Loss: 0.2733\n",
      "2025-04-04 20:28:49,486 - INFO - Epoch [9/10], Batch [150/747], Loss: 0.3022\n",
      "2025-04-04 20:28:49,971 - INFO - Epoch [9/10], Batch [160/747], Loss: 0.2111\n",
      "2025-04-04 20:28:50,504 - INFO - Epoch [9/10], Batch [170/747], Loss: 0.2741\n",
      "2025-04-04 20:28:50,989 - INFO - Epoch [9/10], Batch [180/747], Loss: 0.4486\n",
      "2025-04-04 20:28:51,520 - INFO - Epoch [9/10], Batch [190/747], Loss: 0.4406\n",
      "2025-04-04 20:28:52,009 - INFO - Epoch [9/10], Batch [200/747], Loss: 0.3153\n",
      "2025-04-04 20:28:52,543 - INFO - Epoch [9/10], Batch [210/747], Loss: 0.2784\n",
      "2025-04-04 20:28:53,029 - INFO - Epoch [9/10], Batch [220/747], Loss: 0.5169\n",
      "2025-04-04 20:28:53,560 - INFO - Epoch [9/10], Batch [230/747], Loss: 0.3440\n",
      "2025-04-04 20:28:54,054 - INFO - Epoch [9/10], Batch [240/747], Loss: 0.2226\n",
      "2025-04-04 20:28:54,579 - INFO - Epoch [9/10], Batch [250/747], Loss: 0.3389\n",
      "2025-04-04 20:28:55,064 - INFO - Epoch [9/10], Batch [260/747], Loss: 0.5323\n",
      "2025-04-04 20:28:55,593 - INFO - Epoch [9/10], Batch [270/747], Loss: 0.3291\n",
      "2025-04-04 20:28:56,083 - INFO - Epoch [9/10], Batch [280/747], Loss: 0.3230\n",
      "2025-04-04 20:28:56,621 - INFO - Epoch [9/10], Batch [290/747], Loss: 0.4056\n",
      "2025-04-04 20:28:57,106 - INFO - Epoch [9/10], Batch [300/747], Loss: 0.3579\n",
      "2025-04-04 20:28:57,637 - INFO - Epoch [9/10], Batch [310/747], Loss: 0.3818\n",
      "2025-04-04 20:28:58,122 - INFO - Epoch [9/10], Batch [320/747], Loss: 0.3161\n",
      "2025-04-04 20:28:58,653 - INFO - Epoch [9/10], Batch [330/747], Loss: 0.2810\n",
      "2025-04-04 20:28:59,137 - INFO - Epoch [9/10], Batch [340/747], Loss: 0.2816\n",
      "2025-04-04 20:28:59,668 - INFO - Epoch [9/10], Batch [350/747], Loss: 0.4242\n",
      "2025-04-04 20:29:00,156 - INFO - Epoch [9/10], Batch [360/747], Loss: 0.2176\n",
      "2025-04-04 20:29:00,687 - INFO - Epoch [9/10], Batch [370/747], Loss: 0.3126\n",
      "2025-04-04 20:29:01,173 - INFO - Epoch [9/10], Batch [380/747], Loss: 0.2629\n",
      "2025-04-04 20:29:01,710 - INFO - Epoch [9/10], Batch [390/747], Loss: 0.6879\n",
      "2025-04-04 20:29:02,194 - INFO - Epoch [9/10], Batch [400/747], Loss: 0.2158\n",
      "2025-04-04 20:29:02,728 - INFO - Epoch [9/10], Batch [410/747], Loss: 0.3473\n",
      "2025-04-04 20:29:03,205 - INFO - Epoch [9/10], Batch [420/747], Loss: 0.2684\n",
      "2025-04-04 20:29:03,736 - INFO - Epoch [9/10], Batch [430/747], Loss: 0.2272\n",
      "2025-04-04 20:29:04,223 - INFO - Epoch [9/10], Batch [440/747], Loss: 0.1771\n",
      "2025-04-04 20:29:04,762 - INFO - Epoch [9/10], Batch [450/747], Loss: 0.1731\n",
      "2025-04-04 20:29:05,250 - INFO - Epoch [9/10], Batch [460/747], Loss: 0.4478\n",
      "2025-04-04 20:29:05,779 - INFO - Epoch [9/10], Batch [470/747], Loss: 0.3591\n",
      "2025-04-04 20:29:06,265 - INFO - Epoch [9/10], Batch [480/747], Loss: 0.2531\n",
      "2025-04-04 20:29:06,795 - INFO - Epoch [9/10], Batch [490/747], Loss: 0.3285\n",
      "2025-04-04 20:29:07,283 - INFO - Epoch [9/10], Batch [500/747], Loss: 0.2883\n",
      "2025-04-04 20:29:07,815 - INFO - Epoch [9/10], Batch [510/747], Loss: 0.2858\n",
      "2025-04-04 20:29:08,302 - INFO - Epoch [9/10], Batch [520/747], Loss: 0.3953\n",
      "2025-04-04 20:29:08,835 - INFO - Epoch [9/10], Batch [530/747], Loss: 0.2213\n",
      "2025-04-04 20:29:09,321 - INFO - Epoch [9/10], Batch [540/747], Loss: 0.2711\n",
      "2025-04-04 20:29:09,856 - INFO - Epoch [9/10], Batch [550/747], Loss: 0.2562\n",
      "2025-04-04 20:29:10,343 - INFO - Epoch [9/10], Batch [560/747], Loss: 0.4631\n",
      "2025-04-04 20:29:10,876 - INFO - Epoch [9/10], Batch [570/747], Loss: 0.3441\n",
      "2025-04-04 20:29:11,366 - INFO - Epoch [9/10], Batch [580/747], Loss: 0.3463\n",
      "2025-04-04 20:29:11,908 - INFO - Epoch [9/10], Batch [590/747], Loss: 0.2024\n",
      "2025-04-04 20:29:12,395 - INFO - Epoch [9/10], Batch [600/747], Loss: 0.1406\n",
      "2025-04-04 20:29:12,919 - INFO - Epoch [9/10], Batch [610/747], Loss: 0.2731\n",
      "2025-04-04 20:29:13,407 - INFO - Epoch [9/10], Batch [620/747], Loss: 0.2629\n",
      "2025-04-04 20:29:13,943 - INFO - Epoch [9/10], Batch [630/747], Loss: 0.4125\n",
      "2025-04-04 20:29:14,433 - INFO - Epoch [9/10], Batch [640/747], Loss: 0.2722\n",
      "2025-04-04 20:29:14,966 - INFO - Epoch [9/10], Batch [650/747], Loss: 0.2565\n",
      "2025-04-04 20:29:15,455 - INFO - Epoch [9/10], Batch [660/747], Loss: 0.5531\n",
      "2025-04-04 20:29:15,990 - INFO - Epoch [9/10], Batch [670/747], Loss: 0.3035\n",
      "2025-04-04 20:29:16,479 - INFO - Epoch [9/10], Batch [680/747], Loss: 0.3070\n",
      "2025-04-04 20:29:17,010 - INFO - Epoch [9/10], Batch [690/747], Loss: 0.3385\n",
      "2025-04-04 20:29:17,497 - INFO - Epoch [9/10], Batch [700/747], Loss: 0.3409\n",
      "2025-04-04 20:29:18,024 - INFO - Epoch [9/10], Batch [710/747], Loss: 0.2978\n",
      "2025-04-04 20:29:18,507 - INFO - Epoch [9/10], Batch [720/747], Loss: 0.2683\n",
      "2025-04-04 20:29:19,035 - INFO - Epoch [9/10], Batch [730/747], Loss: 0.2111\n",
      "2025-04-04 20:29:19,526 - INFO - Epoch [9/10], Batch [740/747], Loss: 0.2198\n",
      "2025-04-04 20:29:19,889 - INFO - Epoch 9/10 Train Loss: 0.3301, Train Accuracy: 85.26%\n",
      "2025-04-04 20:29:23,579 - INFO - Epoch 9/10 Val Loss: 0.2701, Val Accuracy: 88.65%, AUC-ROC: 0.9508\n",
      "2025-04-04 20:29:23,583 - INFO - New best model at epoch 9 with Val Accuracy: 88.65%\n",
      "/tmp/ipykernel_1095769/965209251.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
      "2025-04-04 20:29:24,032 - INFO - Epoch [10/10], Batch [0/747], Loss: 0.3187\n",
      "2025-04-04 20:29:24,482 - INFO - Epoch [10/10], Batch [10/747], Loss: 0.3419\n",
      "2025-04-04 20:29:24,975 - INFO - Epoch [10/10], Batch [20/747], Loss: 0.2214\n",
      "2025-04-04 20:29:25,517 - INFO - Epoch [10/10], Batch [30/747], Loss: 0.2536\n",
      "2025-04-04 20:29:26,015 - INFO - Epoch [10/10], Batch [40/747], Loss: 0.2103\n",
      "2025-04-04 20:29:26,559 - INFO - Epoch [10/10], Batch [50/747], Loss: 0.4089\n",
      "2025-04-04 20:29:27,055 - INFO - Epoch [10/10], Batch [60/747], Loss: 0.2991\n",
      "2025-04-04 20:29:27,606 - INFO - Epoch [10/10], Batch [70/747], Loss: 0.3728\n",
      "2025-04-04 20:29:28,108 - INFO - Epoch [10/10], Batch [80/747], Loss: 0.2552\n",
      "2025-04-04 20:29:28,651 - INFO - Epoch [10/10], Batch [90/747], Loss: 0.1820\n",
      "2025-04-04 20:29:29,149 - INFO - Epoch [10/10], Batch [100/747], Loss: 0.1737\n",
      "2025-04-04 20:29:29,700 - INFO - Epoch [10/10], Batch [110/747], Loss: 0.1583\n",
      "2025-04-04 20:29:30,197 - INFO - Epoch [10/10], Batch [120/747], Loss: 0.2776\n",
      "2025-04-04 20:29:30,745 - INFO - Epoch [10/10], Batch [130/747], Loss: 0.2622\n",
      "2025-04-04 20:29:31,241 - INFO - Epoch [10/10], Batch [140/747], Loss: 0.3733\n",
      "2025-04-04 20:29:31,789 - INFO - Epoch [10/10], Batch [150/747], Loss: 0.1905\n",
      "2025-04-04 20:29:32,287 - INFO - Epoch [10/10], Batch [160/747], Loss: 0.3485\n",
      "2025-04-04 20:29:32,840 - INFO - Epoch [10/10], Batch [170/747], Loss: 0.2261\n",
      "2025-04-04 20:29:33,338 - INFO - Epoch [10/10], Batch [180/747], Loss: 0.4798\n",
      "2025-04-04 20:29:33,886 - INFO - Epoch [10/10], Batch [190/747], Loss: 0.2544\n",
      "2025-04-04 20:29:34,380 - INFO - Epoch [10/10], Batch [200/747], Loss: 0.3650\n",
      "2025-04-04 20:29:34,928 - INFO - Epoch [10/10], Batch [210/747], Loss: 0.3728\n",
      "2025-04-04 20:29:35,429 - INFO - Epoch [10/10], Batch [220/747], Loss: 0.4611\n",
      "2025-04-04 20:29:35,985 - INFO - Epoch [10/10], Batch [230/747], Loss: 0.2691\n",
      "2025-04-04 20:29:36,489 - INFO - Epoch [10/10], Batch [240/747], Loss: 0.3062\n",
      "2025-04-04 20:29:37,034 - INFO - Epoch [10/10], Batch [250/747], Loss: 0.2741\n",
      "2025-04-04 20:29:37,528 - INFO - Epoch [10/10], Batch [260/747], Loss: 0.3194\n",
      "2025-04-04 20:29:38,074 - INFO - Epoch [10/10], Batch [270/747], Loss: 0.3352\n",
      "2025-04-04 20:29:38,569 - INFO - Epoch [10/10], Batch [280/747], Loss: 0.1945\n",
      "2025-04-04 20:29:39,119 - INFO - Epoch [10/10], Batch [290/747], Loss: 0.2695\n",
      "2025-04-04 20:29:39,617 - INFO - Epoch [10/10], Batch [300/747], Loss: 0.2380\n",
      "2025-04-04 20:29:40,157 - INFO - Epoch [10/10], Batch [310/747], Loss: 0.1669\n",
      "2025-04-04 20:29:40,653 - INFO - Epoch [10/10], Batch [320/747], Loss: 0.2932\n",
      "2025-04-04 20:29:41,203 - INFO - Epoch [10/10], Batch [330/747], Loss: 0.2704\n",
      "2025-04-04 20:29:41,699 - INFO - Epoch [10/10], Batch [340/747], Loss: 0.3180\n",
      "2025-04-04 20:29:42,250 - INFO - Epoch [10/10], Batch [350/747], Loss: 0.1492\n",
      "2025-04-04 20:29:42,751 - INFO - Epoch [10/10], Batch [360/747], Loss: 0.2046\n",
      "2025-04-04 20:29:43,298 - INFO - Epoch [10/10], Batch [370/747], Loss: 0.4203\n",
      "2025-04-04 20:29:43,797 - INFO - Epoch [10/10], Batch [380/747], Loss: 0.2304\n",
      "2025-04-04 20:29:44,346 - INFO - Epoch [10/10], Batch [390/747], Loss: 0.2772\n",
      "2025-04-04 20:29:44,844 - INFO - Epoch [10/10], Batch [400/747], Loss: 0.2119\n",
      "2025-04-04 20:29:45,391 - INFO - Epoch [10/10], Batch [410/747], Loss: 0.3533\n",
      "2025-04-04 20:29:45,889 - INFO - Epoch [10/10], Batch [420/747], Loss: 0.3783\n",
      "2025-04-04 20:29:46,434 - INFO - Epoch [10/10], Batch [430/747], Loss: 0.3181\n",
      "2025-04-04 20:29:46,934 - INFO - Epoch [10/10], Batch [440/747], Loss: 0.2262\n",
      "2025-04-04 20:29:47,481 - INFO - Epoch [10/10], Batch [450/747], Loss: 0.2419\n",
      "2025-04-04 20:29:47,978 - INFO - Epoch [10/10], Batch [460/747], Loss: 0.3871\n",
      "2025-04-04 20:29:48,527 - INFO - Epoch [10/10], Batch [470/747], Loss: 0.2808\n",
      "2025-04-04 20:29:49,028 - INFO - Epoch [10/10], Batch [480/747], Loss: 0.2656\n",
      "2025-04-04 20:29:49,575 - INFO - Epoch [10/10], Batch [490/747], Loss: 0.3498\n",
      "2025-04-04 20:29:50,074 - INFO - Epoch [10/10], Batch [500/747], Loss: 0.2378\n",
      "2025-04-04 20:29:50,622 - INFO - Epoch [10/10], Batch [510/747], Loss: 0.5415\n",
      "2025-04-04 20:29:51,128 - INFO - Epoch [10/10], Batch [520/747], Loss: 0.2393\n",
      "2025-04-04 20:29:51,684 - INFO - Epoch [10/10], Batch [530/747], Loss: 0.2599\n",
      "2025-04-04 20:29:52,186 - INFO - Epoch [10/10], Batch [540/747], Loss: 0.2626\n",
      "2025-04-04 20:29:52,732 - INFO - Epoch [10/10], Batch [550/747], Loss: 0.3749\n",
      "2025-04-04 20:29:53,231 - INFO - Epoch [10/10], Batch [560/747], Loss: 0.3175\n",
      "2025-04-04 20:29:53,776 - INFO - Epoch [10/10], Batch [570/747], Loss: 0.2176\n",
      "2025-04-04 20:29:54,274 - INFO - Epoch [10/10], Batch [580/747], Loss: 0.3732\n",
      "2025-04-04 20:29:54,825 - INFO - Epoch [10/10], Batch [590/747], Loss: 0.4274\n",
      "2025-04-04 20:29:55,322 - INFO - Epoch [10/10], Batch [600/747], Loss: 0.4409\n",
      "2025-04-04 20:29:55,867 - INFO - Epoch [10/10], Batch [610/747], Loss: 0.4428\n",
      "2025-04-04 20:29:56,366 - INFO - Epoch [10/10], Batch [620/747], Loss: 0.3472\n",
      "2025-04-04 20:29:56,910 - INFO - Epoch [10/10], Batch [630/747], Loss: 0.2041\n",
      "2025-04-04 20:29:57,405 - INFO - Epoch [10/10], Batch [640/747], Loss: 0.4338\n",
      "2025-04-04 20:29:57,945 - INFO - Epoch [10/10], Batch [650/747], Loss: 0.2953\n",
      "2025-04-04 20:29:58,447 - INFO - Epoch [10/10], Batch [660/747], Loss: 0.2306\n",
      "2025-04-04 20:29:58,995 - INFO - Epoch [10/10], Batch [670/747], Loss: 0.4066\n",
      "2025-04-04 20:29:59,495 - INFO - Epoch [10/10], Batch [680/747], Loss: 0.2871\n",
      "2025-04-04 20:30:00,046 - INFO - Epoch [10/10], Batch [690/747], Loss: 0.3319\n",
      "2025-04-04 20:30:00,545 - INFO - Epoch [10/10], Batch [700/747], Loss: 0.1636\n",
      "2025-04-04 20:30:01,094 - INFO - Epoch [10/10], Batch [710/747], Loss: 0.4614\n",
      "2025-04-04 20:30:01,591 - INFO - Epoch [10/10], Batch [720/747], Loss: 0.2988\n",
      "2025-04-04 20:30:02,138 - INFO - Epoch [10/10], Batch [730/747], Loss: 0.3032\n",
      "2025-04-04 20:30:02,635 - INFO - Epoch [10/10], Batch [740/747], Loss: 0.3627\n",
      "2025-04-04 20:30:03,028 - INFO - Epoch 10/10 Train Loss: 0.3190, Train Accuracy: 85.30%\n",
      "2025-04-04 20:30:06,750 - INFO - Epoch 10/10 Val Loss: 0.2609, Val Accuracy: 89.07%, AUC-ROC: 0.9532\n",
      "2025-04-04 20:30:06,753 - INFO - New best model at epoch 10 with Val Accuracy: 89.07%\n",
      "2025-04-04 20:30:06,753 - INFO - Total training time: 430.81 seconds\n",
      "2025-04-04 20:30:06,758 - INFO - Loaded best model with Val Accuracy: 89.07%\n",
      "2025-04-04 20:30:08,771 - INFO - \n",
      "===== Final Test Results =====\n",
      "2025-04-04 20:30:08,772 - INFO - Test Loss: 0.2741\n",
      "2025-04-04 20:30:08,773 - INFO - Test Accuracy: 88.45%\n",
      "2025-04-04 20:30:08,773 - INFO - Test AUC-ROC: 0.9497\n",
      "2025-04-04 20:30:09,116 - INFO - Model saved to models/training_using_gpus_1_amp_model.pth\n",
      "2025-04-04 20:30:10,312 - INFO - Training plots saved as plots/training_using_gpus_1_amp_results.png\n",
      "2025-04-04 20:30:10,349 - INFO - Runtime parameters saved as metrics/training_using_gpus_1_amp_params.json\n",
      "2025-04-04 20:30:10,425 - INFO - Training completed.\n",
      "2025-04-04 20:30:10,425 - INFO - Test Accuracy: 88.45%\n",
      "2025-04-04 20:30:10,425 - INFO - Total Layers: 129\n",
      "2025-04-04 20:30:10,426 - INFO - Total Parameters: 22,494,274\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    hostname = socket.gethostname()\n",
    "    logging.info(f\"Running on node: {hostname}\")\n",
    "    data_dir = \"../../preprocessed_glaucoma_data\" \n",
    "    logging.info(\"Training Streamlined Medical CNN model for Glaucoma for 10 epochs on a single GPU...\")\n",
    "    results = train_and_evaluate(data_dir)\n",
    "    logging.info(\"Training completed.\")\n",
    "    logging.info(f\"Test Accuracy: {results['test_accuracy']:.2f}%\")\n",
    "    logging.info(f\"Total Layers: {results['total_layers']}\")\n",
    "    logging.info(f\"Total Parameters: {results['total_parameters']:,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
